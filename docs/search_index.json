[
["index.html", "Hands-on Machine Learning with R Preface Who should read this Why R Structure of the book Conventions used in this book Additional resources Feedback Acknowledgments Software information", " Hands-on Machine Learning with R 2018-07-23 Preface Welcome to Hands-on Machine Learning with R. This book provides hands-on modules for many of the most common machine learning methods to include: Generalized low rank models Clustering algorithms Autoencoders Regularized models Random forests Gradient boosting machines Deep neural networks Stacking / super learner and more! You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, my motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses. For the most part, I minimize mathematical complexity when possible but also provide resources to get deeper into the details if desired. Who should read this I intend this work to be a practitioner’s guide to the machine learning process and a place where one can come to learn about the approach and to gain intuition about the many commonly used, modern, and powerful methods accepted in the machine learning community. If you are familiar with the analytic methodologies, this book may still serve as a reference for how to work with the various R packages for implementation. While an abundance of videos, blog posts, and tutorials exist online, I’ve long been frustrated by the lack of consistency, completeness, and bias towards singular packages for implementation. This is what inspired this book. This book is not meant to be an introduction to R or to programming in general; as I assume the reader has familiarity with the R language to include defining functions, managing R objects, controlling the flow of a program, and other basic tasks. If not, I would refer you to R for Data Science (Wickham and Grolemund 2016) to learn the fundamentals of data science with R such as importing, cleaning, transforming, visualizing, and exploring your data. For those looking to advance their R programming skills and knowledge of the languge, I would refer you to Advanced R (Wickham 2014). Nor is this book designed to be a deep dive into the theory and math underpinning machine learning algorithms. Several books already exist that do great justice in this arena (i.e. Elements of Statistical Learning (Friedman, Hastie, and Tibshirani 2001), Computer Age Statistical Inference (Kuhn and Johnson 2013), Deep Learning (Goodfellow et al. 2016)). Instead, this book is meant to help R users learn to use the machine learning stack within R, which includes using various R packages such as glmnet, h20, ranger, xgboost, lime, and others to effectively model and gain insight from your data. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory. While you can read this book without opening R, I highly recommend you experiment with the code examples provided throughout. Why R R has emerged over the last couple decades as a first-class tool for scientific computing tasks, and has been a consistent leader in implementing statistical methodologies for analyzing data. The usefulness of R for data science stems from the large, active, and growing ecosystem of third-party packages: tidyverse for common data analysis activities; h2o, ranger, xgboost, and others for fast and scalable machine learning; lime, pdp, DALEX, and others for machine learning interpretability; and many more tools will be mentioned throughout the pages that follow. Structure of the book Each chapter of this book focuses on a particular part of the machine learning process along with various packages to perform that process. TBD… Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following code chunks with images, which signify: Signifies a tip or suggestion Signifies a general note Signifies a warning or caution Additional resources There are many great resources available to learn about machine learning. At the end of each chapter I provide a Learn More section that lists resources that I have found extremely useful for digging deeper into the methodology and applying with code. Feedback Reader comments are greatly appreciated. To report errors or bugs please post an issue at https://github.com/bradleyboehmke/hands-on-machine-learning-with-r/issues. Acknowledgments TBD Software information An online version of this book is available at https://bradleyboehmke.github.io/hands-on-machine-learning-with-r/. The source of the book is available at https://github.com/bradleyboehmke/hands-on-machine-learning-with-r. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with the following packages and R version. All code was executed on 2013 MacBook Pro with a 2.4 GHz Intel Core i5 processor, 8 GB of memory, 1600MHz speed, and double data rate synchronous dynamic random access memory (DDR3). # packages used pkgs &lt;- c( &quot;AmesHousing&quot;, &quot;caret&quot;, &quot;data.table&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;glmnet&quot;, &quot;h2o&quot;, &quot;pROC&quot;, &quot;purrr&quot;, &quot;ROCR&quot;, &quot;rsample&quot; ) # package &amp; session info devtools::session_info(pkgs) #&gt; Session info ------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.5.0 (2018-04-23) #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; tz America/New_York #&gt; date 2018-07-23 #&gt; Packages ----------------------------------------------------------------- #&gt; package * version date source #&gt; abind 1.4-5 2016-07-21 CRAN (R 3.5.0) #&gt; AmesHousing 0.0.3 2017-12-17 CRAN (R 3.5.0) #&gt; assertthat 0.2.0 2017-04-11 CRAN (R 3.5.0) #&gt; BH 1.66.0-1 2018-02-13 CRAN (R 3.5.0) #&gt; bindr 0.1.1 2018-03-13 CRAN (R 3.5.0) #&gt; bindrcpp 0.2.2 2018-03-29 CRAN (R 3.5.0) #&gt; bitops 1.0-6 2013-08-17 CRAN (R 3.5.0) #&gt; broom 0.4.4 2018-03-29 CRAN (R 3.5.0) #&gt; caret 6.0-80 2018-05-26 CRAN (R 3.5.0) #&gt; caTools 1.17.1 2014-09-10 CRAN (R 3.5.0) #&gt; class 7.3-14 2015-08-30 CRAN (R 3.5.0) #&gt; cli 1.0.0 2017-11-05 CRAN (R 3.5.0) #&gt; codetools 0.2-15 2016-10-05 CRAN (R 3.5.0) #&gt; colorspace 1.3-2 2016-12-14 CRAN (R 3.5.0) #&gt; compiler 3.5.0 2018-04-24 local #&gt; crayon 1.3.4 2017-09-16 CRAN (R 3.5.0) #&gt; CVST 0.2-2 2018-05-26 CRAN (R 3.5.0) #&gt; data.table 1.11.4 2018-05-27 CRAN (R 3.5.0) #&gt; ddalpha 1.3.3 2018-04-30 CRAN (R 3.5.0) #&gt; DEoptimR 1.0-8 2016-11-19 CRAN (R 3.5.0) #&gt; dichromat 2.0-0 2013-01-24 CRAN (R 3.5.0) #&gt; digest 0.6.15 2018-01-28 CRAN (R 3.5.0) #&gt; dimRed 0.1.0 2017-05-04 CRAN (R 3.5.0) #&gt; dplyr 0.7.5 2018-05-19 CRAN (R 3.5.0) #&gt; DRR 0.0.3 2018-01-06 CRAN (R 3.5.0) #&gt; foreach 1.4.4 2017-12-12 CRAN (R 3.5.0) #&gt; foreign 0.8-70 2017-11-28 CRAN (R 3.5.0) #&gt; gdata 2.18.0 2017-06-06 CRAN (R 3.5.0) #&gt; geometry 0.3-6 2015-09-09 CRAN (R 3.5.0) #&gt; ggplot2 2.2.1 2016-12-30 CRAN (R 3.5.0) #&gt; glmnet 2.0-16 2018-04-02 CRAN (R 3.5.0) #&gt; glue 1.2.0.9000 2018-07-04 Github (tidyverse/glue@a2c0f8b) #&gt; gower 0.1.2 2017-02-23 CRAN (R 3.5.0) #&gt; gplots 3.0.1 2016-03-30 CRAN (R 3.5.0) #&gt; graphics * 3.5.0 2018-04-24 local #&gt; grDevices * 3.5.0 2018-04-24 local #&gt; grid 3.5.0 2018-04-24 local #&gt; gtable 0.2.0 2016-02-26 CRAN (R 3.5.0) #&gt; gtools 3.5.0 2015-05-29 CRAN (R 3.5.0) #&gt; h2o 3.18.0.11 2018-05-24 CRAN (R 3.5.0) #&gt; ipred 0.9-6 2017-03-01 CRAN (R 3.5.0) #&gt; iterators 1.0.9 2017-12-12 CRAN (R 3.5.0) #&gt; jsonlite 1.5 2017-06-01 CRAN (R 3.5.0) #&gt; kernlab 0.9-26 2018-04-30 CRAN (R 3.5.0) #&gt; KernSmooth 2.23-15 2015-06-29 CRAN (R 3.5.0) #&gt; labeling 0.3 2014-08-23 CRAN (R 3.5.0) #&gt; lattice 0.20-35 2017-03-25 CRAN (R 3.5.0) #&gt; lava 1.6.1 2018-03-28 CRAN (R 3.5.0) #&gt; lazyeval 0.2.1 2017-10-29 CRAN (R 3.5.0) #&gt; lubridate 1.7.4 2018-04-11 CRAN (R 3.5.0) #&gt; magic 1.5-8 2018-01-26 CRAN (R 3.5.0) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.5.0) #&gt; MASS 7.3-49 2018-02-23 CRAN (R 3.5.0) #&gt; Matrix 1.2-14 2018-04-13 CRAN (R 3.5.0) #&gt; methods * 3.5.0 2018-04-24 local #&gt; mnormt 1.5-5 2016-10-15 CRAN (R 3.5.0) #&gt; ModelMetrics 1.1.0 2016-08-26 CRAN (R 3.5.0) #&gt; munsell 0.4.3 2016-02-13 CRAN (R 3.5.0) #&gt; nlme 3.1-137 2018-04-07 CRAN (R 3.5.0) #&gt; nnet 7.3-12 2016-02-02 CRAN (R 3.5.0) #&gt; numDeriv 2016.8-1 2016-08-27 CRAN (R 3.5.0) #&gt; parallel 3.5.0 2018-04-24 local #&gt; pillar 1.2.3 2018-05-25 CRAN (R 3.5.0) #&gt; pkgconfig 2.0.1 2017-03-21 CRAN (R 3.5.0) #&gt; plogr 0.2.0 2018-03-25 CRAN (R 3.5.0) #&gt; plyr 1.8.4 2016-06-08 CRAN (R 3.5.0) #&gt; pROC 1.12.1 2018-05-06 CRAN (R 3.5.0) #&gt; prodlim 2018.04.18 2018-04-18 CRAN (R 3.5.0) #&gt; psych 1.8.4 2018-05-06 CRAN (R 3.5.0) #&gt; purrr 0.2.5 2018-05-29 CRAN (R 3.5.0) #&gt; R6 2.2.2 2017-06-17 CRAN (R 3.5.0) #&gt; RColorBrewer 1.1-2 2014-12-07 CRAN (R 3.5.0) #&gt; Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) #&gt; RcppRoll 0.2.2 2015-04-05 CRAN (R 3.5.0) #&gt; RCurl 1.95-4.10 2018-01-04 CRAN (R 3.5.0) #&gt; recipes 0.1.2 2018-01-11 CRAN (R 3.5.0) #&gt; reshape2 1.4.3 2017-12-11 CRAN (R 3.5.0) #&gt; rlang 0.2.1 2018-05-30 CRAN (R 3.5.0) #&gt; robustbase 0.93-0 2018-04-24 CRAN (R 3.5.0) #&gt; ROCR 1.0-7 2015-03-26 CRAN (R 3.5.0) #&gt; rpart 4.1-13 2018-02-23 CRAN (R 3.5.0) #&gt; rsample 0.0.2 2017-11-12 CRAN (R 3.5.0) #&gt; scales 0.5.0 2017-08-24 CRAN (R 3.5.0) #&gt; sfsmisc 1.1-2 2018-03-05 CRAN (R 3.5.0) #&gt; splines 3.5.0 2018-04-24 local #&gt; SQUAREM 2017.10-1 2017-10-07 CRAN (R 3.5.0) #&gt; stats * 3.5.0 2018-04-24 local #&gt; stats4 3.5.0 2018-04-24 local #&gt; stringi 1.2.2 2018-05-02 CRAN (R 3.5.0) #&gt; stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) #&gt; survival 2.41-3 2017-04-04 CRAN (R 3.5.0) #&gt; tibble 1.4.2 2018-01-22 CRAN (R 3.5.0) #&gt; tidyr 0.8.1 2018-05-18 CRAN (R 3.5.0) #&gt; tidyselect 0.2.4 2018-02-26 CRAN (R 3.5.0) #&gt; timeDate 3043.102 2018-02-21 CRAN (R 3.5.0) #&gt; tools 3.5.0 2018-04-24 local #&gt; utf8 1.1.4 2018-05-24 CRAN (R 3.5.0) #&gt; utils * 3.5.0 2018-04-24 local #&gt; viridisLite 0.3.0 2018-02-01 CRAN (R 3.5.0) #&gt; withr 2.1.2 2018-03-15 CRAN (R 3.5.0) References "],
["intro.html", "Chapter 1 Introduction 1.1 Supervised Learning 1.2 Unsupervised Learning 1.3 Machine learning interpretability 1.4 The data sets", " Chapter 1 Introduction Machine learning continues to grow in importance for many organizations across nearly all domains. Examples include: predicting the likelihood of a patient returning to the hospital (readmission) within 30 days of discharge, segmenting customers based on common attributes or purchasing behavior for target marketing, predicting coupon redemption rates for a given marketing campaign, predicting customer churn so an organization can perform preventative intervention, and many more! In essence, these tasks all seek to learn from data. To address each scenario, we use a given set of features to train an algorithm and extract insights. These algorithms, or learners, can be classified according to the amount and type of supervision provided during training. The two main groups this book focuses on includes: supervised learners that are used to construct predictive models, and unsupervised learners that are used to build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish. 1.1 Supervised Learning A predictive model is used for tasks that involve the prediction of a given output using other variables and their values (features) in the data set. Or as stated by Kuhn and Johnson (2013), predictive modeling is “the process of developing a mathematical tool or model that generates an accurate prediction” (p. 2). The learning algorithm in a predictive model attempts to discover and model the relationship among the target response (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks, using home attributes to predict the sales price, using employee attributes to predict the likelihood of attrition, using patient attributes and symptoms to predict the risk of readmission, using production attributes to predict time to market. Each of these examples have a defined learning task. They each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this text I will use various terms interchangeably for: \\(X\\): “predictor variables”, “independent variables”, “attributes”, “features”, “predictors” \\(Y\\): “target variable”, “dependent variable”, “response”, “outcome measurement” The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the desired solutions. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Supervised learning problems revolve around two primary themes: regression and classification. 1.1.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuous numeric spectrum. In the examples above predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along the continuous spectrum. Figure 1.1 illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along the plane. Figure 1.1: Average home sales price as a function of year built and total square footage. 1.1.2 Classification problems When the objective of our supervised learning is to predict a categorical response, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: did a customer redeem a coupon (yes/no, 1/0), did a customer churn (yes/no, 1/0), did a customer click on our online ad (yes/no, 1/0), classifying customer reviews: binary: positive vs negative multinomial: extremely negative to extremely positive on a 0-5 Likert scale However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e. “yes” or “no”), we often predict the probability of a particular class (i.e. yes: .65, no: .35). Then the class with the highest probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. 1.1.3 Algorithm Comparison Guide TODO: keep this here or move reference guide to back??? Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, the supervised learning algorithms I cover in this book can be applied to both. These algorithms have become the most popular machine learning applications in recent years. Although the chapters that follow will go into detail on each algorithm, the following provides a quick reference guide that compares and contrasts some of their features. Moreover, I provide recommended base learner packages that I have found to scale well with typical rectangular data analyzed by organizations. Characteristics Regularized GLM Random Forest Gradient Boosting Machine Deep Learning Allows n &lt; p Provides automatic feature selection Handles missing values No feature pre-processing required Robust to outliers Easy to tune Computational speed Predictive power Preferred regression base learner glmnet h2o.glm ranger h2o.randomForest xgboost h2o.gbm keras h2o.deeplearning Preferred classifciation base learner glmnet h2o.glm ranger h2o.randomForest xgboost h2o.gbm keras h2o.deeplearning 1.2 Unsupervised Learning Unsupervised learning, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables. For example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical regression models break down in the presence of highly correlated features. Dimension reduction techniques provide a method to reduce the feature set to a potentially smaller set of uncorrelated variables. These variables are often used as the input variables to downstream supervised models like. Unsupervised learning is often performed as part of an exploratory data analysis. However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e. linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised. However, the importance of unsupervised learning should not be overlooked and techniques for unsupervised learning are used in organizations to: Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. Identify products that have similar purchasing behavior so that managers can manage them as product groups. These questions, and many more, can be addressed with unsupervised learning. Moreover, often the results of an unsupervised model can be used as inputs to downstream supervised learning models. 1.2.1 Algorithm Decision Guide TBD 1.3 Machine learning interpretability In his seminal 2001 paper, Leo Breiman popularized the phrase: “the multiplicity of good models.” The phrase means that for the same set of input variables and prediction targets, complex machine learning algorithms can produce multiple accurate models with very similar, but not the exact same, internal architectures. Figure 1.2 is a depiction of a non-convex error surface that is representative of the error function for a machine learning algorithm with two inputs — say, a customer’s income and a customer’s age, and an output, such as the same customer’s probability of redeeming a coupon. This non-convex error surface with no obvious global minimum implies there are many different ways complex machine learning algorithms could learn to weigh a customer’s income and age to make a good decision about if they are likely to redeem a coupon. Each of these different weightings would create a different function for making coupon redemption (and therefore marketing) decisions, and each of these different functions would have different explanations. Figure 1.2: Non-convex error surface with many local minimas. All of this is an obstacle to data scientists. On one hand, different models can have widely different predictions based on the same feature set. Even models built from the same algorithm but with different hyperparameters can lead to different results. Consequently, practitioners should understand how different implementations of algorithms differ, which can cause variance in their results (i.e. a default xgboost model can produce very different results from a default gbm model, even though they both implement gradient boosting machines). Alternatively, data scientists can experience very similar predictions from different models based on the same feature set. However, these models will have very different logic and structure leading to different interpretations. Consequently, practitioners should understand how to interpret different types of models. This book will provide you with a fundamental understanding to compare and contrast models and even package implementations of similiar algorithms. Several machine learning interpretability techniques will be demonstrated to help you understand what is driving model and prediction performance. This will allow you to be more effective and efficient in applying and understanding mutliple good models. 1.4 The data sets The XX data sets chosen for this book allow us to illustrate the different features of our machine learning algorithms. Since the goal of this book is to demonstrate how to implement R’s ML stack, I make the assumption that you have already spent significant time cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this book such as: feature selection: removing unnecessary variables and retaining only those variables you wish to include in your modeling process, recoding variable names and values so that they are meaningful and interpretable, recoding or removing missing values. Consequently, the exemplar data sets I use throughout this book have, for the most part, gone through the necessary cleaning process. These data sets are all freely available and include: Property sales information as described in De Cock (2011). problem type: supervised regression response variable: sale price (i.e. $195,000, $215,000) features: 80 observations: 2,930 objective: use property attributes to predict the sale price of a home access: provided by the AmesHousing package (Kuhn 2017) more details: See ?AmesHousing::ames_raw # access data ames &lt;- AmesHousing::make_ames() # initial dimension dim(ames) ## [1] 2930 81 # response variable head(ames$SalePrice) ## NULL You can see the entire data cleaning process to transform the raw Ames housing data (AmesHousing::ames_raw) to the final clean data (AmesHousing::make_ames) that we will use in machine learning algorithms throughout this book at: https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R Employee attrition information originally provided by IBM Watson Analytics Lab. problem type: supervised binomial classification response variable: Attrition (i.e. “Yes”, “No”) features: 30 observations: 1,470 objective: use employee attributes to predict if they will attrit (leave the company) access: provided by the rsample package (Kuhn and Wickham 2017) more details: See ?rsample::attrition # access data attrition &lt;- rsample::attrition # initial dimension dim(attrition) ## [1] 1470 31 # response variable head(attrition$Attrition) ## [1] Yes No Yes No No No ## Levels: No Yes Image information for handwritten numbers originally presented to AT&amp;T Bell Lab’s to help build automatic mail-sorting machines for the USPS. Has been used since early 1990s to compare machine learning performance on pattern recognition (i.e. LeCun et al. (1990); LeCun et al. (1998); Cireşan, Meier, and Schmidhuber (2012)). Problem type: supervised multinomial classification response variable: V785 (i.e. numbers to predict: 0, 1, …, 9) features: 784 observations: 60,000 (train) / 10,000 (test) objective: use attributes about the “darkness” of each of the 784 pixels in images of handwritten numbers to predict if the number is 0, 1, …, or 9. access: see the code chunk that follows for download instructions more details: See online MNIST documentation # load training data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz train &lt;- data.table::fread(&quot;../data/mnist_train.csv&quot;, data.table = FALSE) # load test data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz test &lt;- data.table::fread(&quot;../data/mnist_test.csv&quot;, data.table = FALSE) # initial dimension dim(train) ## [1] 60000 785 # response variable head(train$V785) ## [1] 2 3 0 0 2 7 TODO: get unsupervised data sets for clustering and dimension reduction examples References "],
["regression-performance.html", "Chapter 2 Preparing for Supervised Machine Learning 2.1 Prerequisites 2.2 Data splitting 2.3 Feature engineering 2.4 Basic model formulation 2.5 Model tuning 2.6 Cross Validation for Generalization 2.7 Model evaluation", " Chapter 2 Preparing for Supervised Machine Learning Machine learning is a very iterative process. If performed and interpreted correctly, we can have great confidence in our outcomes. If not, the results will be useless. Approaching machine learning correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing variables, minimizing data leakage, tuning hyperparameters, and assessing model performance. Before introducing specific algorithms, this chapter introduces concepts that are commonly required in the supervised machine learning process and that you’ll see briskly covered in each chapter. 2.1 Prerequisites This chapter leverages the following packages. library(rsample) library(caret) library(h2o) library(dplyr) # turn off progress bars h2o.no_progress() # launch h2o h2o.init() ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmp1Kquos/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmp1Kquos/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 254 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 1 month and 29 days ## H2O cluster name: H2O_started_from_R_bradboehmke_zuy007 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.0 (2018-04-23) To illustrate some of the concepts, we will use the Ames Housing data and employee attrition data introduced in Chapter 1. Throughout this book, I’ll demonstrate approaches with regular data frames. However, since many of the supervised machine learning chapters leverage the h2o package, we’ll also show how to do some of the tasks with H2O objects. This requires your data to be in an H2O object, which you can convert any data frame easily with as.h2o. If you try to convert the original rsample::attrition data set to an H2O object an error will occur. This is because several variables are ordered factors and H2O has no way of handling this data type. Consequently, you must convert any ordered factors to unordered. # ames data ames &lt;- AmesHousing::make_ames() ames.h2o &lt;- as.h2o(ames) # attrition data churn &lt;- rsample::attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) churn.h2o &lt;- as.h2o(churn) 2.2 Data splitting 2.2.1 Spending our data wisely A major goal of the machine learning process is to find an algorithm \\(f(x)\\) that most accurately predicts future values (\\(y\\)) based on a set of inputs (\\(x\\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we “spend” our data will help us understand how well our algorithm generalizes to unseen data. To provide an accurate understanding of the generalizability of our final optimal model, we split our data into training and test data sets: Training Set: these data are used to train our algorithms and tune hyper-parameters. Test Set: having chosen a final model, these data are used to estimate its prediction error (generalization error). These data should not be used during model training! Figure 2.1: Splitting data into training and test sets. Given a fixed amount of data, typical recommendations for splitting your data into training-testing splits include 60% (training) - 40% (testing), 70%-30%, or 80%-20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep in mind that as your overall data set gets smaller, spending too much in training (\\(&gt;80\\%\\)) won’t allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting), sometimes too much spent in testing (\\(&gt;40\\%\\)) won’t allow us to get a good assessment of model parameters In today’s data-rich environment, typically, we are not lacking in the quantity of observations, so a 70-30 split is often sufficient. The two most common ways of splitting data include simple random sampling and stratified sampling. 2.2.2 Simple random sampling The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the percentage of data represented in your response variable (\\(y\\)). There are multiple ways to split our data. Here we show four options to produce a 70-30 split (note that setting the seed value allows you to reproduce your randomized splits): Sampling is a random process so setting the random number generator with a common seed allows for reproducible results. Throughout this book I will use the number 123 often for reproducibility but the number itself has no special meaning. # base R set.seed(123) index_1 &lt;- sample(1:nrow(ames), round(nrow(ames) * 0.7)) train_1 &lt;- ames[index_1, ] test_1 &lt;- ames[-index_1, ] # caret package set.seed(123) index_2 &lt;- createDataPartition(ames$Sale_Price, p = 0.7, list = FALSE) train_2 &lt;- ames[index_2, ] test_2 &lt;- ames[-index_2, ] # rsample package set.seed(123) split_1 &lt;- initial_split(ames, prop = 0.7) train_3 &lt;- training(split_1) test_3 &lt;- testing(split_1) # h2o package split_2 &lt;- h2o.splitFrame(ames.h2o, ratios = 0.7, seed = 123) train_4 &lt;- split_2[[1]] test_4 &lt;- split_2[[2]] Since this sampling approach will randomly sample across the distribution of \\(y\\) (Sale_Price in our example), you will typically result in a similar distribution between your training and test sets as illustrated below. Figure 2.2: Training (black) vs. test (red) distribution. 2.2.3 Stratified sampling However, if we want to explicitly control our sampling so that our training and test sets have similar \\(y\\) distributions, we can use stratified sampling. This is more common with classification problems where the reponse variable may be imbalanced (90% of observations with response “Yes” and 10% with response “No”). However, we can also apply to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable, stratified sampling will break \\(y\\) down into quantiles and randomly sample from each quantile. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets. The easiest way to perform stratified sampling on a response variable is to use the rsample package, where you specify the response variable to stratafy. The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes: 16%). By enforcing stratified sampling both our training and testing sets have approximately equal response distributions. # orginal response distribution table(churn$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8387755 0.1612245 # stratified sampling with the rsample package set.seed(123) split_strat &lt;- initial_split(churn, prop = 0.7, strata = &quot;Attrition&quot;) train_strat &lt;- training(split_strat) test_strat &lt;- testing(split_strat) # consistent response ratio between train &amp; test table(train_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.838835 0.161165 table(test_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8386364 0.1613636 2.3 Feature engineering Feature engineering generally refers to the process of adding, deleting, and transforming the variables to be applied to your machine learning algorithms. Feature engineering is a significant process and requires you to spend substantial time understanding your data…or as Leo Breiman said “live with your data before you plunge into modeling.” Although this book primarily focuses on applying machine learning algorithms, feature engineering can make or break an algorithm’s predictive ability. We will not cover all the potential ways of implementing feature engineering; however, we will cover a few fundamental pre-processing tasks that can significantly improve modeling performance. To learn more about feature engineering check out Feature Engineering for Machine Learning by Zheng and Casari (2018) and Max Kuhn’s upcoming book Feature Engineering and Selection: A Practical Approach for Predictive Models. 2.3.1 Response Transformation Although not a requirement, normalizing the distribution of the response variable by using a transformation can lead to a big improvement, especially for parametric models. As we saw in the data splitting section, our response variable Sale_Price is right skewed. ggplot(train_1, aes(x = Sale_Price)) + geom_density(trim = TRUE) + geom_density(data = test_1, trim = TRUE, col = &quot;red&quot;) Figure 2.3: Right skewed response variable. To normalize, we have a few options: Option 1: normalize with a log transformation. This will transform most right skewed distributions to be approximately normal. # log transformation train_log_y &lt;- log(train_1$Sale_Price) test_log_y &lt;- log(test_1$Sale_Price) If your reponse has negative values then a log transformation will produce NaNs. If these negative values are small (between -0.99 and 0) then you can apply log1p, which adds 1 to the value prior to applying a log transformation. If your data consists of negative equal to or less than -1, use the Yeo Johnson transformation mentioned next. log(-.5) ## [1] NaN log1p(-.5) ## [1] -0.6931472 Option 2: use a Box Cox transformation. A Box Cox transformation is more flexible and will find the transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution. Be sure to compute the lambda on the training set and apply that same lambda to both the training and test set to minimize data leakage. # Box Cox transformation lambda &lt;- forecast::BoxCox.lambda(train_1$Sale_Price) train_bc_y &lt;- forecast::BoxCox(train_1$Sale_Price, lambda) test_bc_y &lt;- forecast::BoxCox(test_1$Sale_Price, lambda) We can see that in this example, the log transformation and Box Cox transformation both do about equally well in transforming our reponse variable to be normally distributed. Figure 2.4: Response variable transformations. Note that when you model with a transformed response variable, your predictions will also be in the transformed value. You will likely want to re-transform your predicted values back to their normal state so that decision-makers can interpret the results. The following code can do this for you: # log transform a value y &lt;- log(10) # re-transforming the log-transformed value exp(y) ## [1] 10 # Box Cox transform a value y &lt;- forecast::BoxCox(10, lambda) # Inverse Box Cox function inv_box_cox &lt;- function(x, lambda) { if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda) } # re-transforming the Box Cox-transformed value inv_box_cox(y, lambda) ## [1] 10 ## attr(,&quot;lambda&quot;) ## [1] -0.3067918 If your response has negative values, you can use the Yeo-Johnson transformation. To apply, use car::powerTransform to identify the lambda, car::yjPower to apply the transformation, and VGAM::yeo.johnson to apply the transformation and/or the inverse transformation. 2.3.2 Predictor Transformation 2.3.3 One-hot encoding Many models require all predictor variables to be numeric. Consequently, we need to transform any categorical variables into numeric representations so that these algorithms can compute. Some packages automate this process (i.e. h2o, glm, caret) while others do not (i.e. glmnet, keras). Furthermore, there are many ways to encode categorical variables as numeric representations (i.e. one-hot, ordinal, binary, sum, Helmert). The most common is referred to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding variable x in the following: id x 1 a 2 c 3 b 4 c 5 c 6 a 7 b 8 c results in the following representation: id x.a x.b x.c 1 1 0 0 2 0 0 1 3 0 1 0 4 0 0 1 5 0 0 1 6 1 0 0 7 0 1 0 8 0 0 1 This is called less than full rank encoding where we retain all variables for each level of x. However, this creates perfect collinearity which causes problems with some machine learning algorithms (i.e. generalized regression models, neural networks). Alternatively, we can create full-rank one-hot encoding by dropping one of the levels (level a has been dropped): id x.b x.c 1 0 0 2 0 1 3 1 0 4 0 1 5 0 1 6 0 0 7 1 0 8 0 1 If you needed to manually implement one-hot encoding yourself you can with caret::dummyVars. Sometimes you may have a feature level with very few observations and all these observations show up in the test set but not the training set. The benefit of using dummyVars on the full data set and then applying the result to both the train and test data sets is that it will guarantee that the same features are represented in both the train and test data. # full rank one-hot encode - recommended for generalized linear models and # neural networks full_rank &lt;- dummyVars( ~ ., data = ames, fullRank = TRUE) train_oh &lt;- predict(full_rank, train_1) test_oh &lt;- predict(full_rank, test_1) # less than full rank --&gt; dummy encoding dummy &lt;- dummyVars( ~ ., data = ames, fullRank = FALSE) train_oh &lt;- predict(dummy, train_1) test_oh &lt;- predict(dummy, test_1) Two things to note: since one-hot encoding adds new features it can significantly increase the dimensionality of our data. If you have a data set with many categorical variables and those categorical variables in turn have many unique levels, the number of features can explode. In these cases you may want to explore ordinal encoding of your data. if using h2o you do not need to explicity encode your categorical predictor variables but you can override the default encoding. This can be considered a tuning parameter as some encoding approaches will improve modeling accuracy over other encodings. See the encoding options for h2o here. 2.3.4 Standardizing Some models (K-NN, SVMs, PLS, neural networks) require that the predictor variables have the same units. Centering and scaling can be used for this purpose and is often referred to as standardizing the features. Standardizing numeric variables results in zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Some packages have built-in arguments (i.e. h2o, caret) to standardize and some do not (i.e. glm, keras). If you need to manually standardize your variables you can use the preProcess function provided by the caret package. For example, here we center and scale our Ames predictor variables. It is important that you standardize the test data based on the training mean and variance values of each feature. This minimizes data leakage. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;center&quot;, &quot;scale&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 2.3.5 Alternative Feature Transformation There are some alternative transformations that you can perform: Normalizing the predictor variables with a Box Cox transformation can improve parametric model performance. Collapsing highly correlated variables with PCA can reduce the number of features and increase the stability of generalize linear models. However, this reduces the amount of information at your disposal and we show you how to use regularization as a better alternative to PCA. Removing near-zero or zero variance variables. Variables with vary little variance tend to not improve model performance and can be removed. preProcess provides many other transformation options which you can read more about here. For example, the following normalizes predictors with a Box Cox transformation, center and scales continuous variables, performs principal component analysis to reduce the predictor dimensions, and removes predictors with near zero variance. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;, &quot;nzv&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 2.4 Basic model formulation There are many packages to perform machine learning and there are almost always more than one to perform each algorithm (i.e. there are over 20 packages to perform random forests). There are pros and cons to each package; some may be more computationally efficient while others may have more hyperparameter tuning options. Future chapters will expose you to many of the packages and algorithms that perform and scale best to most organization’s problems and data sets. Just realize there are more ways than one to skin a 🙀. For example, these three functions will all produce the same linear regression model output. lm.lm &lt;- lm(Sale_Price ~ ., data = train_1) lm.glm &lt;- glm(Sale_Price ~ ., data = train_1, family = gaussian) lm.caret &lt;- train(Sale_Price ~ ., data = train_1, method = &quot;lm&quot;) One thing you will notice throughout this guide is that we can specify our model formulation in different ways. In the above examples we use the model formulation (Sale_Price ~ . which says explain Sale_Price based on all features) approach. Alternative approaches, which you will see more often throughout this guide, are the matrix formulation and variable name specification approaches. Matrix formulation requires that we separate our response variable from our features. For example, in the regularization section we’ll use glmnet which requires our features (x) and response (y) variable to be specified separately: # get feature names features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # create feature and response set train_x &lt;- train_1[, features] train_y &lt;- train_1$Sale_Price # example of matrix formulation glmnet.m1 &lt;- glmnet(x = train_x, y = train_y) Alternatively, h2o uses variable name specification where we provide all the data combined in one training_frame but we specify the features and response with character strings: # create variable names and h2o training frame y &lt;- &quot;Sale_Price&quot; x &lt;- setdiff(names(train_1), y) train.h2o &lt;- as.h2o(train_1) # example of variable name specification h2o.m1 &lt;- h2o.glm(x = x, y = y, training_frame = train.h2o) 2.5 Model tuning Hyperparameters control the level of model complexity. Some algorithms have many tuning parameters while others have only one or two. Tuning can be a good thing as it allows us to transform our model to better align with patterns within our data. For example, the simple illustration below shows how the more flexible model aligns more closely to the data than the fixed linear model. Figure 2.5: Tuning allows for more flexible patterns to be fit. However, highly tunable models can also be dangerous because they allow us to overfit our model to the training data, which will not generalize well to future unseen data. Figure 2.6: Highly tunable models can overfit if we are not careful. Throughout this guide we will demonstrate how to tune the different parameters for each model. One way to performing hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy. However, this would be very tedious work. An alternative approach is to perform a grid search. A grid search is an automated approach to searching across many combinations of hyperparameter values. Throughout this guide you will be exposed to different approaches to performing grid searches. 2.6 Cross Validation for Generalization Our goal is to not only find a model that performs well on training data but to find one that performs well on future unseen data. So although we can tune our model to reduce some error metric to near zero on our training data, this may not generalize well to future unseen data. Consequently, our goal is to find a model and its hyperparameters that will minimize error on held-out data. Let’s go back to this image… Figure 2.7: Bias versus variance. The model on the left is considered rigid and consistent. If we provided it a new training sample with slightly different values, the model would not change much, if at all. Although it is consistent, the model does not accurately capture the underlying relationship. This is considered a model with high bias. The model on the right is far more inconsistent. Even with small changes to our training sample, this model would likely change significantly. This is considered a model with high variance. The model in the middle balances the two and, likely, will minimize the error on future unseen data compared to the high bias and high variance models. This is our goal. Figure 2.8: Bias-variance tradeoff. To find the model that balances the bias-variance tradeoff, we search for a model that minimizes a k-fold cross-validation error metric. k-fold cross-validation is a resampling method that randomly divides the training data into k groups (aka folds) of approximately equal size. The model is fit on \\(k-1\\) folds and then the held-out validation fold is used to compute the error. This procedure is repeated k times; each time, a different group of observations is treated as the validation set. This process results in k estimates of the test error (\\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_k\\)). Thus, the k-fold CV estimate is computed by averaging these values, which provides us with an approximation of the error to expect on unseen data. Figure 2.9: Illustration of the k-fold cross validation process. The algorithms we cover in this guide all have built-in cross validation capabilities. One typically uses a 5 or 10 fold CV (\\(k = 5\\) or \\(k = 10\\)). For example, h2o implements CV with the nfolds argument: # example of 10 fold CV in h2o h2o.cv &lt;- h2o.glm( x = x, y = y, training_frame = train.h2o, nfolds = 10 ) 2.7 Model evaluation This leads us to our final topic, error metrics to evaluate performance. There are several metrics we can choose from to assess the error of a supervised machine learning model. The most common include: 2.7.1 Regression models MSE: Mean squared error is the average of the squared error (\\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\)). The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. Objective: minimize RMSE: Root mean squared error. This simply takes the square root of the MSE metric (\\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize Deviance: Short for mean residual deviance. In essence, it provides a measure of goodness-of-fit of the model being evaluated when compared to the null model (intercept only). If the response variable distribution is gaussian, then it is equal to MSE. When not, it usually gives a more useful estimate of error. Objective: minimize MAE: Mean absolute error. Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values (\\(MAE = \\frac{1}{n} \\sum^n_{i=1}(\\vert y_i - \\hat y_i \\vert)\\)). Objective: minimize RMSLE: Root mean squared logarithmic error. Similiar to RMSE but it performs a log() on the actual and predicted values prior to computing the difference (\\(RMSLE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(log(y_i + 1) - log(\\hat y_i + 1))^2}\\)). When your response variable has a wide range of values, large repsonse values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. Objective: minimize \\(R^2\\): This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable. Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower \\(R^2\\) than the other. You should not place too much emphasis on this metric. Objective: maximize Most models we assess in this guide will report most, if not all, of these metrics. We will emphasize MSE and RMSE but its good to realize that certain situations warrant emphasis on some more than others. 2.7.2 Classification models Misclassification: This is the overall error. For example, say you are predicting 3 classes ( high, medium, low ) and each class has 25, 30, 35 observations respectively (90 observations total). If you misclassify 3 observations of class high, 6 of class medium, and 4 of class low, then you misclassified 13 out of 90 observations resulting in a 14% misclassification rate. Objective: minimize Mean per class error: This is the average error rate for each class. For the above example, this would be the mean of \\(\\frac{3}{25}, \\frac{6}{30}, \\frac{4}{35}\\), which is 12%. If your classes are balanced this will be identical to misclassification. Objective: minimize MSE: Mean squared error. Computes the distance from 1.0 to the probability suggested. So, say we have three classes, A, B, and C, and your model predicts a probabilty of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer was A the \\(MSE = 0.09^2 = 0.0081\\), if it is B \\(MSE = 0.93^2 = 0.8649\\), if it is C \\(MSE = 0.98^2 = 0.9604\\). The squared component results in large differences in probabilities for the true class having larger penalties. Objective: minimize Cross-entropy (aka Log Loss or Deviance): Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class. Consequently, this metric disproportionately punishes predictions where we predict a small probability for the true class, which is another way of saying having high confidence in the wrong answer is really bad. Objective: minimize Gini index: Mainly used with tree-based methods and commonly referred to as a measure of purity where a small value indicates that a node contains predominantly observations from a single class. Objective: minimize When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a true positive. However, if we predict a level or event that did not happen this is called a false positive (i.e. we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a false negative (i.e. a customer that we did not predict to redeem a coupon does). Figure 2.10: Confusion matrix. We can extract different levels of performance from these measures. For example, given the classification matrix below we can assess the following: Accuracy: Overall, how often is the classifier correct? Opposite of misclassification above. Example: \\(\\frac{TP + TN}{total} = \\frac{100+50}{165} = 0.91\\). Objective: maximize Precision: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: \\(\\frac{TP}{TP + FP} = \\frac{100}{100+10} = 0.91\\). Objective: maximize Sensitivity (aka recall): How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: \\(\\frac{TP}{TP + FN} = \\frac{100}{100+5} = 0.95\\). Objective: maximize Specificity: How accurately does the classifier classify actual non-events? Example: \\(\\frac{TN}{TN + FP} = \\frac{50}{50+10} = 0.83\\). Objective: maximize Figure 2.11: Example confusion matrix. AUC: Area under the curve. A good classifier will have high precision and sensitivity. This means the classifier does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. To capture this balance, we often use a ROC curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve. Objective: maximize Figure 2.12: ROC curve. References "],
["regularized-regression.html", "Chapter 3 Regularized Regression 3.1 Prerequisites 3.2 Advantages &amp; Disadvantages 3.3 The Idea 3.4 Implementation: Regression 3.5 Implementation: Binary Classification 3.6 Implementation: Multinomial Classification 3.7 Learning More", " Chapter 3 Regularized Regression Generalized linear models (GLMs) such as ordinary least squares regression and logistic regression are simple and fundamental approaches for supervised learning. Moreover, when the assumptions required by GLMs are met, the coefficients produced are unbiased and, of all unbiased linear techniques, have the lowest variance. However, in today’s world, data sets being analyzed typically have a large amount of features. As the number of features grow, our GLM assumptions typically break down and our models often overfit (aka have high variance) to the training sample, causing our out of sample error to increase. Regularization methods provide a means to control our coefficients, which can reduce the variance and decrease out of sample error. 3.1 Prerequisites This chapter assumes you are familiary with basic idea behind linear and logistic regression. If not, two tutorials to get you up to speed include this linear regression tutorial and this logistic regression tutorial. This chapter leverages the following packages. Some of these packages are playing a supporting role while the main emphasis will be on the glmnet (Friedman, Hastie, and Tibshirani 2010) and h2o (Kraljevic 2018) packages. library(glmnet) # implementing regularized regression approaches library(h2o) # implementing regularized regression approaches library(rsample) # training vs testing data split library(dplyr) # basic data manipulation procedures library(ggplot2) # plotting 3.2 Advantages &amp; Disadvantages Advantages: Normal GLM models require that you have more observations than variables (\\(n&gt;p\\)); regularized regression allows you to model wide data where \\(n&lt;p\\). Minimizes the impact of multicollinearity. Provides automatic feature selection (at least when you apply a Lasso or elastic net penalty). Minimal hyperparameters making it easy to tune. Computationally efficient - relatively fast compared to other algorithms in this guide and does not require large memory. Disdvantages: Requires data pre-processing - requires all variables to be numeric (i.e. one-hot encode). However, h2o helps to automate this process. Does not handle missing data - must impute or remove observations with missing values. Not robust to outliers as they can still bias the coefficients. Assumes relationships between predictors and response variable to be monotonic linear (always increasing or decreasing in a linear fashion). Typically does not perform as well as more advanced methods that allow non-monotonic and non-linear relationships (i.e. random forests, gradient boosting machines, neural networks). 3.3 The Idea The easiest way to understand regularized regression is to explain how it is applied to ordinary least squares regression (OLS). The objective of OLS regression is to find the plane that minimizes the sum of squared errors (SSE) between the observed and predicted response. Illustrated below, this means identifying the plane that minimizes the grey lines, which measure the distance between the observed (red dots) and predicted response (blue plane). Figure 2.1: Fitted regression line using Ordinary Least Squares. More formally, this objective function is written as: \\[\\text{minimize} \\bigg \\{ SSE = \\sum^n_{i=1} (y_i - \\hat{y}_i)^2 \\bigg \\} \\tag{1}\\] The OLS objective function performs quite well when our data align to the key assumptions of OLS regression: Linear relationship Multivariate normality No autocorrelation Homoscedastic (constant variance in residuals) There are more observations (n) than features (p) (\\(n &gt; p\\)) No or little multicollinearity However, for many real-life data sets we have very wide data, meaning we have a large number of features (p) that we believe are informative in predicting some outcome. As p increases, we can quickly violate some of the OLS assumptions and we require alternative approaches to provide predictive analytic solutions. Specifically, as p increases there are three main issues we most commonly run into: 3.3.1 1. Multicollinearity As p increases we are more likely to capture multiple features that have some multicollinearity. When multicollinearity exists, we often see high variability in our coefficient terms. For example, in our Ames data, Gr_Liv_Area and TotRms_AbvGrd are two variables that have a correlation of 0.808 and both variables are strongly correlated to our response variable (Sale_Price). When we fit a model with both these variables we get a positive coefficient for Gr_Liv_Area but a negative coefficient for TotRms_AbvGrd, suggesting one has a positive impact to Sale_Price and the other a negative impact. # fit with two strongly correlated variables lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames) ## ## Coefficients: ## (Intercept) Gr_Liv_Area TotRms_AbvGrd ## 42767.6 139.4 -11025.9 However, if we refit the model with each variable independently, they both show a positive impact. However, the Gr_Liv_Area effect is now smaller and the TotRms_AbvGrd is positive with a much larger magnitude. # fit with just Gr_Liv_Area lm(Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Coefficients: ## (Intercept) Gr_Liv_Area ## 13289.6 111.7 # fit with just TotRms_Area lm(Sale_Price ~ TotRms_AbvGrd, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames) ## ## Coefficients: ## (Intercept) TotRms_AbvGrd ## 18665 25164 This is a common result when collinearity exists. Coefficients for correlated features become over-inflated and can fluctuate significantly. One consequence of these large fluctuations in the coefficient terms is overfitting, which means we have high variance in the bias-variance tradeoff space. Although an analyst can use tools such as variance inflaction factors (Myers 1990) to identify and remove those strongly correlated variables, it is not always clear which variable(s) to remove. Nor do we always wish to remove variables as this may be removing signal in our data. 3.3.2 2. Insufficient solution When the number of features exceed the number of observations (\\(p &gt; n\\)), the OLS solution matrix is not invertible. This causes significant issues because it means: (1) The least-squares estimates are not unique. In fact, there are an infinite set of solutions available and most of these solutions overfit the data. (2) In many instances the result will be computationally infeasible. Consequently, to resolve this issue an analyst can remove variables until \\(p &lt; n\\) and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach (Kuhn &amp; Johnson, 2013, pp. 43-47), it can be cumbersome and prone to errors. 3.3.3 3. Interpretability With a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection. One approach to this is called hard threshholding feature selection, which can be performed with linear model selection approaches. However, model selection approaches can be computationally inefficient, do not scale well, and they simply assume a feature as in or out. We may wish to use a soft threshholding approach that slowly pushes a feature’s effect towards zero. As will be demonstrated, this can provide additional understanding regarding predictive signals. 3.3.4 Regularized Models When we experience these concerns, one alternative to OLS regression is to use regularized regression (also commonly referred to as penalized models or shrinkage methods) to control the parameter estimates. Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model. The objective function of regularized regression methods is very similar to OLS regression; however, we add a penalty parameter (P). \\[\\text{minimize} \\big \\{ SSE + P \\big \\} \\tag{2}\\] This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE). This concept generalizes to all GLM models. So far, we have be discussing OLS and the sum of squared errors. However, different models within the GLM family (i.e. logistic regression, Poisson regression) have different loss functions. Yet we can think of the penalty parameter all the same - it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model’s loss function. There are three types of penalty parameters we can implement: Ridge Lasso Elastic net, which is a combination of Ridge and Lasso 3.3.4.1 Ridge penalty Ridge regression (Hoerl and Kennard 1970) controls the coefficients by adding \\(\\lambda \\sum^p_{j=1} \\beta_j^2\\) to the objective function. This penalty parameter is also referred to as “\\(L_2\\)” as it signifies a second-order penalty being used on the coefficients.1 \\[\\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} \\beta_j^2 \\bigg \\} \\tag{3}\\] This penalty parameter can take on a wide range of values, which is controlled by the tuning parameter \\(\\lambda\\). When \\(\\lambda = 0\\) there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE. However, as \\(\\lambda \\rightarrow \\infty\\), the penalty becomes large and forces our coefficients to near zero. This is illustrated in Figure 3.1 where exemplar coefficients have been regularized with \\(\\lambda\\) ranging from 0 to over 8,000 (\\(log(8103) = 9\\)). Figure 3.1: Ridge regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Although these coefficients were scaled and centered prior to the analysis, you will notice that some are extremely large when \\(\\lambda \\rightarrow 0\\). Furthermore, you’ll notice the large negative parameter that fluctuates until \\(log(\\lambda) \\approx 2\\) where it then continuously skrinks to zero. This is indicitive of multicollinearity and likely illustrates that constraining our coefficients with \\(log(\\lambda) &gt; 2\\) may reduce the variance, and therefore the error, in our model. In essence, the ridge regression model pushes many of the correlated features towards each other rather than allowing for one to be wildly positive and the other wildly negative. Furthermore, many of the non-important features get pushed to near zero. This allows us to reduce the noise in our data, which provides us more clarity in identifying the true signals in our model. However, a ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection. If greater interpretation is necessary where you need to reduce the signal in your data to a smaller subset then a lasso or elastic net penalty may be preferable. 3.3.4.2 Lasso penalty The least absolute shrinkage and selection operator (lasso) model (Tibshirani 1996) is an alternative to the ridge penalty that has a small modification to the penalty in the objective function. Rather than the \\(L_2\\) penalty we use the following \\(L_1\\) penalty \\(\\lambda \\sum^p_{j=1} | \\beta_j|\\) in the objective function. \\[\\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\tag{4}\\] Whereas the ridge penalty approach pushes variables to approximately but not equal to zero, the lasso penalty will actually push coefficients to zero as illustrated in Figure 3.2. Thus the lasso model not only improves the model with regularization but it also conducts automated feature selection. Figure 3.2: Lasso regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Numbers on top axis illustrate how many non-zero coefficients remain. In the figure above we see that when \\(log(\\lambda) = -5\\) all 15 variables are in the model, when \\(log(\\lambda) = -1\\) 12 variables are retained, and when \\(log(\\lambda) = 1\\) only 3 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal. 3.3.4.3 Elastic nets A generalization of the ridge and lasso penalties is the elastic net penalty (Zou and Hastie 2005), which combines the two penalties. \\[\\text{minimize } \\bigg \\{ SSE + \\lambda_1 \\sum^p_{j=1} \\beta_j^2 + \\lambda_2 \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\tag{5}\\] Although lasso models perform feature selection, a result of their penalty parameter is that typically when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically reducing correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. 3.3.5 Tuning Regularized models are simple to tune as there are only two tuning parameters: Size of penalty (\\(\\lambda\\)): Controls how much we want to constrain our coefficients. Small penalties where \\(\\lambda\\) is close to zero allow our coefficients to be larger; however, larger values of \\(\\lambda\\) penalize our coefficients and forces them to take on smaller values. Hence, this parameter is often called the shrinkage parameter. Alpha: The alpha parameter tells our model to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)). 3.3.6 Package implementation There are a few packages that implement variants of regularized regression. You can find a comprehensive list on the CRAN Machine Learning Task View. However, the most popular implementations which we will cover in this chapter include: glmnet: The original implementation of regularized regression in R. The glmnet R package provides an extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression and the Cox model. Two recent additions are the multiple-response Gaussian, and the grouped multinomial regression. A nice vignette is available here. Features include2: The code can handle sparse input-matrix formats, as well as range constraints on coefficients. Automatically standardizes your feature set. Built-in cross validation. The core of glmnet is a set of fortran subroutines, which make for very fast execution. The algorithms use coordinate descent with warm starts and active set iterations. Supports the following distributions: “gaussian”,“binomial”,“poisson”,“multinomial”,“cox”,“mgaussian” h2o: The h2o R package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with a fairly comprehensive online resource that includes methodology and code documentation along with tutorials. Features include: Fits both regularized and non-regularized GLMs. Automated feature pre-processing (one-hot encode &amp; standardization). Built-in cross validation. Built-in grid search capabilities. Supports the following distributions: “guassian”, “binomial”, “multinomial”, “ordinal”, “poisson”, “gamma”, “tweedie”. Distributed and parallelized computation on either a single node or a multi-node cluster. Automatic early stopping based on convergence of user-specified metrics to user-specified relative tolerance. 3.4 Implementation: Regression To illustrate various regularization concepts for a regression problem we will use the Ames, IA housing data, where our intent is to predict Sale_Price. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 3.4.1 glmnet The glmnet package is a fast implementation, but it requires some extra processing up-front to your data if it’s not already represented as a numeric matrix. glmnet does not use the formula method (y ~ x) so prior to modeling we need to create our feature and target set. Furthermore, we use the model.matrix function on our feature set (see Matrix::sparse.model.matrix for increased efficiency on large dimension data). We also Box Cox transform our response variable due to its skeweness. The Box Cox transformation of the response variable is not required; however, parametric models such as regularized regression are sensitive to skewed values so it is always recommended to normalize your response variable. # Create training and testing feature matrices # we use model.matrix(...)[, -1] to discard the intercept train_x &lt;- model.matrix(Sale_Price ~ ., ames_train)[, -1] test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1] # Create training and testing response vectors # transform y based on skewness of training data train_y &lt;- log(ames_train$Sale_Price) test_y &lt;- log(ames_test$Sale_Price) 3.4.1.1 Basic implementation To apply a regularized model we can use the glmnet::glmnet function. The alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)) model. Behind the scenes, glmnet is doing two things that you should be aware of: It is essential that predictor variables are standardized when performing regularized regression. glmnet performs this for you. If you standardize your predictors prior to glmnet you can turn this argument off with standardize = FALSE. glmnet will perform ridge models across a wide range of \\(\\lambda\\) parameters, which are illustrated in the figure below. # Apply Ridge regression to attrition data ridge &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.3: Coefficients for our ridge regression model as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). In fact, we can see the exact \\(\\lambda\\) values applied with ridge$lambda. Although you can specify your own \\(\\lambda\\) values, by default glmnet applies 100 \\(\\lambda\\) values that are data derived. glmnet has built-in functions to auto-generate the appropriate \\(\\lambda\\) values based on the data so the vast majority of the time you will have little need to adjust the default \\(\\lambda\\) values. We can also directly access the coefficients for a model using coef. glmnet stores all the coefficients for each model in order of largest to smallest \\(\\lambda\\). Due to the number of features, here I just peak at the two largest coefficients (Latitude &amp; Overall_QualVery_Excellent) features for the largest \\(\\lambda\\) (279.1035) and smallest \\(\\lambda\\) (0.02791035). You can see how the largest \\(\\lambda\\) value has pushed these coefficients to nearly 0. # lambdas applied to penalty parameter ridge$lambda %&gt;% head() ## [1] 279.1035 254.3087 231.7166 211.1316 192.3752 175.2851 # small lambda results in large coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 100] ## Latitude Overall_QualVery_Excellent ## 0.60585376 0.09800466 # large lambda results in small coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 1] ## Latitude Overall_QualVery_Excellent ## 6.228028e-36 9.372514e-37 However, at this point, we do not understand how much improvement we are experiencing in our loss function across various \\(\\lambda\\) values. 3.4.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation (CV). cv.glmnet provides a built-in option to perform k-fold CV, and by default, performs 10-fold CV. Here we perform a CV glmnet model for both a ridge and lasso penalty. By default, cv.glmnet uses MSE as the loss function but you can also use mean absolute error by changing the type.measure argument. # Apply CV Ridge regression to attrition data ridge &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 0 ) # Apply CV Ridge regression to attrition data lasso &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 1 ) # plot results par(mfrow = c(1, 2)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.4: 10-fold cross validation MSE for a ridge and lasso model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. Our plots above illustrate the 10-fold CV mean squared error (MSE) across the \\(\\lambda\\) values. In both models we see a slight improvement in the MSE as our penalty \\(log(\\lambda)\\) gets larger , suggesting that a regular OLS model likely overfits our data. But as we constrain it further (continue to increase the penalty), our MSE starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases. The first and second vertical dashed lines represent the \\(\\lambda\\) value with the minimum MSE and the largest \\(\\lambda\\) value within one standard error of the minimum MSE. # Ridge model min(ridge$cvm) # minimum MSE ## [1] 0.02147691 ridge$lambda.min # lambda for this min MSE ## [1] 0.1236602 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min MSE ## [1] 0.02488411 ridge$lambda.1se # lambda for this MSE ## [1] 0.6599372 # Lasso model min(lasso$cvm) # minimum MSE ## [1] 0.02411134 lasso$lambda.min # lambda for this min MSE ## [1] 0.003865266 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min MSE ## [1] 0.02819356 lasso$lambda.1se # lambda for this MSE ## [1] 0.01560415 We can assess this visually. Here we plot the coefficients across the \\(\\lambda\\) values and the dashed red line represents the \\(\\lambda\\) with the smallest MSE and the dashed blue line represents largest \\(\\lambda\\) that falls within one standard error of the minimum MSE. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy. Above, we saw that both ridge and lasso penalties provide similiar MSEs; however, these plots illustrate that ridge is still using all 299 variables whereas the lasso model can get a similar MSE by reducing our feature set from 299 down to 131. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 63 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 63 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor. # Ridge model ridge_min &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) # Lasso model lasso_min &lt;- glmnet( x = train_x, y = train_y, alpha = 1 ) par(mfrow = c(1, 2)) # plot ridge model plot(ridge_min, xvar = &quot;lambda&quot;, main = &quot;Ridge penalty\\n\\n&quot;) abline(v = log(ridge$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(ridge$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) # plot lasso model plot(lasso_min, xvar = &quot;lambda&quot;, main = &quot;Lasso penalty\\n\\n&quot;) abline(v = log(lasso$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(lasso$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Figure 3.5: Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. So far we’ve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas alpha \\(\\rightarrow 0\\) will have a heavier ridge penalty applied and alpha \\(\\rightarrow 1\\) will have a heavier lasso penalty. lasso &lt;- glmnet(train_x, train_y, alpha = 1.0) elastic1 &lt;- glmnet(train_x, train_y, alpha = 0.25) elastic2 &lt;- glmnet(train_x, train_y, alpha = 0.75) ridge &lt;- glmnet(train_x, train_y, alpha = 0.0) par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1) plot(lasso, xvar = &quot;lambda&quot;, main = &quot;Lasso (Alpha = 1)\\n\\n\\n&quot;) plot(elastic1, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .25)\\n\\n\\n&quot;) plot(elastic2, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .75)\\n\\n\\n&quot;) plot(ridge, xvar = &quot;lambda&quot;, main = &quot;Ridge (Alpha = 0)\\n\\n\\n&quot;) Figure 3.6: Coefficients for various penalty parameters. Often, the optimal model contains an alpha somewhere between 0-1, thus we want to tune both the \\(\\lambda\\) and the alpha parameters. To set up our tuning, we create a common fold_id, which just allows us to apply the same CV folds to each model. We then create a tuning grid that searches across a range of alphas from 0-1, and empty columns where we’ll dump our model results into. Use caution when including \\(\\alpha = 0\\) or \\(\\alpha = 1\\) in the grid search. \\(\\alpha = 0\\) will produce a dense solution and it can be very slow (or even impossible) to compute in large N situations. \\(\\alpha = 1\\) has no \\(\\ell_2\\) penalty, so it is therefore less numerically stable and can be very slow as well due to slower convergence. If you experience slow computation, I recommend searching across \\(\\alpha\\) values of 0.1, .25, .5, .75, .9. # maintain the same folds across all models fold_id &lt;- sample(1:10, size = length(train_y), replace = TRUE) # search across a range of alphas tuning_grid &lt;- tibble::tibble( alpha = seq(0, 1, by = .1), mse_min = NA, mse_1se = NA, lambda_min = NA, lambda_1se = NA ) Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error MSE values and their respective \\(\\lambda\\) values. This grid search took 41 seconds to compute. # perform grid search for(i in seq_along(tuning_grid$alpha)) { # fit CV model for each alpha value fit &lt;- cv.glmnet(train_x, train_y, alpha = tuning_grid$alpha[i], foldid = fold_id) # extract MSE and lambda values tuning_grid$mse_min[i] &lt;- fit$cvm[fit$lambda == fit$lambda.min] tuning_grid$mse_1se[i] &lt;- fit$cvm[fit$lambda == fit$lambda.1se] tuning_grid$lambda_min[i] &lt;- fit$lambda.min tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se } tuning_grid ## # A tibble: 11 x 5 ## alpha mse_min mse_1se lambda_min lambda_1se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0230 0.0267 0.179 0.795 ## 2 0.1 0.0237 0.0285 0.0387 0.156 ## 3 0.2 0.0241 0.0289 0.0193 0.0856 ## 4 0.3 0.0243 0.0295 0.0129 0.0627 ## 5 0.4 0.0245 0.0295 0.00966 0.0470 ## 6 0.5 0.0246 0.0295 0.00773 0.0376 ## 7 0.6 0.0247 0.0301 0.00644 0.0344 ## 8 0.7 0.0247 0.0302 0.00552 0.0295 ## 9 0.8 0.0247 0.0302 0.00483 0.0258 ## 10 0.9 0.0247 0.0302 0.00429 0.0229 ## 11 1 0.0248 0.0304 0.00387 0.0206 If we plot the MSE ± one standard error for the optimal \\(\\lambda\\) value for each alpha setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model (alpha = 1) with \\(\\lambda = 0.003521887\\), gain the benefits of its feature selection capability and reasonably assume no loss in accuracy. tuning_grid %&gt;% mutate(se = mse_1se - mse_min) %&gt;% ggplot(aes(alpha, mse_min)) + geom_line(size = 2) + geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) Figure 3.7: MSE ± one standard error for different alpha penalty parameters. 3.4.1.3 Visual interpretation Regularized regression assumes a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. This constant change is given by the given coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. Consequently, this makes understanding variable relationships simple with regularized models. And since the predictors have all been standardized, comparing the coefficients against one another allows us to identify the most influential predictors. Those predictors with the largest positive coefficients have the largest positive impact on our response variable and those variables with really small negative coefficients have a very small negative impact on our response. Furthermore, there is no difference between the global and local model interpretation (see model interpretation chapter for details). We’ll illustrate with the following models. lasso &lt;- cv.glmnet(train_x, train_y, alpha = 1) ridge &lt;- cv.glmnet(train_x, train_y, alpha = 0) Our ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection but it will typically push most variables to near zero and allow the most influential variables to be more prominent. The following extracts the coefficients of our ridge model and plots the top 100 most influential variables. You can see that many of the variables have coefficients closer to zero but there a handful of predictor variables that have noticable influence. For example, properties with above average latitude (northern end of Ames), are in the Green Hills neighborhood (which is actually in the south part of Ames), or have 2 extra miscellaneous garage features will have a positive influence on the sales price. Alternatively, properties that are zoned agricultural, have a home functional code of “Sal” (salvage only) or “Sev” (severely damaged), or have a pool quality assessment of “Good” tend have a negative influence on the sales price. Keep in mind these coefficients are for the standardized variable values. Consequently, the interpretation of these coefficient values are not as clear. Basically, for every unit the Latitude variable is above the mean value, the reponse variable (which has been log transformed) has a 0.474 unit increase. The important insight is to identify those variables with the largest positive and negative impacts on the response variable. coef(ridge, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% top_n(100, wt = abs(value)) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Top 100 influential variables (ridge penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.8: A ridge penalty will retain all variables but push most coefficients to near zero. Consequently, we retain any minor signals that all features provide but we can visualize those coefficients with the largest absolute values to identify the most influential predictors. Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those features most influential to predictive accuracy. If we select the model with the minimum MSE and plot all variables in that model we see similar results to the ridge regarding the most influential variables. Although the ordering typically differs, we often see some commonality between lasso and ridge models regarding the most influential variables. coef(lasso, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (lasso penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.9: A lasso penalty will perform feature selection by pushing most coefficients to zero. Consequently, we can view all coefficients to see which features were selected; however, our objective usually is still to identify those features with the strongest signal (largest absolute coefficient values). 3.4.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. The only caveat is you need to supply predict an s parameter with the preferred models \\(\\lambda\\) value. For example, here we create a lasso model, which provides me a minimum MSE of 0.02398 (RMSE = 0.123). However, our response variable is log transformed so we must re-transform it to get an interpretable RMSE (our average generalized error is $25,156.77). # optimal model cv_lasso &lt;- cv.glmnet(train_x, train_y, alpha = 1.0) min(cv_lasso$cvm) ## [1] 0.02529035 # predict and get RMSE pred &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x) caret::RMSE(pred, test_y) ## [1] 0.1220084 # re-transform predicted values and get interpretable RMSE caret::RMSE(exp(pred), exp(test_y)) ## [1] 24740.36 3.4.2 h2o To perform regularized regression with h2o, we first need to initiate our h2o session. ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpypM9G9/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpypM9G9/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 250 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 1 month and 29 days ## H2O cluster name: H2O_started_from_R_bradboehmke_fmw129 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.0 (2018-04-23) Next, we do not need to one-hot encode or standardize our variables as h2o will do this for us. However, we do want to normalize our response variable due to its skewness and then convert our training and test data to h2o objects. # convert training data to h2o object train_h2o &lt;- ames_train %&gt;% mutate(Sale_Price = log(Sale_Price)) %&gt;% as.h2o() # convert test data to h2o object test_h2o &lt;- ames_test %&gt;% mutate(Sale_Price = log(Sale_Price)) %&gt;% as.h2o() # set the response column to Sale_Price response &lt;- &quot;Sale_Price&quot; # set the predictor names predictors &lt;- setdiff(colnames(ames_train), response) 3.4.2.1 Basic implementation h2o.glm allows us to perform a generalized linear model. If the response variable is continuous, h2o.glm will use a gaussian distribution (see family parameter ?h2o.glm). By default, h2o.glm performs an elastic net model with alpha = .5. Similar to glmnet, h2o.glm will perform an automated search across internally generated lambda values. You can override the automated lambda search by supplying different values to the lambda parameters in h2o.glm but this is not recommended as the default parameters typically perform best. The following performs a default h2o.glm model with alpha = .5 and it performs a 10 fold cross validation (nfolds = 10). # train your model, where you specify alpha (performs 10-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;gaussian&quot; ) # print the MSE and RMSE for the validation data h2o.mse(h2o_fit1, xval = TRUE) ## [1] 0.0405933 h2o.rmse(h2o_fit1, xval = TRUE) ## [1] 0.2014778 If we check out the summary results of our model we get a bunch of information. Below is truncated printout which provides important model information such as the alpha applied and the optimal lambda value identified (\\(\\lambda = 0.056\\)), the number of predictors retained with these penalty parameters (11), and performance results for the training and validation sets. summary(h2o_fit1) ## Model Details: ## ============== ## ## H2ORegressionModel: glm ## Model Key: GLM_model_R_1531935157122_1 ## GLM Model: summary ## family link regularization number_of_predictors_total number_of_active_predictors number_of_iterations #training_frame ## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05581 ) 345 11 2 file13d1f4ffcacf3_sid_be66_16 ## ## H2ORegressionMetrics: glm ## ** Reported on training data. ** ## ## MSE: 0.03800294 ## RMSE: 0.1949434 ## MAE: 0.1277845 ## RMSLE: 0.01521899 ## Mean Residual Deviance : 0.03800294 ## R^2 : 0.7737851 ## Null Deviance :345.0615 ## Null D.o.F. :2053 ## Residual Deviance :78.05805 ## Residual D.o.F. :2042 ## AIC :-861.7687 ## ## ## ## H2ORegressionMetrics: glm ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.0405933 ## RMSE: 0.2014778 ## MAE: 0.1310502 ## RMSLE: 0.01567771 ## Mean Residual Deviance : 0.0405933 ## R^2 : 0.7583658 ## Null Deviance :345.4348 ## Null D.o.F. :2053 ## Residual Deviance :83.37864 ## Residual D.o.F. :2042 ## AIC :-726.3293 ## ## truncated..... ## 3.4.2.2 Tuning As previously stated, a full grid search to identify the optimal alpha is not always necessary; changing its value to 0.5 (or 0 or 1 if we only want Ridge or Lasso, respectively) works in most cases. However, if a full grid search is desired then we need to supply our grid of alpha values in a list. We can then use h2o.grid to perform our grid search. The results show that \\(\\alpha = 0\\) (a full ridge penalty) performed best. This grid search took 5 seconds to compute. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .1)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, grid_id = &quot;grid_search&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search ## Used hyper parameters: ## - alpha ## Number of models: 11 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## alpha model_ids mse ## 1 [0.0] grid_search_model_0 0.022711429277773212 ## 2 [0.6] grid_search_model_6 0.0395992947950626 ## 3 [0.7] grid_search_model_7 0.03968294882574923 ## 4 [1.0] grid_search_model_10 0.03983076501706063 ## 5 [0.4] grid_search_model_4 0.039850863900647 ## 6 [0.8] grid_search_model_8 0.03990607242146858 ## 7 [0.3] grid_search_model_3 0.04019054535587161 ## 8 [0.5] grid_search_model_5 0.04042328742741519 ## 9 [0.9] grid_search_model_9 0.04044324344598643 ## 10 [0.2] grid_search_model_2 0.041434990130949347 ## 11 [0.1] grid_search_model_1 0.04233746428242222 We can check out more details of the best performing model. Our RMSE (0.1279) is an improvement on our default model (RMSE = 0.2015). Also, we can access the optimal model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0279. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model) ## [1] 0.0163625 h2o.rmse(best_model) ## [1] 0.127916 # get optimal parameters best_model@parameters$lambda ## [1] 0.02790355 best_model@parameters$alpha ## [1] 0 3.4.2.3 Visual interpretation h2o provides a built-in function that plots variable importance. To compute variable importance for regularized models, h2o uses the standardized coefficient values (which is the same that we plotted in the glmnet example). You will notice that the the largest influential variables produced by this H2O model differ from the glmnet model. This is because we are assessing the ridge model here, where in the glmnet interpretation section we assessed the lasso and H2O and glmnet use differing approaches to generate the \\(\\lambda\\) search path. However, you will notice that both Overall_Qual.Excellent and Overall_Cond.Fair were also top influencers in the glmnet model suggesting they may have a strong signal regardless of the regularization penalty we use. # get top 25 influential variables h2o.varimp_plot(best_model, num_of_features = 25) Figure 3.10: H2O’s variable importance plot. Provides the same output as plotting the standardized coefficients (h2o.std_coef_plot). Although H2O’s variable importance uses standardized coefficients, a convenient function of H2O is that you can extract the “normal” coefficients which are obtained from the standardized coefficients by reversing the data standardization process (de-scaled, with the intercept adjusted by an added offset) so that they can be applied to data in its original form (i.e. no standardization prior to scoring). For example, every one unit increase in pool size has a $1 decrease in sales price (since our response is log transformed we need to take the exponent \\(e^{-0.000183} = 0.999817\\)). These are not the same as coefficients of a model built on non-standardized data. best_model@model$coefficients_table ## Coefficients: glm coefficients ## names coefficients standardized_coefficients ## 1 Intercept -47.473537 11.713031 ## 2 Neighborhood.Bloomington_Heights -0.019249 -0.019249 ## 3 Neighborhood.Blueste 0.004638 0.004638 ## 4 Neighborhood.Briardale -0.009169 -0.009169 ## 5 Neighborhood.Brookside 0.008538 0.008538 ## ## --- ## names coefficients standardized_coefficients ## 341 Pool_Area -0.000183 -0.005963 ## 342 Misc_Val -0.000038 -0.022608 ## 343 Mo_Sold 0.000444 0.001224 ## 344 Year_Sold -0.007414 -0.009728 ## 345 Longitude -0.303402 -0.007786 ## 346 Latitude 0.948544 0.017592 In the glmnet section we discussed how regularized regression assumes a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. This constant change is given by the given coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. We can illustrate this with a partial dependence plot of the ground living area (square footage) variable. The PDP plot shows that the relationship is monotonic linear (assumes a constant increasing relationships). This PDP plot helps to show the typical values (and one standard error) of our response variable as the square footage of the ground floor living space increases. # partial dependence plots for top 2 influential variables h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Gr_Liv_Area&quot;) Figure 3.11: As the ground living area (square footage) of a home increases, we experience a constant increase in the mean predicted sale price. But what about the two most influential variables (Overall_Qual.Excellent and Overall_Cond.Fair)? These variables are a result of one-hot encoding the original overall quality (Overall_Qual) variable. We can assess a similar plot but must supply the original non-one-hot encoded variable name. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Overall_Qual&quot;) Figure 3.12: The mean predicted sale price for each level of the overall quality variable. Note how the plot does not align with the natural ordering of the predictor variable. Unfortunately, H2O’s function plots the categorical levels in alphabetical order. Alternatively, we can extract the results and plot them in their proper level order to make inference more intuitive. The following shows the marginal effect of the overall quality variable on sales price. It illustrates an interesting finding - the highest and lowest categories do not have the largest marginal sales price effects. It also shows that a house with a very good overall quality has, on average, a $6K higher sale price than a house with only a good overall quality. Alternatively, homes with below average quality receive a substantially lower sale price than homes with average quality. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Overall_Qual&quot;, plot = FALSE) %&gt;% as.data.frame() %&gt;% mutate( Overall_Qual = factor(Overall_Qual, levels = levels(ames$Overall_Qual)), mean_response = exp(mean_response)) %&gt;% ggplot(aes(mean_response, Overall_Qual)) + geom_point() + scale_x_continuous(labels = scales::dollar) + ggtitle(&quot;Average response for each Overall Quality level&quot;) Figure 3.13: The mean predicted sale price for each level of the overall quality variable. This plot now helps to illustrate how the mean predicted sale price changes based on the natural ordering of the overall quality predictor variable. See the model interpretation chapter for more details on variable importance and partial dependence plots. 3.4.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Similar to glmnet, we need to re-transform our predicted values to get a interpretable generalizable error. Our generalizable error is $24,147.60 which is about $1,000 lower than the glmnet model produced. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict ## 1 11.65765 ## 2 11.48417 ## 3 12.50938 ## 4 12.32368 ## 5 13.20056 ## 6 12.69091 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2ORegressionMetrics: glm ## ## MSE: 0.01364287 ## RMSE: 0.1168027 ## MAE: 0.0826363 ## RMSLE: 0.009030871 ## Mean Residual Deviance : 0.01364287 ## R^2 : 0.915491 ## Null Deviance :141.57 ## Null D.o.F. :875 ## Residual Deviance :11.95116 ## Residual D.o.F. :530 ## AIC :-582.0349 # convert predicted values to non-transformed caret::RMSE(as.vector(exp(pred)), ames_test$Sale_Price) ## [1] 24147.6 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) ## [1] TRUE 3.5 Implementation: Binary Classification To illustrate various regularization concepts for a binary classification problem we will use the employee attrition data, where the goal is to predict whether or not an employee attrits (“Yes” vs. “No”). The easiest way to have consistent interpretable results is to recode the response as 1 for the positive class (“Yes”) and 0 for the other class (“No”). attrition &lt;- rsample::attrition %&gt;% mutate(Attrition = recode(Attrition, &quot;Yes&quot; = 1, &quot;No&quot; = 0)) %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) 3.5.1 glmnet Similar to the regression application, for the classification data set we need to perfom some extra processing up-front to prepare for modeling with glmnet. First, many of the features are categorical; consequently, we need to either ordinal encode or one-hot encode these variables so that all features are numeric. Also, glmnet does not use the formula method (y ~ x) so prior to modeling we need to create our feature and target set and convert our features to a matrix. Since our response variable is imbalanced, we use rsample and strat to perform stratified sampling so that both our training and testing data sets have similar proportion of response levels. # one-hot encode our data with model.matrix one_hot &lt;- model.matrix( ~ ., attrition)[, -1] %&gt;% as.data.frame() # Create training and testing sets set.seed(123) split &lt;- initial_split(one_hot, prop = .8, strata = &quot;Attrition&quot;) train &lt;- training(split) test &lt;- testing(split) # separate features from response variable for glmnet train_x &lt;- train %&gt;% select(-Attrition) %&gt;% as.matrix() train_y &lt;- train$Attrition test_x &lt;- test %&gt;% select(-Attrition) %&gt;% as.matrix() test_y &lt;- test$Attrition # check that train &amp; test sets have common response ratio table(train_y) %&gt;% prop.table() ## train_y ## 0 1 ## 0.8385726 0.1614274 table(test_y) %&gt;% prop.table() ## test_y ## 0 1 ## 0.8395904 0.1604096 3.5.1.1 Basic implementation Similar to the regression problem, we apply a regularized classification model with glmnet::glmnet(). The primary difference is that for binary classification models we need to include family = binomial. Remember, the alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)) model. # Apply Ridge regression to attrition data ridge &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.14: Coefficients for our ridge regression model as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). We can also directly access the coefficients for a model using coef and tidy the output with tidy. Here, we check out the top 10 largest absolute coefficient terms when using the smallest and largest lambda values. We see that regardless of a large or small penalty parameter, working overtime and being a Sales Rep has the largest positive influence on the probability of attrition. Whereas being a Research Director and having high job involvement (among others) are the strongest influencers for reducing the probability of attrition. # lambdas applied to penalty parameter ridge$lambda %&gt;% head() ## [1] 86.49535 78.81134 71.80996 65.43056 59.61789 54.32160 # small lambda results in large coefficients coef(ridge)[, 100] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 58 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 OverTimeYes 1.68 ## 2 JobInvolvementVery_High -1.42 ## 3 JobRoleSales_Representative 1.35 ## 4 (Intercept) 1.21 ## 5 BusinessTravelTravel_Frequently 1.17 ## 6 JobRoleResearch_Director -1.05 ## 7 JobInvolvementHigh -0.974 ## 8 EnvironmentSatisfactionVery_High -0.951 ## 9 JobSatisfactionVery_High -0.932 ## 10 WorkLifeBalanceBetter -0.908 ## # ... with 48 more rows # large lambda results in small coefficients coef(ridge)[, 1] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 58 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -1.65e+ 0 ## 2 JobRoleSales_Representative 2.62e-37 ## 3 OverTimeYes 1.95e-37 ## 4 JobRoleResearch_Director -1.55e-37 ## 5 MaritalStatusSingle 1.39e-37 ## 6 JobRoleManager -1.24e-37 ## 7 BusinessTravelTravel_Frequently 1.16e-37 ## 8 JobRoleLaboratory_Technician 9.16e-38 ## 9 JobRoleManufacturing_Director -8.95e-38 ## 10 JobInvolvementVery_High -7.66e-38 ## # ... with 48 more rows However, at this point, we do not understand how much improvement we are experiencing in our loss function across various \\(\\lambda\\) values. 3.5.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation with cv.glmnet. Here we perform a 10-fold CV glmnet model for both a ridge and lasso penalty. By default, cv.glmnet uses deviance as the loss function for binomial classification but you could adjust type.measure to “auc”, “mse”, “mae”, or “class” (missclassification). # Apply CV Ridge regression to attrition data ridge &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) # Apply CV Ridge regression to attrition data lasso &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 1 ) # plot results par(mfrow = c(1, 2)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.15: 10-fold cross validation deviance for a ridge and lasso model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest deviance and the second represents the \\(\\lambda\\) with a deviance within one standard error of the minimum deviance. Our plots above illustrate the 10-fold CV deviance across the \\(\\lambda\\) values. With the ridge model, we don’t see any improvement in the loss function as \\(\\lambda\\) increases but, with the lasso model, we see a slight improvement in the deviance as our penalty \\(\\lambda\\) gets larger, suggesting that a regular logistic regression model likely overfits our data. But as we constrain it further (continue to increase the penalty), our deviance starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases, with the optimal model containing between 38-48 predictor variables. The first and second vertical dashed lines represent the \\(\\lambda\\) value with the minimum deviance and the largest \\(\\lambda\\) value within one standard error of the minimum deviance. # Ridge model min(ridge$cvm) # minimum deviance ## [1] 0.6483054 ridge$lambda.min # lambda for this min deviance ## [1] 0.0104184 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min deviance ## [1] 0.6788463 ridge$lambda.1se # lambda for this deviance ## [1] 0.04615997 # Lasso model min(lasso$cvm) # minimum deviance ## [1] 0.6511349 lasso$lambda.min # lambda for this min deviance ## [1] 0.001737893 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min deviance ## [1] 0.6737481 lasso$lambda.1se # lambda for this deviance ## [1] 0.005307275 We can assess this visually. Here we plot the coefficients across the \\(\\lambda\\) values and the dashed red line represents the \\(\\lambda\\) with the smallest deviance and the dashed blue line represents largest \\(\\lambda\\) that falls within one standard error of the minimum deviance. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy. Above, we saw that both ridge and lasso penalties provide similiar minimum deviance scores; however, these plots illustrate that ridge is still using all 57 variables whereas the lasso model can get a similar deviance by reducing our feature set from 57 down to 48. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 38 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 38 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor. # Ridge model ridge_min &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) # Lasso model lasso_min &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 1 ) par(mfrow = c(1, 2)) # plot ridge model plot(ridge_min, xvar = &quot;lambda&quot;, main = &quot;Ridge penalty\\n\\n&quot;) abline(v = log(ridge$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(ridge$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) # plot lasso model plot(lasso_min, xvar = &quot;lambda&quot;, main = &quot;Lasso penalty\\n\\n&quot;) abline(v = log(lasso$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(lasso$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Figure 3.16: Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. So far we’ve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas alpha \\(\\rightarrow 0\\) will have a heavier ridge penalty applied and alpha \\(\\rightarrow 1\\) will have a heavier lasso penalty. lasso &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1.0) elastic1 &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.25) elastic2 &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.75) ridge &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.0) par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1) plot(lasso, xvar = &quot;lambda&quot;, main = &quot;Lasso (Alpha = 1)\\n\\n\\n&quot;) plot(elastic1, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .25)\\n\\n\\n&quot;) plot(elastic2, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .75)\\n\\n\\n&quot;) plot(ridge, xvar = &quot;lambda&quot;, main = &quot;Ridge (Alpha = 0)\\n\\n\\n&quot;) Figure 3.17: Coefficients for various penalty parameters. Often, the optimal model contains an alpha somewhere between 0-1, thus we want to tune both the \\(\\lambda\\) and the alpha parameters. As we did in the regression section, we create a common fold_id, which just allows us to apply the same CV folds to each model. We then create a tuning grid that searches across a range of alphas from 0-1, and empty columns where we’ll dump our model results into. Use caution when including \\(\\alpha = 0\\) or \\(\\alpha = 1\\) in the grid search. \\(\\alpha = 0\\) will produce a dense solution and it can be very slow (or even impossible) to compute in large N situations. \\(\\alpha = 1\\) has no \\(\\ell_2\\) penalty, so it is therefore less numerically stable and can be very slow as well due to slower convergence. If you experience slow computation, we recommend searching across \\(\\alpha\\) values of 0.1, .25, .5, .75, .9. # maintain the same folds across all models fold_id &lt;- sample(1:10, size = length(train_y), replace=TRUE) # search across a range of alphas tuning_grid &lt;- tibble::tibble( alpha = seq(0, 1, by = .1), dev_min = NA, dev_1se = NA, lambda_min = NA, lambda_1se = NA ) Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error deviance values and their respective \\(\\lambda\\) values. This grid search took 36 seconds. # Warning - this is not fast! See H2O section for faster approach for(i in seq_along(tuning_grid$alpha)) { # fit CV model for each alpha value fit &lt;- cv.glmnet( train_x, train_y, family = &quot;binomial&quot;, alpha = tuning_grid$alpha[i], foldid = fold_id ) # extract MSE and lambda values tuning_grid$dev_min[i] &lt;- fit$cvm[fit$lambda == fit$lambda.min] tuning_grid$dev_1se[i] &lt;- fit$cvm[fit$lambda == fit$lambda.1se] tuning_grid$lambda_min[i] &lt;- fit$lambda.min tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se } tuning_grid ## # A tibble: 11 x 5 ## alpha dev_min dev_1se lambda_min lambda_1se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.656 0.699 0.00949 0.0735 ## 2 0.1 0.656 0.696 0.00685 0.0441 ## 3 0.2 0.655 0.696 0.00599 0.0320 ## 4 0.3 0.655 0.698 0.00481 0.0257 ## 5 0.4 0.655 0.699 0.00434 0.0211 ## 6 0.5 0.655 0.697 0.00381 0.0169 ## 7 0.6 0.655 0.699 0.00349 0.0155 ## 8 0.7 0.656 0.698 0.00299 0.0132 ## 9 0.8 0.656 0.697 0.00287 0.0116 ## 10 0.9 0.656 0.701 0.00255 0.0113 ## 11 1 0.656 0.700 0.00230 0.0102 If we plot the deviance ± one standard error for the optimal \\(\\lambda\\) value for each alpha setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model (alpha = 1) with \\(\\lambda = 0.0023\\), gain the benefits of its feature selection capability and reasonably assume no loss in accuracy. tuning_grid %&gt;% mutate(se = dev_1se - dev_min) %&gt;% ggplot(aes(alpha, dev_min)) + geom_line(size = 2) + geom_ribbon(aes(ymax = dev_min + se, ymin = dev_min - se), alpha = .25) + ggtitle(&quot;Deviance ± one standard error&quot;) Figure 3.18: MSE ± one standard error for different alpha penalty parameters. 3.5.1.3 Visual interpretation 3.5.1.3.1 Variable importance Similar to the regularized regression models, regularized classification models assume a monotonic linear relationship between the predictor variables and the response. The primary difference is in what the linear relationship means. For binary classification models, the linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the log-odds probability of the response variable. This constant change is represented by the given coefficient for a predictor. Log odds are an alternate way of expressing probabilities. The odds of a positive outcome are simply represented as \\(\\frac{prob(positive\\_outcome)}{prob(negative\\_outcome)}\\). So if there is a .2 probability of an employee attriting, the odds ratio is \\(.2 \\div .8 = .25\\). Consequently, the log odds for this employee is \\(log(.25) = -1.386294\\). An employee with 50% change of attriting has a log odds of \\(log(.5 \\div .5) = 0\\) so negative log odds means greater probability of not attriting and positive log odds means greater probability of attriting. Consequently, this makes understanding variable relationships simple with regularized models. Those variables with largest positive coefficients have the strongest influence on increasing the probability of attrition whereas those variables with the largest negative coefficients have the stongest influence on decreasing the probability of attrition. I’ll illustrate with the following models. lasso &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1) ridge &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0) Our ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection but it will typically push most variables to near zero and allow the most influential variables to be more prominent. The following extracts the coefficients of our ridge model and plots predictor coefficients. You can see that some of the variables have coefficients closer to zero but there are also many variables that have a strong positive influence on the probability of attrition (i.e. OverTimeYes, JobRoleSales_Representative, BusinessTravelTravel_Frequently) and there are others that have a strong negative influence on the probability of attrition (i.e. JobInvolvementVery_High, JobRoleResearch_Director, JobInvolvementHigh). coef(ridge, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (ridge penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.19: A ridge penalty will retain all variables but push many coefficients to near zero. Consequently, we retain any minor signals that all features provide but those coefficients with the largest absolute values represent the most influential predictors in our model. Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those features most influential to predictive accuracy. If we select the model with the minimum deviance and plot all variables in that model we see similar results to the ridge model regarding the most influential variables. However, the lasso model has pushed 9 variables to have zero coefficients and has retained the remaining 48, effectively performing automated feature selection. coef(lasso, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (lasso penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.20: A lasso penalty will perform feature selection by pushing coefficients to zero. Consequently, we can view all coefficients to see which features were selected; however, our objective usually is still to identify those features with the strongest signal (largest absolute coefficient values). 3.5.1.3.2 ROC curve We can visualize the ROC curve with the ROCR and pROC packages. Both packages compare the predicted log-odds output (pred) to the actual observed class. ROC curves become more interesting and useful when we compare multiple models, which we will see in later chapters. If you do not include legacy.axes = TRUE in the plot() call for the pROC curve, your x-axis will be reversed. library(ROCR) library(pROC) # predict pred &lt;- predict(ridge, s = ridge$lambda.min, train_x) # plot structure par(mfrow = c(1, 2)) # ROCR plot prediction(pred, train_y) %&gt;% performance(&quot;tpr&quot;, &quot;fpr&quot;) %&gt;% plot(main = &quot;ROCR ROC curve&quot;) #pROC plot roc(train_y, as.vector(pred)) %&gt;% plot(main = &quot;pROC ROC curve&quot;, legacy.axes = TRUE) 3.5.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. Two caveats: You need to supply predict an s parameter with the preferred model’s \\(\\lambda\\) value. For example, here we create a lasso model, which provides me a minimum deviance of 0.648. We use the \\(\\lambda\\) for this model by specifying s = cv_lasso$lambda.min. The default predicted values are the log odds. If you want the probability, include type = &quot;response&quot;. If you want to predict the categorical response, include type = &quot;class&quot;. # optimal model cv_lasso &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1.0) min(cv_lasso$cvm) ## [1] 0.6475805 # predict and get log-odds pred_log_odds &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x) head(pred_log_odds) ## 1 ## 2 -4.11789899 ## 3 0.18751231 ## 14 -3.26688392 ## 25 -2.17385203 ## 39 0.02202369 ## 46 -4.44305986 # predict probability pred_probs &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x, type = &quot;response&quot;) head(pred_probs) ## 1 ## 2 0.01601793 ## 3 0.54674120 ## 14 0.03672490 ## 25 0.10212328 ## 39 0.50550570 ## 46 0.01162321 # predict and get predicted class pred_class &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x, type = &quot;class&quot;) head(pred_class) ## 1 ## 2 &quot;0&quot; ## 3 &quot;1&quot; ## 14 &quot;0&quot; ## 25 &quot;0&quot; ## 39 &quot;1&quot; ## 46 &quot;0&quot; Lastly, to assess various performance metrics on our test data we can use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. We can see that the no information rate is 0.8396. This represents the ratio of non-attrition to attrition rates. The goal is to increase prediction accuracy over and above this rate. We see that our overall accuracy is 0.887. The primary weakness in our model is that for many employees that attrit, we tend to predict non-attrit. This is illustrated by our low sensitivity. The positive argument allows you to specify which value corresponds to the “positive” result. This will impact how you interpret certain metrics that are based on true positive and false negative results (i.e. sensitivity, specificity). caret::confusionMatrix(factor(pred_class), factor(test_y), positive = &quot;1&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 238 25 ## 1 8 22 ## ## Accuracy : 0.8874 ## 95% CI : (0.8455, 0.9212) ## No Information Rate : 0.8396 ## P-Value [Acc &gt; NIR] : 0.013017 ## ## Kappa : 0.5102 ## Mcnemar&#39;s Test P-Value : 0.005349 ## ## Sensitivity : 0.46809 ## Specificity : 0.96748 ## Pos Pred Value : 0.73333 ## Neg Pred Value : 0.90494 ## Prevalence : 0.16041 ## Detection Rate : 0.07509 ## Detection Prevalence : 0.10239 ## Balanced Accuracy : 0.71778 ## ## &#39;Positive&#39; Class : 1 ## 3.5.2 h2o To perform regularized logistic regression with h2o, we first need to initiate our h2o session. h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 14 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 1 month and 29 days ## H2O cluster name: H2O_started_from_R_bradboehmke_fmw129 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.0 (2018-04-23) Next, we do not need to one-hot encode or standardize our variables as h2o will do this for us. However, we do need to convert our training and test data to h2o objects. # Create training and testing sets set.seed(123) split &lt;- initial_split(attrition, prop = .8, strata = &quot;Attrition&quot;) train &lt;- training(split) test &lt;- testing(split) # convert training data to h2o object train_h2o &lt;- as.h2o(train) # convert test data to h2o object test_h2o &lt;- as.h2o(test) # set the response column to Attrition response &lt;- &quot;Attrition&quot; # set the predictor names predictors &lt;- setdiff(colnames(train), &quot;Attrition&quot;) 3.5.2.1 Basic implementation Similar to our regression problem, we use h2o.glm to perform a regularized logistic regression model. The primary difference is that we need to set family = &quot;binomial&quot; to signal a binary classification problem. As before, by default, h2o.glm performs an elastic net model with alpha = .5 and will perform an automated search across internally generated lambda values. The following performs a default h2o.glm model with alpha = .5 and it performs a 10 fold cross validation (nfolds = 10). # train your model, where you specify alpha (performs 10-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;binomial&quot; ) # print the MSE and AUC for the validation data h2o.mse(h2o_fit1, xval = TRUE) ## [1] 0.09739988 h2o.auc(h2o_fit1, xval = TRUE) ## [1] 0.8345038 We can check out the cross-validated performance results of our model with h2o.performance. You can also get more results information using summary(h2o_fit1). h2o.performance(h2o_fit1, xval = TRUE) ## H2OBinomialMetrics: glm ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.09739988 ## RMSE: 0.3120895 ## LogLoss: 0.3347975 ## Mean Per-Class Error: 0.258244 ## AUC: 0.8345038 ## Gini: 0.6690076 ## R^2: 0.2804837 ## Residual Deviance: 788.1133 ## AIC: 914.1133 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 898 89 0.090172 =89/987 ## 1 81 109 0.426316 =81/190 ## Totals 979 198 0.144435 =170/1177 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.351656 0.561856 156 ## 2 max f2 0.127301 0.631939 267 ## 3 max f0point5 0.497019 0.608696 105 ## 4 max accuracy 0.728176 0.875106 54 ## 5 max precision 0.989718 1.000000 0 ## 6 max recall 0.001056 1.000000 396 ## 7 max specificity 0.989718 1.000000 0 ## 8 max absolute_mcc 0.497019 0.478350 105 ## 9 max min_per_class_accuracy 0.140575 0.757852 257 ## 10 max mean_per_class_accuracy 0.234064 0.762571 209 3.5.2.2 Tuning Next, we’ll use h2o.grid to perform our grid search. The results show that \\(\\alpha = .1\\) performed best; however, the improvement is marginal. This grid search took 7 seconds. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .1)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, family = &quot;binomial&quot;, grid_id = &quot;grid_search_glm_classification&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search_glm_classification&quot;, sort_by = &quot;logloss&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search_glm_classification ## Used hyper parameters: ## - alpha ## Number of models: 11 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## alpha model_ids logloss ## 1 [0.1] grid_search_glm_classification_model_1 0.32729191697214505 ## 2 [0.4] grid_search_glm_classification_model_4 0.3308752559420129 ## 3 [0.2] grid_search_glm_classification_model_2 0.33248215525410824 ## 4 [0.8] grid_search_glm_classification_model_8 0.33249327956862135 ## 5 [0.7] grid_search_glm_classification_model_7 0.3348307314321876 ## 6 [0.5] grid_search_glm_classification_model_5 0.3354479575686281 ## 7 [1.0] grid_search_glm_classification_model_10 0.33563334658821187 ## 8 [0.3] grid_search_glm_classification_model_3 0.33779138839826356 ## 9 [0.0] grid_search_glm_classification_model_0 0.338497245432312 ## 10 [0.6] grid_search_glm_classification_model_6 0.33925009415446955 ## 11 [0.9] grid_search_glm_classification_model_9 0.33927152577597053 We can check out more details of the best performing model. Our AUC (.84) is no better than our default cross-validated model. We can also extract other model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0006. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model, xval = TRUE) ## [1] 0.09538328 h2o.auc(best_model, xval = TRUE) ## [1] 0.8384072 # get optimal parameters best_model@parameters$lambda ## [1] 0.0006111815 best_model@parameters$alpha ## [1] 0.1 3.5.2.3 Visual Interpretation 3.5.2.3.1 Variable importance To identify the most influential variables we can use h2o’s variable importance plot. Recall that for a GLM model, variable importance is simply represented by the standardized coefficients. We see that JobInvolvement.Low and JobRole.Sales_Representative have the largest influence in increasing the probability of attrition whereas JobRole.Research_Director and OverTime.No have the largest influence in decreasing the probability of attrition. # get top 25 influential variables h2o.varimp_plot(best_model, num_of_features = 25) Figure 3.21: H2O’s variable importance plot. Provides the same output as plotting the standardized coefficients (h2o.std_coef_plot). To illustrate how the predicted response changes based on these influential variables, we can leverage the partial dependence inforamtion. For example, we can assess the partial dependence plots of the JobInvolvement predictor which shows up at the top of our variable importance plot. We see that mean predicted probability of attrition increases as job involvement decreases; but we can also see the significant increase in probability when an employee has a low job involvement. # partial dependence plots for top 2 influential variables h2o.partialPlot(best_model, data = train_h2o, cols = &quot;JobInvolvement&quot;, plot = FALSE) %&gt;% as.data.frame() %&gt;% mutate(JobInvolvement = factor(JobInvolvement, levels = levels(attrition$JobInvolvement))) %&gt;% ggplot(aes(JobInvolvement, mean_response)) + geom_col() + ggtitle(&quot;Average predicted probability of attrition&quot;) Figure 3.22: There is a significant increase in the predicted probability of attrition as an employee’s level of job involvement decreases to a low level. Similarly, for a continuous variable, we can assess the PDP. For example, age is one of the few continuous predictor variables in this data set and we see that our regularized logistic regression model predicts a continuously decreasing probability of attrition as employees get older. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Age&quot;) Figure 3.23: As an employee gets older, the predicted probability of attrition decreases. 3.5.2.3.2 ROC curve Earlier, we saw that this model produced a 10-fold CV AUC of .84. We can visualize this by plotting the ROC curve using the following: h2o.performance(best_model, xval = TRUE) %&gt;% plot() Figure 3.24: ROC curve for our best performing regularized H2O GLM model. 3.5.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Note how the h2o.predict function provides 3 columns - the predicted class and the probability of each class. We also produce our test set performance results with h2o.performance. If you compare the confusion matrix with the one produced by glmnet you will notice they produce very similar results. The h2o model does a slightly better job predicting true attrition but also does slightly worse in false positives. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict p0 p1 ## 1 0 0.9921441 0.007855913 ## 2 1 0.4230720 0.576927982 ## 3 0 0.9724310 0.027569028 ## 4 0 0.9093784 0.090621612 ## 5 1 0.4451610 0.554838982 ## 6 0 0.9863366 0.013663436 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2OBinomialMetrics: glm ## ## MSE: 0.0936628 ## RMSE: 0.3060438 ## LogLoss: 0.328802 ## Mean Per-Class Error: 0.2543678 ## AUC: 0.8418959 ## Gini: 0.6837917 ## R^2: 0.3045444 ## Residual Deviance: 192.678 ## AIC: 324.678 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 236 10 0.040650 =10/246 ## 1 22 25 0.468085 =22/47 ## Totals 258 35 0.109215 =32/293 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.499766 0.609756 34 ## 2 max f2 0.215378 0.690299 79 ## 3 max f0point5 0.600664 0.677419 26 ## 4 max accuracy 0.600664 0.890785 26 ## 5 max precision 0.991619 1.000000 0 ## 6 max recall 0.003581 1.000000 274 ## 7 max specificity 0.991619 1.000000 0 ## 8 max absolute_mcc 0.499766 0.555889 34 ## 9 max min_per_class_accuracy 0.215378 0.787234 79 ## 10 max mean_per_class_accuracy 0.215378 0.806219 79 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) 3.6 Implementation: Multinomial Classification To illustrate various regularization concepts for a multinomial classification problem we will use the mnist data, where the goal is to predict handwritten numbers ranging from 0-9. # import mnist training and testing data train &lt;- data.table::fread(&quot;../data/mnist_train.csv&quot;, data.table = FALSE) test &lt;- data.table::fread(&quot;../data/mnist_test.csv&quot;, data.table = FALSE) 3.6.1 glmnet Since the mnist data contains all numeric predictors (darkness density ranging from 0-255), we do not need to one-hot encode our feature set. Consequently, we only need to convert our features into a matrix and seperate the response variable (V785). For a multinomial problem our response needs to be either discrete integer values or set as a factor. Since our response values are discrete integer values from 0-9 we can leave them as is. # separate features from response variable for glmnet train_x &lt;- train %&gt;% select(-V785) %&gt;% as.matrix() train_y &lt;- train$V785 test_x &lt;- test %&gt;% select(-V785) %&gt;% as.matrix() test_y &lt;- test$V785 # check the response ratios across the train &amp; test sets table(train_y) %&gt;% prop.table() %&gt;% round(2) ## train_y ## 0 1 2 3 4 5 6 7 8 9 ## 0.10 0.11 0.10 0.10 0.10 0.09 0.10 0.10 0.10 0.10 table(test_y) %&gt;% prop.table() %&gt;% round(2) ## test_y ## 0 1 2 3 4 5 6 7 8 9 ## 0.10 0.11 0.10 0.10 0.10 0.09 0.10 0.10 0.10 0.10 3.6.1.1 Basic implementation To perform a regularized multinomial GLM, we simply change the family parameter to “multinomial”. In this example we perform a full ridge penalty (alpha = 0) model. One difference with a multinomial model is that when you plot the model results, rather than seeing only one plot with the coefficient values across the spectrum of \\(\\lambda\\) values, you will get an individual plot for the coefficients for each response category. # Apply Ridge regression to mnist data ridge &lt;- glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 0 ) par(mfrow = c(2, 5)) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.25: Coefficients for each multinomial response as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Also, the coefficients are stored in a list separated by the response category. We can access these coefficients in a similar manner as we did the regression and binary classification coefficients; however, we need to index for the response category of interest. For example, the following gets the coefficients for response category “9” (note the indexing: coef(ridge)$9). # coefficients for response &quot;9&quot; with small lambda penalty coef(ridge)$`9`[, 100] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 785 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;&quot; -0.428 ## 2 V170 -0.0762 ## 3 V753 0.0505 ## 4 V703 0.0345 ## 5 V780 -0.0317 ## 6 V732 0.0264 ## 7 V726 0.0246 ## 8 V505 0.0220 ## 9 V421 -0.0147 ## 10 V225 -0.0139 ## # ... with 775 more rows # coefficients for response &quot;9&quot; with small lambda penalty coef(ridge)$`9`[, 1] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 785 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;&quot; -7.12e- 3 ## 2 V753 3.18e-38 ## 3 V732 2.39e-38 ## 4 V170 -1.69e-38 ## 5 V16 -1.11e-38 ## 6 V703 8.13e-39 ## 7 V533 -6.68e-39 ## 8 V505 6.46e-39 ## 9 V33 -6.26e-39 ## 10 V752 4.64e-39 ## # ... with 775 more rows 3.6.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation with cv.glmnet as we did with the regression and binary classification examples. However, due to the magnitude of the data slowing glmnet down, rather than perform a full grid search across many alpha settings we only perform a 5-fold CV glmnet model for three alpha values: 1 (ridge), 0.5 (elastic net), and 2 (lasso). As your data set increases, glmnet begins to slow down considerably compared to h2o. Consequently, I reduced k to a 5-fold CV; however, to run a single 5-fold CV on this training set still took 109 minutes! If you parallelize the process with parallel = TRUE (which requires you to use doMC or some other parallelizer) you can achieve some speed improvements. # parallelize the process library(doMC) registerDoMC(cores = 4) # Apply CV Ridge regression to mnist data ridge &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 0, nfolds = 5, parallel = TRUE ) # Apply CV elastic net regression to mnist data elastic &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = .5, nfolds = 5, parallel = TRUE ) # Apply CV lasso regression to mnist data lasso &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 1, nfolds = 5, parallel = TRUE ) # plot results par(mfrow = c(1, 3)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(elastic, main = &quot;Elastic net penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.26: 5-fold cross validation deviance for a ridge, lasso, and elastic net model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest deviance and the second represents the \\(\\lambda\\) with a deviance within one standard error of the minimum deviance. Note how the top of the ridge penalty deviance plot indicates only 717 of the 784 predictors are in the model. The reason for this is not because the ridge model pushed 67 coefficients to zero because a ridge model does not do that. Rather, there are 67 predictors that have zero variance so they are not included in the model (see this with preProcess(train_x, “zv”)). In the performance plots in Figure 3.26 it is difficult to see if the models differ in their minimum error rate (multinomial deviance). But if we look at the minimum deviance (and the largest deviance within one standard error) in the below code chunk, we see that the elastic net provides the optimal performance. Consequently, it appears we could use between 250-300 of the 784 predictors and still achieve optimal performance. # Ridge model min(ridge$cvm) # minimum deviance ## [1] 0.6194693 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min deviance ## [1] 0.6194693 # Elastic net model min(elastic$cvm) # minimum deviance ## [1] 0.5627915 elastic$cvm[elastic$lambda == elastic$lambda.1se] # 1 st.error of min deviance ## [1] 0.5682305 # Lasso model min(lasso$cvm) # minimum deviance ## [1] 0.6557272 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min deviance ## [1] 0.6557272 So the elastic net model is minimizing the multinomial deviance loss function, but how does this translate to the accuracy of this model? We can assess the confusion matrix for this data on the training data. At first glance, the confusion matrix is difficult to discern differences. However, looking at the class statistics and specifically the sensitivity and specificity we can extract some useful insights. First, the specificity is 0.99 across all numbers indicating that the model does well in classifying non-events (basically, no number has a significantly higher false positive rate than the other numbers). Second, looking at the sensitivity, we can see that our model does the best at accurately predicting the numbers 0, 1, and 6. However, it does worst at accurately predicting the numbers 2, 3, 5, 8, and 9. The number 8 has the lowest sensitivity rate and when we look at the confusion matrix we can see that our model often classifies the number 8 as the numbers 5, 3, and 1. pred_class &lt;- predict(elastic, s = elastic$lambda.min, train_x, type = &quot;class&quot;) caret::confusionMatrix(factor(pred_class), factor(test_y)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 5780 1 26 16 11 43 27 10 28 19 ## 1 1 6580 48 28 27 20 15 25 115 25 ## 2 16 33 5452 126 24 38 33 59 56 14 ## 3 8 15 84 5547 9 145 0 19 125 78 ## 4 10 6 65 8 5506 50 30 47 26 135 ## 5 27 23 19 181 9 4882 69 10 129 36 ## 6 34 3 57 17 45 80 5714 3 36 2 ## 7 5 13 67 47 13 16 2 5911 15 157 ## 8 36 58 120 114 31 109 26 15 5251 41 ## 9 6 10 20 47 167 38 2 166 70 5442 ## ## Overall Statistics ## ## Accuracy : 0.9344 ## 95% CI : (0.9324, 0.9364) ## No Information Rate : 0.1124 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9271 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Statistics by Class: ## ## Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 0.97586 0.9760 0.91507 0.90475 0.94249 0.90057 0.96553 0.94350 0.89745 0.91478 ## Specificity 0.99665 0.9943 0.99262 0.99103 0.99304 0.99078 0.99488 0.99377 0.98984 0.99027 ## Pos Pred Value 0.96964 0.9558 0.93181 0.91990 0.93592 0.90659 0.95376 0.94637 0.90519 0.91186 ## Neg Pred Value 0.99735 0.9970 0.99066 0.98918 0.99379 0.99013 0.99622 0.99341 0.98893 0.99062 ## Prevalence 0.09872 0.1124 0.09930 0.10218 0.09737 0.09035 0.09863 0.10442 0.09752 0.09915 ## Detection Rate 0.09633 0.1097 0.09087 0.09245 0.09177 0.08137 0.09523 0.09852 0.08752 0.09070 ## Detection Prevalence 0.09935 0.1147 0.09752 0.10050 0.09805 0.08975 0.09985 0.10410 0.09668 0.09947 ## Balanced Accuracy 0.98625 0.9851 0.95384 0.94789 0.96776 0.94568 0.98020 0.96863 0.94365 0.95252 3.6.1.3 Visual interpretation Interpreting the underlying predictor mechanisms with a multinomial problem is much like the regression and binary classification problems but with a few extra nuances. The first thing to remember is that although we started with 784 predictors, our full ridge model only used 717 because 67 predictors had zero variance. This is not unique to multinomial problems but its important to understand which variables these are because, in an organizational situation, we can assess whether or not we should continue collecting this information. In this example, they primarily represent pixels along the very edge of the images. # zero variance predictor variables that offer no potential signal names(which(sapply(train, var) == 0)) ## [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; &quot;V5&quot; &quot;V6&quot; &quot;V7&quot; &quot;V8&quot; &quot;V9&quot; &quot;V10&quot; &quot;V11&quot; ## [12] &quot;V12&quot; &quot;V17&quot; &quot;V18&quot; &quot;V19&quot; &quot;V20&quot; &quot;V21&quot; &quot;V22&quot; &quot;V23&quot; &quot;V24&quot; &quot;V25&quot; &quot;V26&quot; ## [23] &quot;V27&quot; &quot;V28&quot; &quot;V29&quot; &quot;V30&quot; &quot;V31&quot; &quot;V32&quot; &quot;V53&quot; &quot;V54&quot; &quot;V55&quot; &quot;V56&quot; &quot;V57&quot; ## [34] &quot;V58&quot; &quot;V83&quot; &quot;V84&quot; &quot;V85&quot; &quot;V86&quot; &quot;V112&quot; &quot;V113&quot; &quot;V141&quot; &quot;V142&quot; &quot;V169&quot; &quot;V477&quot; ## [45] &quot;V561&quot; &quot;V645&quot; &quot;V646&quot; &quot;V672&quot; &quot;V673&quot; &quot;V674&quot; &quot;V700&quot; &quot;V701&quot; &quot;V702&quot; &quot;V728&quot; &quot;V729&quot; ## [56] &quot;V730&quot; &quot;V731&quot; &quot;V755&quot; &quot;V756&quot; &quot;V757&quot; &quot;V758&quot; &quot;V759&quot; &quot;V760&quot; &quot;V781&quot; &quot;V782&quot; &quot;V783&quot; ## [67] &quot;V784&quot; However, in a multinomial problem, each response category will not always use all the predictors that have variance. In fact, the below code shows that the number zero only uses 189 predictors whereas the number 3 uses 296. You can set type.multinomial = “grouped” within cv.glmnet() to force all predictors to be in or out together. coef(elastic) %&gt;% purrr::map(tidy) %&gt;% bind_rows(.id = &quot;response&quot;) %&gt;% count(response) ## # A tibble: 10 x 2 ## Response n ## &lt;chr&gt; &lt;int&gt; ## 1 0 189 ## 2 1 206 ## 3 2 281 ## 4 3 296 ## 5 4 272 ## 6 5 264 ## 7 6 267 ## 8 7 278 ## 9 8 228 ## 10 9 283 Moreover, each response category will use the predictors in different ways. Similar to the binary classification problem, the coefficients represent the change in log-odds probability of the response variable for a one unit change in the predictor. For example, below we see that for the response category “0”, a one unit increase in the darkness of the pixel represented by feature V41 causes a 0.0089 log odds increase that the response will be the number “0”. A 0.0089 log odds increase translates to \\(\\frac{e^{0.008912499}}{1+e^{0.008912499}} = 0.5022281\\) probability increase. The below code chunk extracts the coefficients for each response category, combines them all into a single data frame, and removes the intercept (since we just care about the predictor coefficients). # tidy up the coefficients for downstream assessment vi &lt;- coef(elastic) %&gt;% purrr::map(tidy) %&gt;% bind_rows(.id = &quot;response&quot;) %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% select(response, feature = row, coefficient = value) head(vi) ## response feature coefficient ## 1 0 V41 8.912499e-03 ## 2 0 V45 8.816906e-03 ## 3 0 V60 8.381852e-02 ## 4 0 V72 -2.083206e-04 ## 5 0 V106 -1.630429e-03 ## 6 0 V124 1.720361e-05 Similar to the binary classification problem, one of our main concerns is to understand which predictors have the largest influence on each response category. The following identifies the top 10 predictors with the largest absolute coefficients. These predictors represent those that have the largest impact to the probability of that response. For example, as features V60, V225, and V504 increase (those pixels become darker), they have the largest increase in the probability that the response will be “0”. Alternatively, as features V719 and V363 increase, they have the largest decrease in the probability that the response will be “0”. Since a given predictor can have a different coefficient for each response category, each response category can have their own unique variable importance list. vi %&gt;% group_by(response) %&gt;% top_n(10, wt = abs(coefficient)) %&gt;% mutate(predictor = paste(response, feature, sep = &quot;: &quot;)) %&gt;% ggplot(aes(coefficient, reorder(predictor, coefficient), color = coefficient &gt; 0)) + geom_point(show.legend = FALSE) + facet_wrap(~ response, scales = &quot;free_y&quot;, ncol = 5) + ylab(NULL) Figure 3.27: Top 10 most influential predictors for each multinomial response. Predictors with coefficients to the right of zero (blue) increase the probability of that response whereas predictors with coefficients to the left of zero (red) decrease the probability of that response. The above plot identifies thse variables most influential for each given response category. However, we also want to understand which variables are influential across all, or most, of the responses. The below identifies three predictors (V375, V515, V572) that have non-zero coefficients for all the response categories. Plotting these variables allows us to see how the strength and direction of the signal varies across the response categories. These features represent pixels in the images that are used by each handwritten number. # predictors that are influential across all or many of the responses vi %&gt;% count(feature) %&gt;% arrange(desc(n)) ## # A tibble: 653 x 2 ## feature n ## &lt;chr&gt; &lt;int&gt; ## 1 V375 10 ## 2 V515 10 ## 3 V572 10 ## 4 V402 9 ## 5 V239 8 ## 6 V268 8 ## 7 V301 8 ## 8 V324 8 ## 9 V326 8 ## 10 V353 8 ## # ... with 643 more rows vi %&gt;% filter(feature %in% c(&quot;V375&quot;, &quot;V515&quot;, &quot;V572&quot;)) %&gt;% ggplot(aes(coefficient, feature, fill = coefficient &gt; 0)) + geom_col() + coord_flip() + facet_wrap(~ response, ncol = 5) Figure 3.28: Three variables provide signals for all the response categories; however, the strength and direction of the signal can vary across the responses. Similarly, certain predictors may only provide a signal for only one response category. We also want to assess these as they provide a unique signal for a particular response. For example, feature V170 is only influential for response “3” and increases the probability by 0.525. # only 82 predictors that play a role in just one response singles &lt;- vi %&gt;% count(feature) %&gt;% filter(n == 1) %&gt;% .$feature vi %&gt;% filter(feature %in% singles) %&gt;% arrange(desc(abs(coefficient))) %&gt;% slice(1:10) %&gt;% mutate(prob = exp(coefficient) / (1 + exp(coefficient))) ## response feature coefficient prob ## 1 3 V170 0.10146013 0.5253433 ## 2 0 V60 0.08381852 0.5209424 ## 3 0 V225 0.07995005 0.5199769 ## 4 7 V780 0.05972853 0.5149277 ## 5 7 V534 0.03223708 0.5080586 ## 6 7 V778 0.02278233 0.5056953 ## 7 2 V392 0.02252209 0.5056303 ## 8 2 V82 0.02225574 0.5055637 ## 9 8 V335 0.01925485 0.5048136 ## 10 6 V88 0.01859690 0.5046491 Remember that these linear models assume a monotonic linear relationship between the predictors and the response; meaning that for a given response category, the relationship represented by the coefficient is constant. Therefore, global and local model interpreation will be the same. See more in the machine learning interpretability chapter. 3.6.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. Similar to the binary classification problem: You need to supply predict an s parameter with the preferred model’s \\(\\lambda\\) value. The default predicted values are the log odds. If you want the probability, include type = &quot;response&quot;. If you want to predict the categorical response, include type = &quot;class&quot;. Additionally, the predicted output is in the form of an array. Consequently, to assess the predicted values for the first five observations across all response categories, index with the following: # predict and get log-odds pred_log_odds &lt;- predict(elastic, s = elastic$lambda.min, test_x) pred_log_odds[1:5, , 1] ## 0 1 2 3 4 5 6 7 8 9 ## [1,] -5.432659 -1.339061 -2.333135 0.7305298 -3.606102 0.5498658 -6.628586 -6.8034856 6.3175820 0.353073 ## [2,] -8.302420 -5.464651 -3.410668 7.5492386 -4.806773 3.3335312 -4.941261 -0.1618101 0.7612956 2.373736 ## [3,] 7.266914 -8.224763 1.052633 1.6484586 -8.055438 -9.2497608 2.820177 -12.5006341 8.8967193 -10.473515 ## [4,] 8.415969 -10.700918 1.233072 -5.6888200 -2.364087 -0.6202491 5.928128 -7.3944607 0.2065292 -5.634506 ## [5,] -6.395731 7.228976 2.144250 -0.1299773 -4.381965 -2.0272380 -2.086397 -1.8958485 1.1838882 -2.138867 # predict probability pred_probs &lt;- predict(elastic, s = elastic$lambda.min, test_x, type = &quot;response&quot;) pred_probs[1:5, , 1] ## 0 1 2 3 4 5 6 7 8 9 ## [1,] 7.808164e-06 4.681405e-04 0.0001732428 3.708412e-03 4.850791e-05 3.095470e-03 2.361373e-06 1.982470e-06 0.9899515761 2.542498e-03 ## [2,] 1.277183e-07 2.181128e-06 0.0000170104 9.784562e-01 4.211081e-06 1.444385e-02 3.681174e-06 4.382031e-04 0.0011029991 5.531581e-03 ## [3,] 1.633925e-01 3.056908e-08 0.0003268920 5.931549e-04 3.620925e-08 1.096811e-08 1.914428e-03 4.249083e-10 0.8337729101 3.225981e-09 ## [4,] 9.222861e-01 4.597347e-09 0.0007004437 6.906115e-07 1.919319e-05 1.097706e-04 7.663202e-02 1.254537e-07 0.0002509293 7.291585e-07 ## [5,] 1.198730e-06 9.905039e-01 0.0061317946 6.308170e-04 8.980257e-06 9.460923e-05 8.917459e-05 1.078935e-04 0.0023469721 8.461622e-05 # predict and get predicted class pred_class &lt;- predict(elastic, s = elastic$lambda.min, test_x, type = &quot;class&quot;) head(pred_class) ## 1 ## [1,] &quot;8&quot; ## [2,] &quot;3&quot; ## [3,] &quot;8&quot; ## [4,] &quot;0&quot; ## [5,] &quot;1&quot; ## [6,] &quot;5&quot; Lastly, to assess various performance metrics on our test data we can use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. We can see that the overall accuracy rate is 0.9281 and our model does well predicting “0”, “1”, and “6” but poorly predicting “2”, “3”, “5”, “8”, and “9”. caret::confusionMatrix(factor(pred_class), factor(test_y)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 959 0 7 3 1 8 10 2 6 10 ## 1 0 1114 8 1 1 2 3 9 7 8 ## 2 1 2 929 16 4 2 4 22 6 1 ## 3 1 2 15 921 1 33 2 5 21 10 ## 4 0 0 7 0 919 11 7 5 10 25 ## 5 7 1 3 29 0 779 14 0 24 5 ## 6 6 4 12 2 12 14 913 0 9 0 ## 7 5 2 11 10 6 8 3 954 10 22 ## 8 1 10 36 20 8 31 2 1 871 6 ## 9 0 0 4 8 30 4 0 30 10 922 ## ## Overall Statistics ## ## Accuracy : 0.9281 ## 95% CI : (0.9229, 0.9331) ## No Information Rate : 0.1135 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9201 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 0.9786 0.9815 0.9002 0.9119 0.9358 0.8733 0.9530 0.9280 0.8943 0.9138 ## Specificity 0.9948 0.9956 0.9935 0.9900 0.9928 0.9909 0.9935 0.9914 0.9873 0.9904 ## Pos Pred Value 0.9533 0.9662 0.9412 0.9110 0.9339 0.9037 0.9393 0.9253 0.8834 0.9147 ## Neg Pred Value 0.9977 0.9976 0.9886 0.9901 0.9930 0.9876 0.9950 0.9917 0.9886 0.9903 ## Prevalence 0.0980 0.1135 0.1032 0.1010 0.0982 0.0892 0.0958 0.1028 0.0974 0.1009 ## Detection Rate 0.0959 0.1114 0.0929 0.0921 0.0919 0.0779 0.0913 0.0954 0.0871 0.0922 ## Detection Prevalence 0.1006 0.1153 0.0987 0.1011 0.0984 0.0862 0.0972 0.1031 0.0986 0.1008 ## Balanced Accuracy 0.9867 0.9885 0.9469 0.9509 0.9643 0.9321 0.9733 0.9597 0.9408 0.9521 3.6.2 h2o To perform regularized multinomial logistic regression with h2o, we first need to initiate our h2o session. h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 545 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 1 month and 29 days ## H2O cluster name: H2O_started_from_R_bradboehmke_fmw129 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.0 (2018-04-23) Since our data is already split into a training test set, to prepare for modeling, we just need to convert our training and test data to h2o objects and identify the response and predictor variables. One key difference compared to prior classification procedures, to perform a multinomial modeling with h2o we need to convert the response variable to a factor. # convert training data to h2o object train_h2o &lt;- train %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # convert test data to h2o object test_h2o &lt;- test %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # set the response column to V785 response &lt;- &quot;V785&quot; # set the predictor names predictors &lt;- setdiff(colnames(train), response) 3.6.2.1 Basic implementation Similar to our regression and binary classification problem, we use h2o.glm to perform a regularized multinomial logistic regression model. The primary difference is that we need to set family = &quot;multinomial&quot; to signal a multinomial classification problem. As before, by default, h2o.glm performs an elastic net model with alpha = .5 and will perform an automated search across internally generated lambda values. The following performs a default multinomial h2o.glm model with alpha = .5 and it performs a 5 fold cross validation (nfolds = 10). When you are working with large data sets, h2o provides a far more efficient approach for regularized models. Whereas glmnet took over an hour to perform a 5-fold cross validated regularized model on the mnist data, h2o took less than 2.5 minutes! # train your model, where you specify alpha (performs 5-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 5, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;multinomial&quot; ) We can check out the cross-validated performance results of our model with h2o.performance. You can also get more results information using summary(h2o_fit1). One unique output worth discussing is the “Top-10 Hit Ratios” table. The first line of this table (k = 1, hit_ratio = 0.9213) represents the mean accuracy of our model across the 5-fold validated set. However, the second line tells us that when we take those missed predictions and use the second highest predicted probability we get a mean accuracy of 96.85% (we get an additional \\((0.9685 - 0.9213) \\times 60000 = 2832\\) observations correct.) After only 4 reapplications we are able to achieve 99% accuracy. h2o.performance(h2o_fit1, xval = TRUE) ## H2OMultinomialMetrics: glm ## ** Reported on cross-validation data. ** ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## Cross-Validation Set Metrics: ## ===================== ## ## Extract cross-validation frame with `h2o.getFrame(&quot;filea18f736ccac9_sid_8725_4&quot;)` ## MSE: (Extract with `h2o.mse`) 0.07378137 ## RMSE: (Extract with `h2o.rmse`) 0.2716273 ## Logloss: (Extract with `h2o.logloss`) 0.2843358 ## Mean Per-Class Error: 0.07972688 ## Null Deviance: (Extract with `h2o.nulldeviance`) 276156.9 ## Residual Deviance: (Extract with `h2o.residual_deviance`) 34193.91 ## R^2: (Extract with `h2o.r2`) 0.9911615 ## AIC: (Extract with `h2o.aic`) NaN ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,xval = TRUE)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.921300 ## 2 2 0.968500 ## 3 3 0.984300 ## 4 4 0.991517 ## 5 5 0.995133 ## 6 6 0.997200 ## 7 7 0.998400 ## 8 8 0.999183 ## 9 9 0.999800 ## 10 10 1.000000 3.6.2.2 Tuning Since h2o is much faster than glmnet, we can perform a grid search across a wider range of alpha parameters. Here, I assess alphas equal to 0, 0.25, 0.5, 0.75, and 1 using h2o.grid to perform our grid search. The results show that \\(\\alpha = .25\\) performed best. The results also show that 4 out of 5 models have minor differences in the Log Loss loss function; however, \\(\\alpha = .0\\) (full ridge penalty) definitely performs the worst. This grid search took 20 minutes. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .25)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, family = &quot;multinomial&quot;, grid_id = &quot;grid_search_glm_multinomial&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search_glm_multinomial&quot;, sort_by = &quot;logloss&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search_glm_multinomial ## Used hyper parameters: ## - alpha ## Number of models: 5 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## alpha model_ids logloss ## 1 [0.25] grid_search_glm_multinomial_model_1 0.2826176758945207 ## 2 [1.0] grid_search_glm_multinomial_model_4 0.2833413952701668 ## 3 [0.75] grid_search_glm_multinomial_model_3 0.2834683548256082 ## 4 [0.5] grid_search_glm_multinomial_model_2 0.2855257887768779 ## 5 [0.0] grid_search_glm_multinomial_model_0 0.31518212134903734 We can check out more details of the best performing model. Our AUC (.84) is no better than our default cross-validated model. We can also extract other model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0006. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model, xval = TRUE) ## [1] 0.0733229 h2o.rmse(best_model, xval = TRUE) ## [1] 0.270782 # get optimal parameters best_model@parameters$lambda ## [1] 0.000673668 best_model@parameters$alpha ## [1] 0.25 We can also assess the confusion matrix for our optimal model. Our overall accuracy is 0.9146 (\\(1-0.0854\\)), which is slightly less than the mean 5-fold CV accuracy produced by the glmnet model (0.9344). Similar to our glmnet results, our optimal model is doing a good job predicting “0”, “1”, and “6”, but is poorly predicting “2”, “3”, “5”, and “8”. h2o.confusionMatrix(best_model) ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 5776 1 14 9 10 28 36 6 38 5 0.0248 = 147 / 5,923 ## 1 1 6581 32 15 6 26 3 14 55 9 0.0239 = 161 / 6,742 ## 2 28 53 5449 85 64 16 57 67 121 18 0.0854 = 509 / 5,958 ## 3 18 31 119 5553 7 180 16 49 109 49 0.0943 = 578 / 6,131 ## 4 11 34 24 8 5504 7 44 12 30 168 0.0579 = 338 / 5,842 ## 5 50 32 31 146 52 4855 78 17 121 39 0.1044 = 566 / 5,421 ## 6 31 18 31 0 35 62 5710 4 25 2 0.0351 = 208 / 5,918 ## 7 11 35 58 16 46 8 4 5905 12 170 0.0575 = 360 / 6,265 ## 8 33 151 51 128 26 134 34 15 5211 68 0.1094 = 640 / 5,851 ## 9 25 27 11 76 142 30 3 156 38 5441 0.0854 = 508 / 5,949 ## Totals 5984 6963 5820 6036 5892 5346 5985 6245 5760 5969 0.0669 = 4,015 / 60,000 3.6.2.3 Visual Interpretation Similar to glmnet, we can extract useful information from our coefficients to interpret influential variables in our predictors. First, we need to do a little clean up of our coefficient table. The coefficient table provides both the coefficient estimate and the standard error; however, what follows only focuses on the coefficient estimate. # clean up coefficient information vi &lt;- best_model@model$coefficients_table %&gt;% as.data.frame() %&gt;% tidyr::gather(response, coefficient, -names) %&gt;% filter( names != &quot;Intercept&quot;, !(stringr::str_detect(response, &quot;std&quot;))) %&gt;% mutate(response = stringr::str_remove(response, &quot;coefs_class_&quot;)) head(vi) ## names response coefficient ## 1 V13 0 1.637796e-04 ## 2 V14 0 2.447977e-05 ## 3 V15 0 1.202897e-04 ## 4 V16 0 2.886954e-03 ## 5 V33 0 1.270802e-03 ## 6 V34 0 3.842376e-04 Unlike glmnet, h2o forces all predictors to be either in or out across all response categories (there are options to remove some of the collinear columns; however, this will still produce a consistent number of predictors across all response categories). Consequently, we see that all 717 features are present for each response. count(vi, response) ## # A tibble: 10 x 2 ## response n ## &lt;chr&gt; &lt;int&gt; ## 1 0 717 ## 2 1 717 ## 3 2 717 ## 4 3 717 ## 5 4 717 ## 6 5 717 ## 7 6 717 ## 8 7 717 ## 9 8 717 ## 10 9 717 Similar to glment, we can use this information to identify the top 10 predictors with the largest absolute coefficients for each response category. These predictors represent those that have the largest impact to the probability of that response. Comparing to the glmnet variable importance plots earlier, features V60, V225, and V504 are the top 3 features that have the largest increase in the probability that the response will be “0” (although their ordering differs slightly). However, the features that have the largest decrease in the probability of response “0” differ from before. This may be a result of the different penalty parameter that this optimal model has compared to what we used with glmnet (\\(\\alpha=0.25\\) versus \\(\\alpha=0.5\\)). vi %&gt;% group_by(response) %&gt;% top_n(10, wt = abs(coefficient)) %&gt;% mutate(predictor = paste(response, names, sep = &quot;: &quot;)) %&gt;% ggplot(aes(coefficient, reorder(predictor, coefficient), color = coefficient &gt; 0)) + geom_point(show.legend = FALSE) + facet_wrap(~ response, scales = &quot;free_y&quot;, ncol = 5) + ylab(NULL) Figure 3.29: Top 10 most influential predictors for each multinomial response. Predictors with coefficients to the right of zero (blue) increase the probability of that response whereas predictors with coefficients to the left of zero (red) decrease the probability of that response. Remember that these linear models assume a monotonic linear relationship between the predictors and the response; meaning that for a given response category, the relationship represented by the coefficient is constant. Therefore, global and local model interpreation will be the same. See more in the machine learning interpretability chapter. 3.6.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Note how the h2o.predict function provides 11 columns - the predicted class and the probability of each class. We also produce our test set performance results with h2o.performance. If you compare the confusion matrix with the one produced by glmnet you will notice they produce very similar results. The h2o model has an overall accuracy of 0.9254 (\\(1-0.0746\\)) whereas the glmnet model achieved an accuracy of 0.9281 on the test set. And both models have about the same level of accuracy across individual response categories. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 ## 1 8 1.202141e-05 5.972906e-04 1.974938e-04 3.407596e-03 7.913304e-05 2.536697e-03 1.125671e-05 5.718130e-06 0.9905660926 2.586700e-03 ## 2 3 3.049209e-07 2.484003e-05 5.767582e-06 9.805161e-01 6.366074e-06 1.372732e-02 8.326104e-06 4.850814e-04 0.0009518292 4.274033e-03 ## 3 8 4.521685e-01 1.335448e-07 6.741594e-04 7.897412e-05 1.444127e-07 8.704872e-07 3.896986e-03 5.145982e-09 0.5431802637 3.238420e-09 ## 4 0 8.901589e-01 4.027749e-07 9.102679e-04 5.754058e-07 3.002048e-05 1.979771e-04 1.081991e-01 1.176814e-06 0.0005003709 1.131257e-06 ## 5 1 7.789492e-06 9.859295e-01 9.676005e-03 8.276910e-04 1.749899e-05 1.464323e-04 2.368448e-04 1.720208e-04 0.0029125347 7.370373e-05 ## 6 5 1.597256e-06 2.661786e-08 1.713028e-08 8.928673e-11 4.543192e-05 9.399342e-01 1.337817e-06 3.112696e-07 0.0600170128 2.307191e-08 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2OMultinomialMetrics: glm ## ## Test Set Metrics: ## ===================== ## ## MSE: (Extract with `h2o.mse`) 0.06887707 ## RMSE: (Extract with `h2o.rmse`) 0.2624444 ## Logloss: (Extract with `h2o.logloss`) 0.2682622 ## Mean Per-Class Error: 0.07570879 ## Null Deviance: (Extract with `h2o.nulldeviance`) 46020.38 ## Residual Deviance: (Extract with `h2o.residual_deviance`) 5365.243 ## R^2: (Extract with `h2o.r2`) 0.9917859 ## AIC: (Extract with `h2o.aic`) NaN ## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 963 0 0 1 0 5 5 4 2 0 0.0173 = 17 / 980 ## 1 0 1112 2 2 0 1 4 2 12 0 0.0203 = 23 / 1,135 ## 2 7 9 926 17 9 3 13 9 36 3 0.1027 = 106 / 1,032 ## 3 4 1 17 921 1 24 2 11 23 6 0.0881 = 89 / 1,010 ## 4 1 3 3 2 919 0 14 3 7 30 0.0642 = 63 / 982 ## 5 8 2 1 33 10 774 17 12 31 4 0.1323 = 118 / 892 ## 6 11 3 6 1 7 15 910 3 2 0 0.0501 = 48 / 958 ## 7 2 9 22 6 6 0 0 951 1 31 0.0749 = 77 / 1,028 ## 8 7 12 6 19 11 24 11 12 861 11 0.1160 = 113 / 974 ## 9 10 8 1 10 28 5 0 24 6 917 0.0912 = 92 / 1,009 ## Totals 1013 1159 984 1012 991 851 976 1031 981 1002 0.0746 = 746 / 10,000 ## ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.925400 ## 2 2 0.970200 ## 3 3 0.983900 ## 4 4 0.991300 ## 5 5 0.995600 ## 6 6 0.997400 ## 7 7 0.998600 ## 8 8 0.999300 ## 9 9 0.999700 ## 10 10 1.000000 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) 3.7 Learning More This serves as an introduction to regularized regression; however, it just scrapes the surface. Regularized regression approaches have been extended to other parametric (i.e. Cox proportional hazard, poisson, support vector machines) and non-parametric (i.e. Least Angle Regression, the Bayesian Lasso, neural networks) models. The following are great resources to learn more (listed in order of complexity): Applied Predictive Modeling Practical Machine Learning with H2o Introduction to Statistical Learning The Elements of Statistical Learning Statistical Learning with Sparsity References "],
["references.html", "References", " References "]
]
