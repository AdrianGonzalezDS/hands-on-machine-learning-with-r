[
["index.html", "Hands-on Machine Learning with R Preface Who should read this Why R Structure of the book Conventions used in this book Additional resources Feedback Acknowledgments Software information", " Hands-on Machine Learning with R 2018-08-11 Preface Welcome to Hands-on Machine Learning with R. This book provides hands-on modules for many of the most common machine learning methods to include: Generalized low rank models Clustering algorithms Autoencoders Regularized models Random forests Gradient boosting machines Deep neural networks Stacking / super learner and more! You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, my motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses. For the most part, I minimize mathematical complexity when possible but also provide resources to get deeper into the details if desired. Who should read this I intend this work to be a practitioner’s guide to the machine learning process and a place where one can come to learn about the approach and to gain intuition about the many commonly used, modern, and powerful methods accepted in the machine learning community. If you are familiar with the analytic methodologies, this book may still serve as a reference for how to work with the various R packages for implementation. While an abundance of videos, blog posts, and tutorials exist online, I’ve long been frustrated by the lack of consistency, completeness, and bias towards singular packages for implementation. This is what inspired this book. This book is not meant to be an introduction to R or to programming in general; as I assume the reader has familiarity with the R language to include defining functions, managing R objects, controlling the flow of a program, and other basic tasks. If not, I would refer you to R for Data Science (Wickham and Grolemund 2016) to learn the fundamentals of data science with R such as importing, cleaning, transforming, visualizing, and exploring your data. For those looking to advance their R programming skills and knowledge of the languge, I would refer you to Advanced R (Wickham 2014). Nor is this book designed to be a deep dive into the theory and math underpinning machine learning algorithms. Several books already exist that do great justice in this arena (i.e. Elements of Statistical Learning (Friedman, Hastie, and Tibshirani 2001), Computer Age Statistical Inference (Kuhn and Johnson 2013), Deep Learning (Goodfellow et al. 2016)). Instead, this book is meant to help R users learn to use the machine learning stack within R, which includes using various R packages such as glmnet, h20, ranger, xgboost, lime, and others to effectively model and gain insight from your data. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory. While you can read this book without opening R, I highly recommend you experiment with the code examples provided throughout. Why R R has emerged over the last couple decades as a first-class tool for scientific computing tasks, and has been a consistent leader in implementing statistical methodologies for analyzing data. The usefulness of R for data science stems from the large, active, and growing ecosystem of third-party packages: tidyverse for common data analysis activities; h2o, ranger, xgboost, and others for fast and scalable machine learning; lime, pdp, DALEX, and others for machine learning interpretability; and many more tools will be mentioned throughout the pages that follow. Structure of the book Each chapter of this book focuses on a particular part of the machine learning process along with various packages to perform that process. TBD… Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following code chunks with images, which signify: Signifies a tip or suggestion Signifies a general note Signifies a warning or caution Additional resources There are many great resources available to learn about machine learning. At the end of each chapter I provide a Learn More section that lists resources that I have found extremely useful for digging deeper into the methodology and applying with code. Feedback Reader comments are greatly appreciated. To report errors or bugs please post an issue at https://github.com/bradleyboehmke/hands-on-machine-learning-with-r/issues. Acknowledgments TBD Software information An online version of this book is available at https://bradleyboehmke.github.io/hands-on-machine-learning-with-r/. The source of the book is available at https://github.com/bradleyboehmke/hands-on-machine-learning-with-r. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with the following packages and R version. All code was executed on 2013 MacBook Pro with a 2.4 GHz Intel Core i5 processor, 8 GB of memory, 1600MHz speed, and double data rate synchronous dynamic random access memory (DDR3). # packages used pkgs &lt;- c( &quot;AmesHousing&quot;, &quot;caret&quot;, &quot;data.table&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;gbm&quot;, &quot;glmnet&quot;, &quot;h2o&quot;, &quot;pdp&quot;, &quot;pROC&quot;, &quot;purrr&quot;, &quot;ranger&quot;, &quot;ROCR&quot;, &quot;rsample&quot;, &quot;vip&quot;, &quot;xgboost&quot; ) # package &amp; session info devtools::session_info(pkgs) #&gt; Session info ------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.5.1 (2018-07-02) #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; tz America/New_York #&gt; date 2018-08-11 #&gt; Packages ----------------------------------------------------------------- #&gt; package * version date source #&gt; abind 1.4-5 2016-07-21 CRAN (R 3.5.0) #&gt; AmesHousing 0.0.3 2017-12-17 CRAN (R 3.5.0) #&gt; assertthat 0.2.0 2017-04-11 CRAN (R 3.5.0) #&gt; backports 1.1.2 2017-12-13 CRAN (R 3.5.0) #&gt; BH 1.66.0-1 2018-02-13 CRAN (R 3.5.0) #&gt; bindr 0.1.1 2018-03-13 CRAN (R 3.5.0) #&gt; bindrcpp 0.2.2 2018-03-29 CRAN (R 3.5.0) #&gt; bitops 1.0-6 2013-08-17 CRAN (R 3.5.0) #&gt; broom 0.5.0 2018-07-17 CRAN (R 3.5.0) #&gt; caret 6.0-80 2018-05-26 CRAN (R 3.5.0) #&gt; caTools 1.17.1 2014-09-10 CRAN (R 3.5.0) #&gt; class 7.3-14 2015-08-30 CRAN (R 3.5.1) #&gt; cli 1.0.0 2017-11-05 CRAN (R 3.5.0) #&gt; codetools 0.2-15 2016-10-05 CRAN (R 3.5.1) #&gt; colorspace 1.3-2 2016-12-14 CRAN (R 3.5.0) #&gt; compiler 3.5.1 2018-07-05 local #&gt; crayon 1.3.4 2017-09-16 CRAN (R 3.5.0) #&gt; CVST 0.2-2 2018-05-26 CRAN (R 3.5.0) #&gt; data.table 1.11.4 2018-05-27 CRAN (R 3.5.0) #&gt; ddalpha 1.3.3 2018-04-30 CRAN (R 3.5.0) #&gt; DEoptimR 1.0-8 2016-11-19 CRAN (R 3.5.0) #&gt; dichromat 2.0-0 2013-01-24 CRAN (R 3.5.0) #&gt; digest 0.6.15 2018-01-28 CRAN (R 3.5.0) #&gt; dimRed 0.1.0 2017-05-04 CRAN (R 3.5.0) #&gt; dplyr 0.7.6 2018-06-29 cran (@0.7.6) #&gt; DRR 0.0.3 2018-01-06 CRAN (R 3.5.0) #&gt; fansi 0.2.3 2018-05-06 cran (@0.2.3) #&gt; foreach 1.4.4 2017-12-12 CRAN (R 3.5.0) #&gt; gbm 2.1.3 2017-03-21 CRAN (R 3.5.0) #&gt; gdata 2.18.0 2017-06-06 CRAN (R 3.5.0) #&gt; geometry 0.3-6 2015-09-09 CRAN (R 3.5.0) #&gt; ggplot2 3.0.0 2018-07-03 CRAN (R 3.5.0) #&gt; glmnet 2.0-16 2018-04-02 CRAN (R 3.5.0) #&gt; glue 1.3.0 2018-07-23 Github (tidyverse/glue@66de125) #&gt; gower 0.1.2 2017-02-23 CRAN (R 3.5.0) #&gt; gplots 3.0.1 2016-03-30 CRAN (R 3.5.0) #&gt; graphics * 3.5.1 2018-07-05 local #&gt; grDevices * 3.5.1 2018-07-05 local #&gt; grid 3.5.1 2018-07-05 local #&gt; gridExtra 2.3 2017-09-09 CRAN (R 3.5.0) #&gt; gtable 0.2.0 2016-02-26 CRAN (R 3.5.0) #&gt; gtools 3.5.0 2015-05-29 CRAN (R 3.5.0) #&gt; h2o 3.18.0.11 2018-05-24 CRAN (R 3.5.0) #&gt; ipred 0.9-6 2017-03-01 CRAN (R 3.5.0) #&gt; iterators 1.0.9 2017-12-12 CRAN (R 3.5.0) #&gt; jsonlite 1.5 2017-06-01 CRAN (R 3.5.0) #&gt; kernlab 0.9-26 2018-04-30 CRAN (R 3.5.0) #&gt; KernSmooth 2.23-15 2015-06-29 CRAN (R 3.5.1) #&gt; labeling 0.3 2014-08-23 CRAN (R 3.5.0) #&gt; lattice 0.20-35 2017-03-25 CRAN (R 3.5.1) #&gt; lava 1.6.1 2018-03-28 CRAN (R 3.5.0) #&gt; lazyeval 0.2.1 2017-10-29 CRAN (R 3.5.0) #&gt; lubridate 1.7.4 2018-04-11 CRAN (R 3.5.0) #&gt; magic 1.5-8 2018-01-26 CRAN (R 3.5.0) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.5.0) #&gt; MASS 7.3-50 2018-04-30 CRAN (R 3.5.1) #&gt; Matrix 1.2-14 2018-04-13 CRAN (R 3.5.1) #&gt; methods * 3.5.1 2018-07-05 local #&gt; mgcv 1.8-24 2018-06-23 CRAN (R 3.5.1) #&gt; ModelMetrics 1.1.0 2016-08-26 CRAN (R 3.5.0) #&gt; munsell 0.4.3 2016-02-13 CRAN (R 3.5.0) #&gt; nlme 3.1-137 2018-04-07 CRAN (R 3.5.1) #&gt; nnet 7.3-12 2016-02-02 CRAN (R 3.5.1) #&gt; numDeriv 2016.8-1 2016-08-27 CRAN (R 3.5.0) #&gt; parallel 3.5.1 2018-07-05 local #&gt; pdp 0.6.0 2017-07-20 CRAN (R 3.5.0) #&gt; pillar 1.3.0 2018-07-14 cran (@1.3.0) #&gt; pkgconfig 2.0.1 2017-03-21 CRAN (R 3.5.0) #&gt; plogr 0.2.0 2018-03-25 CRAN (R 3.5.0) #&gt; plyr 1.8.4 2016-06-08 CRAN (R 3.5.0) #&gt; pROC 1.12.1 2018-05-06 CRAN (R 3.5.0) #&gt; prodlim 2018.04.18 2018-04-18 CRAN (R 3.5.0) #&gt; purrr 0.2.5 2018-05-29 CRAN (R 3.5.0) #&gt; R6 2.2.2 2017-06-17 CRAN (R 3.5.0) #&gt; ranger 0.10.0 2018-05-29 CRAN (R 3.5.0) #&gt; RColorBrewer 1.1-2 2014-12-07 CRAN (R 3.5.0) #&gt; Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) #&gt; RcppEigen 0.3.3.4.0 2018-02-07 CRAN (R 3.5.0) #&gt; RcppRoll 0.2.2 2015-04-05 CRAN (R 3.5.0) #&gt; RCurl 1.95-4.10 2018-01-04 CRAN (R 3.5.0) #&gt; recipes 0.1.2 2018-01-11 CRAN (R 3.5.0) #&gt; reshape2 1.4.3 2017-12-11 CRAN (R 3.5.0) #&gt; rlang 0.2.1 2018-05-30 CRAN (R 3.5.0) #&gt; robustbase 0.93-0 2018-04-24 CRAN (R 3.5.0) #&gt; ROCR 1.0-7 2015-03-26 CRAN (R 3.5.0) #&gt; rpart 4.1-13 2018-02-23 CRAN (R 3.5.1) #&gt; rsample 0.0.2 2017-11-12 CRAN (R 3.5.0) #&gt; scales 0.5.0 2017-08-24 CRAN (R 3.5.0) #&gt; sfsmisc 1.1-2 2018-03-05 CRAN (R 3.5.0) #&gt; splines 3.5.1 2018-07-05 local #&gt; SQUAREM 2017.10-1 2017-10-07 CRAN (R 3.5.0) #&gt; stats * 3.5.1 2018-07-05 local #&gt; stats4 3.5.1 2018-07-05 local #&gt; stringi 1.2.4 2018-07-20 cran (@1.2.4) #&gt; stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) #&gt; survival 2.42-3 2018-04-16 CRAN (R 3.5.1) #&gt; tibble 1.4.2 2018-01-22 CRAN (R 3.5.0) #&gt; tidyr 0.8.1 2018-05-18 CRAN (R 3.5.0) #&gt; tidyselect 0.2.4 2018-02-26 CRAN (R 3.5.0) #&gt; timeDate 3043.102 2018-02-21 CRAN (R 3.5.0) #&gt; tools 3.5.1 2018-07-05 local #&gt; utf8 1.1.4 2018-05-24 CRAN (R 3.5.0) #&gt; utils * 3.5.1 2018-07-05 local #&gt; vip 0.1.0 2018-06-22 Github (koalaverse/vip@86b4c48) #&gt; viridis 0.5.1 2018-03-29 CRAN (R 3.5.0) #&gt; viridisLite 0.3.0 2018-02-01 CRAN (R 3.5.0) #&gt; withr 2.1.2 2018-03-15 CRAN (R 3.5.0) #&gt; xgboost 0.71.1 2018-05-16 CRAN (R 3.5.0) References "],
["intro.html", "Chapter 1 Introduction 1.1 Supervised Learning 1.2 Unsupervised Learning 1.3 Machine learning interpretability 1.4 The data sets", " Chapter 1 Introduction Machine learning continues to grow in importance for many organizations across nearly all domains. Examples include: predicting the likelihood of a patient returning to the hospital (readmission) within 30 days of discharge, segmenting customers based on common attributes or purchasing behavior for target marketing, predicting coupon redemption rates for a given marketing campaign, predicting customer churn so an organization can perform preventative intervention, and many more! In essence, these tasks all seek to learn from data. To address each scenario, we use a given set of features to train an algorithm and extract insights. These algorithms, or learners, can be classified according to the amount and type of supervision provided during training. The two main groups this book focuses on includes: supervised learners that are used to construct predictive models, and unsupervised learners that are used to build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish. 1.1 Supervised Learning A predictive model is used for tasks that involve the prediction of a given output using other variables and their values (features) in the data set. Or as stated by Kuhn and Johnson (2013), predictive modeling is “the process of developing a mathematical tool or model that generates an accurate prediction” (p. 2). The learning algorithm in a predictive model attempts to discover and model the relationship among the target response (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks, using home attributes to predict the sales price, using employee attributes to predict the likelihood of attrition, using patient attributes and symptoms to predict the risk of readmission, using production attributes to predict time to market. Each of these examples have a defined learning task. They each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this text I will use various terms interchangeably for: \\(X\\): “predictor variables”, “independent variables”, “attributes”, “features”, “predictors” \\(Y\\): “target variable”, “dependent variable”, “response”, “outcome measurement” The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the desired solutions. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Supervised learning problems revolve around two primary themes: regression and classification. 1.1.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuous numeric spectrum. In the examples above predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along the continuous spectrum. Figure 1.1 illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along the plane. Figure 1.1: Average home sales price as a function of year built and total square footage. 1.1.2 Classification problems When the objective of our supervised learning is to predict a categorical response, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: did a customer redeem a coupon (yes/no, 1/0), did a customer churn (yes/no, 1/0), did a customer click on our online ad (yes/no, 1/0), classifying customer reviews: binary: positive vs negative multinomial: extremely negative to extremely positive on a 0-5 Likert scale However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e. “yes” or “no”), we often predict the probability of a particular class (i.e. yes: .65, no: .35). Then the class with the highest probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. 1.1.3 Algorithm Comparison Guide TODO: keep this here or move reference guide to back??? Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, the supervised learning algorithms I cover in this book can be applied to both. These algorithms have become the most popular machine learning applications in recent years. Although the chapters that follow will go into detail on each algorithm, the following provides a quick reference guide that compares and contrasts some of their features. Moreover, I provide recommended base learner packages that I have found to scale well with typical rectangular data analyzed by organizations. Characteristics Regularized GLM Random Forest Gradient Boosting Machine Deep Learning Allows n &lt; p Provides automatic feature selection Handles missing values No feature pre-processing required Robust to outliers Easy to tune Computational speed Predictive power Preferred regression base learner glmnet h2o.glm ranger h2o.randomForest xgboost h2o.gbm keras h2o.deeplearning Preferred classifciation base learner glmnet h2o.glm ranger h2o.randomForest xgboost h2o.gbm keras h2o.deeplearning 1.2 Unsupervised Learning Unsupervised learning, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables. For example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical regression models break down in the presence of highly correlated features. Dimension reduction techniques provide a method to reduce the feature set to a potentially smaller set of uncorrelated variables. These variables are often used as the input variables to downstream supervised models like. Unsupervised learning is often performed as part of an exploratory data analysis. However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e. linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised. However, the importance of unsupervised learning should not be overlooked and techniques for unsupervised learning are used in organizations to: Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. Identify products that have similar purchasing behavior so that managers can manage them as product groups. These questions, and many more, can be addressed with unsupervised learning. Moreover, often the results of an unsupervised model can be used as inputs to downstream supervised learning models. 1.2.1 Algorithm Decision Guide TBD 1.3 Machine learning interpretability In his seminal 2001 paper, Leo Breiman popularized the phrase: “the multiplicity of good models.” The phrase means that for the same set of input variables and prediction targets, complex machine learning algorithms can produce multiple accurate models with very similar, but not the exact same, internal architectures. Figure 1.2 is a depiction of a non-convex error surface that is representative of the error function for a machine learning algorithm with two inputs — say, a customer’s income and a customer’s age, and an output, such as the same customer’s probability of redeeming a coupon. This non-convex error surface with no obvious global minimum implies there are many different ways complex machine learning algorithms could learn to weigh a customer’s income and age to make a good decision about if they are likely to redeem a coupon. Each of these different weightings would create a different function for making coupon redemption (and therefore marketing) decisions, and each of these different functions would have different explanations. Figure 1.2: Non-convex error surface with many local minimas. All of this is an obstacle to data scientists. On one hand, different models can have widely different predictions based on the same feature set. Even models built from the same algorithm but with different hyperparameters can lead to different results. Consequently, practitioners should understand how different implementations of algorithms differ, which can cause variance in their results (i.e. a default xgboost model can produce very different results from a default gbm model, even though they both implement gradient boosting machines). Alternatively, data scientists can experience very similar predictions from different models based on the same feature set. However, these models will have very different logic and structure leading to different interpretations. Consequently, practitioners should understand how to interpret different types of models. This book will provide you with a fundamental understanding to compare and contrast models and even package implementations of similiar algorithms. Several machine learning interpretability techniques will be demonstrated to help you understand what is driving model and prediction performance. This will allow you to be more effective and efficient in applying and understanding mutliple good models. 1.4 The data sets The XX data sets chosen for this book allow us to illustrate the different features of our machine learning algorithms. Since the goal of this book is to demonstrate how to implement R’s ML stack, I make the assumption that you have already spent significant time cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this book such as: feature selection: removing unnecessary variables and retaining only those variables you wish to include in your modeling process, recoding variable names and values so that they are meaningful and interpretable, recoding or removing missing values. Consequently, the exemplar data sets I use throughout this book have, for the most part, gone through the necessary cleaning process. These data sets are all freely available and include: Property sales information as described in De Cock (2011). problem type: supervised regression response variable: sale price (i.e. $195,000, $215,000) features: 80 observations: 2,930 objective: use property attributes to predict the sale price of a home access: provided by the AmesHousing package (Kuhn 2017) more details: See ?AmesHousing::ames_raw # access data ames &lt;- AmesHousing::make_ames() # initial dimension dim(ames) ## [1] 2930 81 # response variable head(ames$SalePrice) ## NULL You can see the entire data cleaning process to transform the raw Ames housing data (AmesHousing::ames_raw) to the final clean data (AmesHousing::make_ames) that we will use in machine learning algorithms throughout this book at: https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R Employee attrition information originally provided by IBM Watson Analytics Lab. problem type: supervised binomial classification response variable: Attrition (i.e. “Yes”, “No”) features: 30 observations: 1,470 objective: use employee attributes to predict if they will attrit (leave the company) access: provided by the rsample package (Kuhn and Wickham 2017) more details: See ?rsample::attrition # access data attrition &lt;- rsample::attrition # initial dimension dim(attrition) ## [1] 1470 31 # response variable head(attrition$Attrition) ## [1] Yes No Yes No No No ## Levels: No Yes Image information for handwritten numbers originally presented to AT&amp;T Bell Lab’s to help build automatic mail-sorting machines for the USPS. Has been used since early 1990s to compare machine learning performance on pattern recognition (i.e. LeCun et al. (1990); LeCun et al. (1998); Cireşan, Meier, and Schmidhuber (2012)). Problem type: supervised multinomial classification response variable: V785 (i.e. numbers to predict: 0, 1, …, 9) features: 784 observations: 60,000 (train) / 10,000 (test) objective: use attributes about the “darkness” of each of the 784 pixels in images of handwritten numbers to predict if the number is 0, 1, …, or 9. access: see the code chunk that follows for download instructions more details: See online MNIST documentation # load training data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz train &lt;- data.table::fread(&quot;../data/mnist_train.csv&quot;, data.table = FALSE) # load test data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz test &lt;- data.table::fread(&quot;../data/mnist_test.csv&quot;, data.table = FALSE) # initial dimension dim(train) ## [1] 60000 785 # response variable head(train$V785) ## [1] 2 3 0 0 2 7 TODO: get unsupervised data sets for clustering and dimension reduction examples References "],
["regression-performance.html", "Chapter 2 Preparing for Supervised Machine Learning 2.1 Prerequisites 2.2 Data splitting 2.3 Feature engineering 2.4 Basic model formulation 2.5 Model tuning 2.6 Cross Validation for Generalization 2.7 Model evaluation", " Chapter 2 Preparing for Supervised Machine Learning Machine learning is a very iterative process. If performed and interpreted correctly, we can have great confidence in our outcomes. If not, the results will be useless. Approaching machine learning correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing variables, minimizing data leakage, tuning hyperparameters, and assessing model performance. Before introducing specific algorithms, this chapter introduces concepts that are commonly required in the supervised machine learning process and that you’ll see briskly covered in each chapter. 2.1 Prerequisites This chapter leverages the following packages. library(rsample) library(caret) library(h2o) library(dplyr) # turn off progress bars h2o.no_progress() # launch h2o h2o.init() ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 1 minutes 54 seconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_thv371 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.43 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) To illustrate some of the concepts, we will use the Ames Housing data and employee attrition data introduced in Chapter 1. Throughout this book, I’ll demonstrate approaches with regular data frames. However, since many of the supervised machine learning chapters leverage the h2o package, we’ll also show how to do some of the tasks with H2O objects. This requires your data to be in an H2O object, which you can convert any data frame easily with as.h2o. If you try to convert the original rsample::attrition data set to an H2O object an error will occur. This is because several variables are ordered factors and H2O has no way of handling this data type. Consequently, you must convert any ordered factors to unordered. # ames data ames &lt;- AmesHousing::make_ames() ames.h2o &lt;- as.h2o(ames) # attrition data churn &lt;- rsample::attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) churn.h2o &lt;- as.h2o(churn) 2.2 Data splitting 2.2.1 Spending our data wisely A major goal of the machine learning process is to find an algorithm \\(f(x)\\) that most accurately predicts future values (\\(y\\)) based on a set of inputs (\\(x\\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we “spend” our data will help us understand how well our algorithm generalizes to unseen data. To provide an accurate understanding of the generalizability of our final optimal model, we split our data into training and test data sets: Training Set: these data are used to train our algorithms and tune hyper-parameters. Test Set: having chosen a final model, these data are used to estimate its prediction error (generalization error). These data should not be used during model training! Figure 2.1: Splitting data into training and test sets. Given a fixed amount of data, typical recommendations for splitting your data into training-testing splits include 60% (training) - 40% (testing), 70%-30%, or 80%-20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep in mind that as your overall data set gets smaller, spending too much in training (\\(&gt;80\\%\\)) won’t allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting), sometimes too much spent in testing (\\(&gt;40\\%\\)) won’t allow us to get a good assessment of model parameters In today’s data-rich environment, typically, we are not lacking in the quantity of observations, so a 70-30 split is often sufficient. The two most common ways of splitting data include simple random sampling and stratified sampling. 2.2.2 Simple random sampling The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the percentage of data represented in your response variable (\\(y\\)). There are multiple ways to split our data. Here we show four options to produce a 70-30 split (note that setting the seed value allows you to reproduce your randomized splits): Sampling is a random process so setting the random number generator with a common seed allows for reproducible results. Throughout this book I will use the number 123 often for reproducibility but the number itself has no special meaning. # base R set.seed(123) index_1 &lt;- sample(1:nrow(ames), round(nrow(ames) * 0.7)) train_1 &lt;- ames[index_1, ] test_1 &lt;- ames[-index_1, ] # caret package set.seed(123) index_2 &lt;- createDataPartition(ames$Sale_Price, p = 0.7, list = FALSE) train_2 &lt;- ames[index_2, ] test_2 &lt;- ames[-index_2, ] # rsample package set.seed(123) split_1 &lt;- initial_split(ames, prop = 0.7) train_3 &lt;- training(split_1) test_3 &lt;- testing(split_1) # h2o package split_2 &lt;- h2o.splitFrame(ames.h2o, ratios = 0.7, seed = 123) train_4 &lt;- split_2[[1]] test_4 &lt;- split_2[[2]] Since this sampling approach will randomly sample across the distribution of \\(y\\) (Sale_Price in our example), you will typically result in a similar distribution between your training and test sets as illustrated below. Figure 2.2: Training (black) vs. test (red) distribution. 2.2.3 Stratified sampling However, if we want to explicitly control our sampling so that our training and test sets have similar \\(y\\) distributions, we can use stratified sampling. This is more common with classification problems where the reponse variable may be imbalanced (90% of observations with response “Yes” and 10% with response “No”). However, we can also apply to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable, stratified sampling will break \\(y\\) down into quantiles and randomly sample from each quantile. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets. The easiest way to perform stratified sampling on a response variable is to use the rsample package, where you specify the response variable to stratafy. The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes: 16%). By enforcing stratified sampling both our training and testing sets have approximately equal response distributions. # orginal response distribution table(churn$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8387755 0.1612245 # stratified sampling with the rsample package set.seed(123) split_strat &lt;- initial_split(churn, prop = 0.7, strata = &quot;Attrition&quot;) train_strat &lt;- training(split_strat) test_strat &lt;- testing(split_strat) # consistent response ratio between train &amp; test table(train_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.838835 0.161165 table(test_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8386364 0.1613636 2.3 Feature engineering Feature engineering generally refers to the process of adding, deleting, and transforming the variables to be applied to your machine learning algorithms. Feature engineering is a significant process and requires you to spend substantial time understanding your data…or as Leo Breiman said “live with your data before you plunge into modeling.” Although this book primarily focuses on applying machine learning algorithms, feature engineering can make or break an algorithm’s predictive ability. We will not cover all the potential ways of implementing feature engineering; however, we will cover a few fundamental pre-processing tasks that can significantly improve modeling performance. To learn more about feature engineering check out Feature Engineering for Machine Learning by Zheng and Casari (2018) and Max Kuhn’s upcoming book Feature Engineering and Selection: A Practical Approach for Predictive Models. 2.3.1 Response Transformation Although not a requirement, normalizing the distribution of the response variable by using a transformation can lead to a big improvement, especially for parametric models. As we saw in the data splitting section, our response variable Sale_Price is right skewed. ggplot(train_1, aes(x = Sale_Price)) + geom_density(trim = TRUE) + geom_density(data = test_1, trim = TRUE, col = &quot;red&quot;) Figure 2.3: Right skewed response variable. To normalize, we have a few options: Option 1: normalize with a log transformation. This will transform most right skewed distributions to be approximately normal. # log transformation train_log_y &lt;- log(train_1$Sale_Price) test_log_y &lt;- log(test_1$Sale_Price) If your reponse has negative values then a log transformation will produce NaNs. If these negative values are small (between -0.99 and 0) then you can apply log1p, which adds 1 to the value prior to applying a log transformation. If your data consists of negative equal to or less than -1, use the Yeo Johnson transformation mentioned next. log(-.5) ## [1] NaN log1p(-.5) ## [1] -0.6931472 Option 2: use a Box Cox transformation. A Box Cox transformation is more flexible and will find the transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution. Be sure to compute the lambda on the training set and apply that same lambda to both the training and test set to minimize data leakage. # Box Cox transformation lambda &lt;- forecast::BoxCox.lambda(train_1$Sale_Price) train_bc_y &lt;- forecast::BoxCox(train_1$Sale_Price, lambda) test_bc_y &lt;- forecast::BoxCox(test_1$Sale_Price, lambda) We can see that in this example, the log transformation and Box Cox transformation both do about equally well in transforming our reponse variable to be normally distributed. Figure 2.4: Response variable transformations. Note that when you model with a transformed response variable, your predictions will also be in the transformed value. You will likely want to re-transform your predicted values back to their normal state so that decision-makers can interpret the results. The following code can do this for you: # log transform a value y &lt;- log(10) # re-transforming the log-transformed value exp(y) ## [1] 10 # Box Cox transform a value y &lt;- forecast::BoxCox(10, lambda) # Inverse Box Cox function inv_box_cox &lt;- function(x, lambda) { if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda) } # re-transforming the Box Cox-transformed value inv_box_cox(y, lambda) ## [1] 10 ## attr(,&quot;lambda&quot;) ## [1] -0.3067918 If your response has negative values, you can use the Yeo-Johnson transformation. To apply, use car::powerTransform to identify the lambda, car::yjPower to apply the transformation, and VGAM::yeo.johnson to apply the transformation and/or the inverse transformation. 2.3.2 Predictor Transformation 2.3.3 One-hot encoding Many models require all predictor variables to be numeric. Consequently, we need to transform any categorical variables into numeric representations so that these algorithms can compute. Some packages automate this process (i.e. h2o, glm, caret) while others do not (i.e. glmnet, keras). Furthermore, there are many ways to encode categorical variables as numeric representations (i.e. one-hot, ordinal, binary, sum, Helmert). The most common is referred to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding variable x in the following: id x 1 a 2 c 3 b 4 c 5 c 6 a 7 b 8 c results in the following representation: id x.a x.b x.c 1 1 0 0 2 0 0 1 3 0 1 0 4 0 0 1 5 0 0 1 6 1 0 0 7 0 1 0 8 0 0 1 This is called less than full rank encoding where we retain all variables for each level of x. However, this creates perfect collinearity which causes problems with some machine learning algorithms (i.e. generalized regression models, neural networks). Alternatively, we can create full-rank one-hot encoding by dropping one of the levels (level a has been dropped): id x.b x.c 1 0 0 2 0 1 3 1 0 4 0 1 5 0 1 6 0 0 7 1 0 8 0 1 If you needed to manually implement one-hot encoding yourself you can with caret::dummyVars. Sometimes you may have a feature level with very few observations and all these observations show up in the test set but not the training set. The benefit of using dummyVars on the full data set and then applying the result to both the train and test data sets is that it will guarantee that the same features are represented in both the train and test data. # full rank one-hot encode - recommended for generalized linear models and # neural networks full_rank &lt;- dummyVars( ~ ., data = ames, fullRank = TRUE) train_oh &lt;- predict(full_rank, train_1) test_oh &lt;- predict(full_rank, test_1) # less than full rank --&gt; dummy encoding dummy &lt;- dummyVars( ~ ., data = ames, fullRank = FALSE) train_oh &lt;- predict(dummy, train_1) test_oh &lt;- predict(dummy, test_1) Two things to note: since one-hot encoding adds new features it can significantly increase the dimensionality of our data. If you have a data set with many categorical variables and those categorical variables in turn have many unique levels, the number of features can explode. In these cases you may want to explore ordinal encoding of your data. if using h2o you do not need to explicity encode your categorical predictor variables but you can override the default encoding. This can be considered a tuning parameter as some encoding approaches will improve modeling accuracy over other encodings. See the encoding options for h2o here. 2.3.4 Standardizing Some models (K-NN, SVMs, PLS, neural networks) require that the predictor variables have the same units. Centering and scaling can be used for this purpose and is often referred to as standardizing the features. Standardizing numeric variables results in zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Some packages have built-in arguments (i.e. h2o, caret) to standardize and some do not (i.e. glm, keras). If you need to manually standardize your variables you can use the preProcess function provided by the caret package. For example, here we center and scale our Ames predictor variables. It is important that you standardize the test data based on the training mean and variance values of each feature. This minimizes data leakage. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;center&quot;, &quot;scale&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 2.3.5 Alternative Feature Transformation There are some alternative transformations that you can perform: Normalizing the predictor variables with a Box Cox transformation can improve parametric model performance. Collapsing highly correlated variables with PCA can reduce the number of features and increase the stability of generalize linear models. However, this reduces the amount of information at your disposal and we show you how to use regularization as a better alternative to PCA. Removing near-zero or zero variance variables. Variables with vary little variance tend to not improve model performance and can be removed. preProcess provides many other transformation options which you can read more about here. For example, the following normalizes predictors with a Box Cox transformation, center and scales continuous variables, performs principal component analysis to reduce the predictor dimensions, and removes predictors with near zero variance. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;, &quot;nzv&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 2.4 Basic model formulation There are many packages to perform machine learning and there are almost always more than one to perform each algorithm (i.e. there are over 20 packages to perform random forests). There are pros and cons to each package; some may be more computationally efficient while others may have more hyperparameter tuning options. Future chapters will expose you to many of the packages and algorithms that perform and scale best to most organization’s problems and data sets. Just realize there are more ways than one to skin a 🙀. For example, these three functions will all produce the same linear regression model output. lm.lm &lt;- lm(Sale_Price ~ ., data = train_1) lm.glm &lt;- glm(Sale_Price ~ ., data = train_1, family = gaussian) lm.caret &lt;- train(Sale_Price ~ ., data = train_1, method = &quot;lm&quot;) One thing you will notice throughout this guide is that we can specify our model formulation in different ways. In the above examples we use the model formulation (Sale_Price ~ . which says explain Sale_Price based on all features) approach. Alternative approaches, which you will see more often throughout this guide, are the matrix formulation and variable name specification approaches. Matrix formulation requires that we separate our response variable from our features. For example, in the regularization section we’ll use glmnet which requires our features (x) and response (y) variable to be specified separately: # get feature names features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # create feature and response set train_x &lt;- train_1[, features] train_y &lt;- train_1$Sale_Price # example of matrix formulation glmnet.m1 &lt;- glmnet(x = train_x, y = train_y) Alternatively, h2o uses variable name specification where we provide all the data combined in one training_frame but we specify the features and response with character strings: # create variable names and h2o training frame y &lt;- &quot;Sale_Price&quot; x &lt;- setdiff(names(train_1), y) train.h2o &lt;- as.h2o(train_1) # example of variable name specification h2o.m1 &lt;- h2o.glm(x = x, y = y, training_frame = train.h2o) 2.5 Model tuning Hyperparameters control the level of model complexity. Some algorithms have many tuning parameters while others have only one or two. Tuning can be a good thing as it allows us to transform our model to better align with patterns within our data. For example, the simple illustration below shows how the more flexible model aligns more closely to the data than the fixed linear model. Figure 2.5: Tuning allows for more flexible patterns to be fit. However, highly tunable models can also be dangerous because they allow us to overfit our model to the training data, which will not generalize well to future unseen data. Figure 2.6: Highly tunable models can overfit if we are not careful. Throughout this guide we will demonstrate how to tune the different parameters for each model. One way to performing hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy. However, this would be very tedious work. An alternative approach is to perform a grid search. A grid search is an automated approach to searching across many combinations of hyperparameter values. Throughout this guide you will be exposed to different approaches to performing grid searches. 2.6 Cross Validation for Generalization Our goal is to not only find a model that performs well on training data but to find one that performs well on future unseen data. So although we can tune our model to reduce some error metric to near zero on our training data, this may not generalize well to future unseen data. Consequently, our goal is to find a model and its hyperparameters that will minimize error on held-out data. Let’s go back to this image… Figure 2.7: Bias versus variance. The model on the left is considered rigid and consistent. If we provided it a new training sample with slightly different values, the model would not change much, if at all. Although it is consistent, the model does not accurately capture the underlying relationship. This is considered a model with high bias. The model on the right is far more inconsistent. Even with small changes to our training sample, this model would likely change significantly. This is considered a model with high variance. The model in the middle balances the two and, likely, will minimize the error on future unseen data compared to the high bias and high variance models. This is our goal. Figure 2.8: Bias-variance tradeoff. To find the model that balances the bias-variance tradeoff, we search for a model that minimizes a k-fold cross-validation error metric. k-fold cross-validation is a resampling method that randomly divides the training data into k groups (aka folds) of approximately equal size. The model is fit on \\(k-1\\) folds and then the held-out validation fold is used to compute the error. This procedure is repeated k times; each time, a different group of observations is treated as the validation set. This process results in k estimates of the test error (\\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_k\\)). Thus, the k-fold CV estimate is computed by averaging these values, which provides us with an approximation of the error to expect on unseen data. Figure 2.9: Illustration of the k-fold cross validation process. The algorithms we cover in this guide all have built-in cross validation capabilities. One typically uses a 5 or 10 fold CV (\\(k = 5\\) or \\(k = 10\\)). For example, h2o implements CV with the nfolds argument: # example of 10 fold CV in h2o h2o.cv &lt;- h2o.glm( x = x, y = y, training_frame = train.h2o, nfolds = 10 ) 2.7 Model evaluation This leads us to our final topic, error metrics to evaluate performance. There are several metrics we can choose from to assess the error of a supervised machine learning model. The most common include: 2.7.1 Regression models MSE: Mean squared error is the average of the squared error (\\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\)). The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. Objective: minimize RMSE: Root mean squared error. This simply takes the square root of the MSE metric (\\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize Deviance: Short for mean residual deviance. In essence, it provides a measure of goodness-of-fit of the model being evaluated when compared to the null model (intercept only). If the response variable distribution is gaussian, then it is equal to MSE. When not, it usually gives a more useful estimate of error. Objective: minimize MAE: Mean absolute error. Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values (\\(MAE = \\frac{1}{n} \\sum^n_{i=1}(\\vert y_i - \\hat y_i \\vert)\\)). Objective: minimize RMSLE: Root mean squared logarithmic error. Similiar to RMSE but it performs a log() on the actual and predicted values prior to computing the difference (\\(RMSLE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(log(y_i + 1) - log(\\hat y_i + 1))^2}\\)). When your response variable has a wide range of values, large repsonse values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. Objective: minimize \\(R^2\\): This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable. Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower \\(R^2\\) than the other. You should not place too much emphasis on this metric. Objective: maximize Most models we assess in this guide will report most, if not all, of these metrics. We will emphasize MSE and RMSE but its good to realize that certain situations warrant emphasis on some more than others. 2.7.2 Classification models Misclassification: This is the overall error. For example, say you are predicting 3 classes ( high, medium, low ) and each class has 25, 30, 35 observations respectively (90 observations total). If you misclassify 3 observations of class high, 6 of class medium, and 4 of class low, then you misclassified 13 out of 90 observations resulting in a 14% misclassification rate. Objective: minimize Mean per class error: This is the average error rate for each class. For the above example, this would be the mean of \\(\\frac{3}{25}, \\frac{6}{30}, \\frac{4}{35}\\), which is 12%. If your classes are balanced this will be identical to misclassification. Objective: minimize MSE: Mean squared error. Computes the distance from 1.0 to the probability suggested. So, say we have three classes, A, B, and C, and your model predicts a probabilty of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer was A the \\(MSE = 0.09^2 = 0.0081\\), if it is B \\(MSE = 0.93^2 = 0.8649\\), if it is C \\(MSE = 0.98^2 = 0.9604\\). The squared component results in large differences in probabilities for the true class having larger penalties. Objective: minimize Cross-entropy (aka Log Loss or Deviance): Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class. Consequently, this metric disproportionately punishes predictions where we predict a small probability for the true class, which is another way of saying having high confidence in the wrong answer is really bad. Objective: minimize Gini index: Mainly used with tree-based methods and commonly referred to as a measure of purity where a small value indicates that a node contains predominantly observations from a single class. Objective: minimize When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a true positive. However, if we predict a level or event that did not happen this is called a false positive (i.e. we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a false negative (i.e. a customer that we did not predict to redeem a coupon does). Figure 2.10: Confusion matrix. We can extract different levels of performance from these measures. For example, given the classification matrix below we can assess the following: Accuracy: Overall, how often is the classifier correct? Opposite of misclassification above. Example: \\(\\frac{TP + TN}{total} = \\frac{100+50}{165} = 0.91\\). Objective: maximize Precision: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: \\(\\frac{TP}{TP + FP} = \\frac{100}{100+10} = 0.91\\). Objective: maximize Sensitivity (aka recall): How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: \\(\\frac{TP}{TP + FN} = \\frac{100}{100+5} = 0.95\\). Objective: maximize Specificity: How accurately does the classifier classify actual non-events? Example: \\(\\frac{TN}{TN + FP} = \\frac{50}{50+10} = 0.83\\). Objective: maximize Figure 2.11: Example confusion matrix. AUC: Area under the curve. A good classifier will have high precision and sensitivity. This means the classifier does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. To capture this balance, we often use a ROC curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve. Objective: maximize Figure 2.12: ROC curve. References "],
["regularized-regression.html", "Chapter 3 Regularized Regression 3.1 Prerequisites 3.2 Advantages &amp; Disadvantages 3.3 The Idea 3.4 Implementation: Regression 3.5 Implementation: Binary Classification 3.6 Implementation: Multinomial Classification 3.7 Learning More", " Chapter 3 Regularized Regression Generalized linear models (GLMs) such as ordinary least squares regression and logistic regression are simple and fundamental approaches for supervised learning. Moreover, when the assumptions required by GLMs are met, the coefficients produced are unbiased and, of all unbiased linear techniques, have the lowest variance. However, in today’s world, data sets being analyzed typically have a large amount of features. As the number of features grow, our GLM assumptions typically break down and our models often overfit (aka have high variance) to the training sample, causing our out of sample error to increase. Regularization methods provide a means to control our coefficients, which can reduce the variance and decrease out of sample error. 3.1 Prerequisites This chapter assumes you are familiary with basic idea behind linear and logistic regression. If not, two tutorials to get you up to speed include this linear regression tutorial and this logistic regression tutorial. This chapter leverages the following packages. Some of these packages are playing a supporting role while the main emphasis will be on the glmnet (Friedman, Hastie, and Tibshirani 2010) and h2o (Kraljevic 2018) packages. library(glmnet) # implementing regularized regression approaches library(h2o) # implementing regularized regression approaches library(rsample) # training vs testing data split library(dplyr) # basic data manipulation procedures library(ggplot2) # plotting 3.2 Advantages &amp; Disadvantages Advantages: Normal GLM models require that you have more observations than variables (\\(n&gt;p\\)); regularized regression allows you to model wide data where \\(n&lt;p\\). Minimizes the impact of multicollinearity. Provides automatic feature selection (at least when you apply a Lasso or elastic net penalty). Minimal hyperparameters making it easy to tune. Computationally efficient - relatively fast compared to other algorithms in this guide and does not require large memory. Disdvantages: Requires data pre-processing - requires all variables to be numeric (i.e. one-hot encode). However, h2o helps to automate this process. Does not handle missing data - must impute or remove observations with missing values. Not robust to outliers as they can still bias the coefficients. Assumes relationships between predictors and response variable to be monotonic linear (always increasing or decreasing in a linear fashion). Typically does not perform as well as more advanced methods that allow non-monotonic and non-linear relationships (i.e. random forests, gradient boosting machines, neural networks). 3.3 The Idea The easiest way to understand regularized regression is to explain how it is applied to ordinary least squares regression (OLS). The objective of OLS regression is to find the plane that minimizes the sum of squared errors (SSE) between the observed and predicted response. Illustrated below, this means identifying the plane that minimizes the grey lines, which measure the distance between the observed (red dots) and predicted response (blue plane). Figure 2.1: Fitted regression line using Ordinary Least Squares. More formally, this objective function is written as: \\[\\text{minimize} \\bigg \\{ SSE = \\sum^n_{i=1} (y_i - \\hat{y}_i)^2 \\bigg \\} \\tag{1}\\] The OLS objective function performs quite well when our data align to the key assumptions of OLS regression: Linear relationship Multivariate normality No autocorrelation Homoscedastic (constant variance in residuals) There are more observations (n) than features (p) (\\(n &gt; p\\)) No or little multicollinearity However, for many real-life data sets we have very wide data, meaning we have a large number of features (p) that we believe are informative in predicting some outcome. As p increases, we can quickly violate some of the OLS assumptions and we require alternative approaches to provide predictive analytic solutions. Specifically, as p increases there are three main issues we most commonly run into: 3.3.1 1. Multicollinearity As p increases we are more likely to capture multiple features that have some multicollinearity. When multicollinearity exists, we often see high variability in our coefficient terms. For example, in our Ames data, Gr_Liv_Area and TotRms_AbvGrd are two variables that have a correlation of 0.808 and both variables are strongly correlated to our response variable (Sale_Price). When we fit a model with both these variables we get a positive coefficient for Gr_Liv_Area but a negative coefficient for TotRms_AbvGrd, suggesting one has a positive impact to Sale_Price and the other a negative impact. # fit with two strongly correlated variables lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames) ## ## Coefficients: ## (Intercept) Gr_Liv_Area TotRms_AbvGrd ## 42767.6 139.4 -11025.9 However, if we refit the model with each variable independently, they both show a positive impact. However, the Gr_Liv_Area effect is now smaller and the TotRms_AbvGrd is positive with a much larger magnitude. # fit with just Gr_Liv_Area lm(Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Coefficients: ## (Intercept) Gr_Liv_Area ## 13289.6 111.7 # fit with just TotRms_Area lm(Sale_Price ~ TotRms_AbvGrd, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames) ## ## Coefficients: ## (Intercept) TotRms_AbvGrd ## 18665 25164 This is a common result when collinearity exists. Coefficients for correlated features become over-inflated and can fluctuate significantly. One consequence of these large fluctuations in the coefficient terms is overfitting, which means we have high variance in the bias-variance tradeoff space. Although an analyst can use tools such as variance inflaction factors (Myers 1990) to identify and remove those strongly correlated variables, it is not always clear which variable(s) to remove. Nor do we always wish to remove variables as this may be removing signal in our data. 3.3.2 2. Insufficient solution When the number of features exceed the number of observations (\\(p &gt; n\\)), the OLS solution matrix is not invertible. This causes significant issues because it means: (1) The least-squares estimates are not unique. In fact, there are an infinite set of solutions available and most of these solutions overfit the data. (2) In many instances the result will be computationally infeasible. Consequently, to resolve this issue an analyst can remove variables until \\(p &lt; n\\) and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach (Kuhn &amp; Johnson, 2013, pp. 43-47), it can be cumbersome and prone to errors. 3.3.3 3. Interpretability With a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection. One approach to this is called hard threshholding feature selection, which can be performed with linear model selection approaches. However, model selection approaches can be computationally inefficient, do not scale well, and they simply assume a feature as in or out. We may wish to use a soft threshholding approach that slowly pushes a feature’s effect towards zero. As will be demonstrated, this can provide additional understanding regarding predictive signals. 3.3.4 Regularized Models When we experience these concerns, one alternative to OLS regression is to use regularized regression (also commonly referred to as penalized models or shrinkage methods) to control the parameter estimates. Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model. The objective function of regularized regression methods is very similar to OLS regression; however, we add a penalty parameter (P). \\[\\text{minimize} \\big \\{ SSE + P \\big \\} \\tag{2}\\] This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE). This concept generalizes to all GLM models. So far, we have be discussing OLS and the sum of squared errors. However, different models within the GLM family (i.e. logistic regression, Poisson regression) have different loss functions. Yet we can think of the penalty parameter all the same - it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model’s loss function. There are three types of penalty parameters we can implement: Ridge Lasso Elastic net, which is a combination of Ridge and Lasso 3.3.4.1 Ridge penalty Ridge regression (Hoerl and Kennard 1970) controls the coefficients by adding \\(\\lambda \\sum^p_{j=1} \\beta_j^2\\) to the objective function. This penalty parameter is also referred to as “\\(L_2\\)” as it signifies a second-order penalty being used on the coefficients.1 \\[\\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} \\beta_j^2 \\bigg \\} \\tag{3}\\] This penalty parameter can take on a wide range of values, which is controlled by the tuning parameter \\(\\lambda\\). When \\(\\lambda = 0\\) there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE. However, as \\(\\lambda \\rightarrow \\infty\\), the penalty becomes large and forces our coefficients to near zero. This is illustrated in Figure 3.1 where exemplar coefficients have been regularized with \\(\\lambda\\) ranging from 0 to over 8,000 (\\(log(8103) = 9\\)). Figure 3.1: Ridge regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Although these coefficients were scaled and centered prior to the analysis, you will notice that some are extremely large when \\(\\lambda \\rightarrow 0\\). Furthermore, you’ll notice the large negative parameter that fluctuates until \\(log(\\lambda) \\approx 2\\) where it then continuously skrinks to zero. This is indicitive of multicollinearity and likely illustrates that constraining our coefficients with \\(log(\\lambda) &gt; 2\\) may reduce the variance, and therefore the error, in our model. In essence, the ridge regression model pushes many of the correlated features towards each other rather than allowing for one to be wildly positive and the other wildly negative. Furthermore, many of the non-important features get pushed to near zero. This allows us to reduce the noise in our data, which provides us more clarity in identifying the true signals in our model. However, a ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection. If greater interpretation is necessary where you need to reduce the signal in your data to a smaller subset then a lasso or elastic net penalty may be preferable. 3.3.4.2 Lasso penalty The least absolute shrinkage and selection operator (lasso) model (Tibshirani 1996) is an alternative to the ridge penalty that has a small modification to the penalty in the objective function. Rather than the \\(L_2\\) penalty we use the following \\(L_1\\) penalty \\(\\lambda \\sum^p_{j=1} | \\beta_j|\\) in the objective function. \\[\\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\tag{4}\\] Whereas the ridge penalty approach pushes variables to approximately but not equal to zero, the lasso penalty will actually push coefficients to zero as illustrated in Figure 3.2. Thus the lasso model not only improves the model with regularization but it also conducts automated feature selection. Figure 3.2: Lasso regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Numbers on top axis illustrate how many non-zero coefficients remain. In the figure above we see that when \\(log(\\lambda) = -5\\) all 15 variables are in the model, when \\(log(\\lambda) = -1\\) 12 variables are retained, and when \\(log(\\lambda) = 1\\) only 3 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal. 3.3.4.3 Elastic nets A generalization of the ridge and lasso penalties is the elastic net penalty (Zou and Hastie 2005), which combines the two penalties. \\[\\text{minimize } \\bigg \\{ SSE + \\lambda_1 \\sum^p_{j=1} \\beta_j^2 + \\lambda_2 \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\tag{5}\\] Although lasso models perform feature selection, a result of their penalty parameter is that typically when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically reducing correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. 3.3.5 Tuning Regularized models are simple to tune as there are only two tuning parameters: Size of penalty (\\(\\lambda\\)): Controls how much we want to constrain our coefficients. Small penalties where \\(\\lambda\\) is close to zero allow our coefficients to be larger; however, larger values of \\(\\lambda\\) penalize our coefficients and forces them to take on smaller values. Hence, this parameter is often called the shrinkage parameter. Alpha: The alpha parameter tells our model to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)). 3.3.6 Package implementation There are a few packages that implement variants of regularized regression. You can find a comprehensive list on the CRAN Machine Learning Task View. However, the most popular implementations which we will cover in this chapter include: glmnet: The original implementation of regularized regression in R. The glmnet R package provides an extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression and the Cox model. Two recent additions are the multiple-response Gaussian, and the grouped multinomial regression. A nice vignette is available here. Features include2: The code can handle sparse input-matrix formats, as well as range constraints on coefficients. Automatically standardizes your feature set. Built-in cross validation. The core of glmnet is a set of fortran subroutines, which make for very fast execution. The algorithms use coordinate descent with warm starts and active set iterations. Supports the following distributions: “gaussian”,“binomial”,“poisson”,“multinomial”,“cox”,“mgaussian” h2o: The h2o R package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with a fairly comprehensive online resource that includes methodology and code documentation along with tutorials. Features include: Fits both regularized and non-regularized GLMs. Automated feature pre-processing (one-hot encode &amp; standardization). Built-in cross validation. Built-in grid search capabilities. Supports the following distributions: “guassian”, “binomial”, “multinomial”, “ordinal”, “poisson”, “gamma”, “tweedie”. Distributed and parallelized computation on either a single node or a multi-node cluster. Automatic early stopping based on convergence of user-specified metrics to user-specified relative tolerance. 3.4 Implementation: Regression To illustrate various regularization concepts for a regression problem we will use the Ames, IA housing data, where our intent is to predict Sale_Price. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 3.4.1 glmnet The glmnet package is a fast implementation, but it requires some extra processing up-front to your data if it’s not already represented as a numeric matrix. glmnet does not use the formula method (y ~ x) so prior to modeling we need to create our feature and target set. Furthermore, we use the model.matrix function on our feature set (see Matrix::sparse.model.matrix for increased efficiency on large dimension data). We also log transform our response variable due to its skeweness. The log transformation of the response variable is not required; however, parametric models such as regularized regression are sensitive to skewed values so it is always recommended to normalize your response variable. # Create training and testing feature matrices # we use model.matrix(...)[, -1] to discard the intercept train_x &lt;- model.matrix(Sale_Price ~ ., ames_train)[, -1] test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1] # Create training and testing response vectors # transform y with log transformation train_y &lt;- log(ames_train$Sale_Price) test_y &lt;- log(ames_test$Sale_Price) 3.4.1.1 Basic implementation To apply a regularized model we can use the glmnet::glmnet function. The alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)) model. Behind the scenes, glmnet is doing two things that you should be aware of: It is essential that predictor variables are standardized when performing regularized regression. glmnet performs this for you. If you standardize your predictors prior to glmnet you can turn this argument off with standardize = FALSE. glmnet will perform ridge models across a wide range of \\(\\lambda\\) parameters, which are illustrated in the figure below. # Apply Ridge regression to attrition data ridge &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.3: Coefficients for our ridge regression model as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). In fact, we can see the exact \\(\\lambda\\) values applied with ridge$lambda. Although you can specify your own \\(\\lambda\\) values, by default glmnet applies 100 \\(\\lambda\\) values that are data derived. glmnet has built-in functions to auto-generate the appropriate \\(\\lambda\\) values based on the data so the vast majority of the time you will have little need to adjust the default \\(\\lambda\\) values. We can also directly access the coefficients for a model using coef. glmnet stores all the coefficients for each model in order of largest to smallest \\(\\lambda\\). Due to the number of features, here I just peak at the two largest coefficients (Latitude &amp; Overall_QualVery_Excellent) features for the largest \\(\\lambda\\) (279.1035) and smallest \\(\\lambda\\) (0.02791035). You can see how the largest \\(\\lambda\\) value has pushed these coefficients to nearly 0. # lambdas applied to penalty parameter ridge$lambda %&gt;% head() ## [1] 279.1035 254.3087 231.7166 211.1316 192.3752 175.2851 # small lambda results in large coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 100] ## Latitude Overall_QualVery_Excellent ## 0.60585376 0.09800466 # large lambda results in small coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 1] ## Latitude Overall_QualVery_Excellent ## 6.228028e-36 9.372514e-37 However, at this point, we do not understand how much improvement we are experiencing in our loss function across various \\(\\lambda\\) values. 3.4.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation (CV). cv.glmnet provides a built-in option to perform k-fold CV, and by default, performs 10-fold CV. Here we perform a CV glmnet model for both a ridge and lasso penalty. By default, cv.glmnet uses MSE as the loss function but you can also use mean absolute error by changing the type.measure argument. # Apply CV Ridge regression to Ames data ridge &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 0 ) # Apply CV Lasso regression to Ames data lasso &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 1 ) # plot results par(mfrow = c(1, 2)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.4: 10-fold cross validation MSE for a ridge and lasso model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. Our plots above illustrate the 10-fold CV mean squared error (MSE) across the \\(\\lambda\\) values. In both models we see a slight improvement in the MSE as our penalty \\(log(\\lambda)\\) gets larger , suggesting that a regular OLS model likely overfits our data. But as we constrain it further (continue to increase the penalty), our MSE starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases. The first and second vertical dashed lines represent the \\(\\lambda\\) value with the minimum MSE and the largest \\(\\lambda\\) value within one standard error of the minimum MSE. # Ridge model min(ridge$cvm) # minimum MSE ## [1] 0.02147691 ridge$lambda.min # lambda for this min MSE ## [1] 0.1236602 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min MSE ## [1] 0.02488411 ridge$lambda.1se # lambda for this MSE ## [1] 0.6599372 # Lasso model min(lasso$cvm) # minimum MSE ## [1] 0.02411134 lasso$lambda.min # lambda for this min MSE ## [1] 0.003865266 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min MSE ## [1] 0.02819356 lasso$lambda.1se # lambda for this MSE ## [1] 0.01560415 We can assess this visually. Here we plot the coefficients across the \\(\\lambda\\) values and the dashed red line represents the \\(\\lambda\\) with the smallest MSE and the dashed blue line represents largest \\(\\lambda\\) that falls within one standard error of the minimum MSE. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy. Above, we saw that both ridge and lasso penalties provide similiar MSEs; however, these plots illustrate that ridge is still using all 299 variables whereas the lasso model can get a similar MSE by reducing our feature set from 299 down to 131. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 63 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 63 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor. # Ridge model ridge_min &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) # Lasso model lasso_min &lt;- glmnet( x = train_x, y = train_y, alpha = 1 ) par(mfrow = c(1, 2)) # plot ridge model plot(ridge_min, xvar = &quot;lambda&quot;, main = &quot;Ridge penalty\\n\\n&quot;) abline(v = log(ridge$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(ridge$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) # plot lasso model plot(lasso_min, xvar = &quot;lambda&quot;, main = &quot;Lasso penalty\\n\\n&quot;) abline(v = log(lasso$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(lasso$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Figure 3.5: Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. So far we’ve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas alpha \\(\\rightarrow 0\\) will have a heavier ridge penalty applied and alpha \\(\\rightarrow 1\\) will have a heavier lasso penalty. lasso &lt;- glmnet(train_x, train_y, alpha = 1.0) elastic1 &lt;- glmnet(train_x, train_y, alpha = 0.25) elastic2 &lt;- glmnet(train_x, train_y, alpha = 0.75) ridge &lt;- glmnet(train_x, train_y, alpha = 0.0) par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1) plot(lasso, xvar = &quot;lambda&quot;, main = &quot;Lasso (Alpha = 1)\\n\\n\\n&quot;) plot(elastic1, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .25)\\n\\n\\n&quot;) plot(elastic2, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .75)\\n\\n\\n&quot;) plot(ridge, xvar = &quot;lambda&quot;, main = &quot;Ridge (Alpha = 0)\\n\\n\\n&quot;) Figure 3.6: Coefficients for various penalty parameters. Often, the optimal model contains an alpha somewhere between 0-1, thus we want to tune both the \\(\\lambda\\) and the alpha parameters. To set up our tuning, we create a common fold_id, which just allows us to apply the same CV folds to each model. We then create a tuning grid that searches across a range of alphas from 0-1, and empty columns where we’ll dump our model results into. Use caution when including \\(\\alpha = 0\\) or \\(\\alpha = 1\\) in the grid search. \\(\\alpha = 0\\) will produce a dense solution and it can be very slow (or even impossible) to compute in large N situations. \\(\\alpha = 1\\) has no \\(\\ell_2\\) penalty, so it is therefore less numerically stable and can be very slow as well due to slower convergence. If you experience slow computation, I recommend searching across \\(\\alpha\\) values of 0.1, .25, .5, .75, .9. # maintain the same folds across all models fold_id &lt;- sample(1:10, size = length(train_y), replace = TRUE) # search across a range of alphas tuning_grid &lt;- tibble::tibble( alpha = seq(0, 1, by = .1), mse_min = NA, mse_1se = NA, lambda_min = NA, lambda_1se = NA ) Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error MSE values and their respective \\(\\lambda\\) values. This grid search took 41 seconds to compute. # perform grid search for(i in seq_along(tuning_grid$alpha)) { # fit CV model for each alpha value fit &lt;- cv.glmnet(train_x, train_y, alpha = tuning_grid$alpha[i], foldid = fold_id) # extract MSE and lambda values tuning_grid$mse_min[i] &lt;- fit$cvm[fit$lambda == fit$lambda.min] tuning_grid$mse_1se[i] &lt;- fit$cvm[fit$lambda == fit$lambda.1se] tuning_grid$lambda_min[i] &lt;- fit$lambda.min tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se } tuning_grid ## # A tibble: 11 x 5 ## alpha mse_min mse_1se lambda_min lambda_1se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0230 0.0267 0.179 0.795 ## 2 0.1 0.0237 0.0285 0.0387 0.156 ## 3 0.2 0.0241 0.0289 0.0193 0.0856 ## 4 0.3 0.0243 0.0295 0.0129 0.0627 ## 5 0.4 0.0245 0.0295 0.00966 0.0470 ## 6 0.5 0.0246 0.0295 0.00773 0.0376 ## 7 0.6 0.0247 0.0301 0.00644 0.0344 ## 8 0.7 0.0247 0.0302 0.00552 0.0295 ## 9 0.8 0.0247 0.0302 0.00483 0.0258 ## 10 0.9 0.0247 0.0302 0.00429 0.0229 ## 11 1 0.0248 0.0304 0.00387 0.0206 If we plot the MSE ± one standard error for the optimal \\(\\lambda\\) value for each alpha setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model (alpha = 1) with \\(\\lambda = 0.003521887\\), gain the benefits of its feature selection capability and reasonably assume no loss in accuracy. tuning_grid %&gt;% mutate(se = mse_1se - mse_min) %&gt;% ggplot(aes(alpha, mse_min)) + geom_line(size = 2) + geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) Figure 3.7: MSE ± one standard error for different alpha penalty parameters. 3.4.1.3 Feature interpretation Regularized regression assumes a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. This constant change is given by the given coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. Consequently, this makes understanding variable relationships simple with regularized models. And since the predictors have all been standardized, comparing the coefficients against one another allows us to identify the most influential predictors. Those predictors with the largest positive coefficients have the largest positive impact on our response variable and those variables with really small negative coefficients have a very small negative impact on our response. Furthermore, there is no difference between the global and local model interpretation (see model interpretation chapter for details). We’ll illustrate with the following models. lasso &lt;- cv.glmnet(train_x, train_y, alpha = 1) ridge &lt;- cv.glmnet(train_x, train_y, alpha = 0) Our ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection but it will typically push most variables to near zero and allow the most influential variables to be more prominent. The following extracts the coefficients of our ridge model and plots the top 100 most influential variables. You can see that many of the variables have coefficients closer to zero but there a handful of predictor variables that have noticable influence. For example, properties with above average latitude (northern end of Ames), are in the Green Hills neighborhood (which is actually in the south part of Ames), or have 2 extra miscellaneous garage features will have a positive influence on the sales price. Alternatively, properties that are zoned agricultural, have a home functional code of “Sal” (salvage only) or “Sev” (severely damaged), or have a pool quality assessment of “Good” tend have a negative influence on the sales price. Keep in mind these coefficients are for the standardized variable values. Consequently, the interpretation of these coefficient values are not as clear. Basically, for every unit the Latitude variable is above the mean value, the reponse variable (which has been log transformed) has a 0.474 unit increase. The important insight is to identify those variables with the largest positive and negative impacts on the response variable. coef(ridge, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% top_n(100, wt = abs(value)) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Top 100 influential variables (ridge penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.8: A ridge penalty will retain all variables but push most coefficients to near zero. Consequently, we retain any minor signals that all features provide but we can visualize those coefficients with the largest absolute values to identify the most influential predictors. Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those features most influential to predictive accuracy. If we select the model with the minimum MSE and plot all variables in that model we see similar results to the ridge regarding the most influential variables. Although the ordering typically differs, we often see some commonality between lasso and ridge models regarding the most influential variables. coef(lasso, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (lasso penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.9: A lasso penalty will perform feature selection by pushing most coefficients to zero. Consequently, we can view all coefficients to see which features were selected; however, our objective usually is still to identify those features with the strongest signal (largest absolute coefficient values). 3.4.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. The only caveat is you need to supply predict an s parameter with the preferred models \\(\\lambda\\) value. For example, here we create a lasso model, which provides me a minimum MSE of 0.02398 (RMSE = 0.123). However, our response variable is log transformed so we must re-transform it to get an interpretable RMSE (our average generalized error is $25,156.77). During a normal modeling process, you would not assess the results on your test set until you’ve assessed all potential models with the training data and decided upon a final model. Then, and only then, should you make predictions on the test set and assess the final generalizable error. However, for educational purposes I will show you how to do this for each model covered in this book as there are some model and package specific differences in the predicting process. # optimal model cv_lasso &lt;- cv.glmnet(train_x, train_y, alpha = 1.0) min(cv_lasso$cvm) ## [1] 0.02529035 # predict and get RMSE pred &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x) caret::RMSE(pred, test_y) ## [1] 0.1220084 # re-transform predicted values and get interpretable RMSE caret::RMSE(exp(pred), exp(test_y)) ## [1] 24740.36 3.4.2 h2o To perform regularized regression with h2o, we first need to initiate our h2o session. ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 1 minutes 59 seconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_thv371 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.43 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we do not need to one-hot encode or standardize our variables as h2o will do this for us. However, we do want to normalize our response variable due to its skewness and then convert our training and test data to h2o objects. # convert training data to h2o object train_h2o &lt;- ames_train %&gt;% mutate(Sale_Price = log(Sale_Price)) %&gt;% as.h2o() # convert test data to h2o object test_h2o &lt;- ames_test %&gt;% mutate(Sale_Price = log(Sale_Price)) %&gt;% as.h2o() # set the response column to Sale_Price response &lt;- &quot;Sale_Price&quot; # set the predictor names predictors &lt;- setdiff(colnames(ames_train), response) 3.4.2.1 Basic implementation h2o.glm allows us to perform a generalized linear model. If the response variable is continuous, h2o.glm will use a gaussian distribution (see family parameter ?h2o.glm). By default, h2o.glm performs an elastic net model with alpha = .5. Similar to glmnet, h2o.glm will perform an automated search across internally generated lambda values. You can override the automated lambda search by supplying different values to the lambda parameters in h2o.glm but this is not recommended as the default parameters typically perform best. The following performs a default h2o.glm model with alpha = .5 and it performs a 10 fold cross validation (nfolds = 10). # train your model, where you specify alpha (performs 10-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;gaussian&quot; ) # print the MSE and RMSE for the validation data h2o.mse(h2o_fit1, xval = TRUE) ## [1] 0.0405933 h2o.rmse(h2o_fit1, xval = TRUE) ## [1] 0.2014778 If we check out the summary results of our model we get a bunch of information. Below is truncated printout which provides important model information such as the alpha applied and the optimal lambda value identified (\\(\\lambda = 0.056\\)), the number of predictors retained with these penalty parameters (11), and performance results for the training and validation sets. summary(h2o_fit1) ## Model Details: ## ============== ## ## H2ORegressionModel: glm ## Model Key: GLM_model_R_1531935157122_1 ## GLM Model: summary ## family link regularization number_of_predictors_total number_of_active_predictors number_of_iterations #training_frame ## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05581 ) 345 11 2 file13d1f4ffcacf3_sid_be66_16 ## ## H2ORegressionMetrics: glm ## ** Reported on training data. ** ## ## MSE: 0.03800294 ## RMSE: 0.1949434 ## MAE: 0.1277845 ## RMSLE: 0.01521899 ## Mean Residual Deviance : 0.03800294 ## R^2 : 0.7737851 ## Null Deviance :345.0615 ## Null D.o.F. :2053 ## Residual Deviance :78.05805 ## Residual D.o.F. :2042 ## AIC :-861.7687 ## ## ## ## H2ORegressionMetrics: glm ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.0405933 ## RMSE: 0.2014778 ## MAE: 0.1310502 ## RMSLE: 0.01567771 ## Mean Residual Deviance : 0.0405933 ## R^2 : 0.7583658 ## Null Deviance :345.4348 ## Null D.o.F. :2053 ## Residual Deviance :83.37864 ## Residual D.o.F. :2042 ## AIC :-726.3293 ## ## truncated..... ## 3.4.2.2 Tuning As previously stated, a full grid search to identify the optimal alpha is not always necessary; changing its value to 0.5 (or 0 or 1 if we only want Ridge or Lasso, respectively) works in most cases. However, if a full grid search is desired then we need to supply our grid of alpha values in a list. We can then use h2o.grid to perform our grid search. The results show that \\(\\alpha = 0\\) (a full ridge penalty) performed best. This grid search took 5 seconds to compute. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .1)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, grid_id = &quot;grid_search&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search ## Used hyper parameters: ## - alpha ## Number of models: 11 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## alpha model_ids mse ## 1 [0.0] grid_search_model_0 0.022711429277773212 ## 2 [0.6] grid_search_model_6 0.0395992947950626 ## 3 [0.7] grid_search_model_7 0.03968294882574923 ## 4 [1.0] grid_search_model_10 0.03983076501706063 ## 5 [0.4] grid_search_model_4 0.039850863900647 ## 6 [0.8] grid_search_model_8 0.03990607242146858 ## 7 [0.3] grid_search_model_3 0.04019054535587161 ## 8 [0.5] grid_search_model_5 0.04042328742741519 ## 9 [0.9] grid_search_model_9 0.04044324344598643 ## 10 [0.2] grid_search_model_2 0.041434990130949347 ## 11 [0.1] grid_search_model_1 0.04233746428242222 We can check out more details of the best performing model. Our RMSE (0.1279) is an improvement on our default model (RMSE = 0.2015). Also, we can access the optimal model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0279. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model) ## [1] 0.0163625 h2o.rmse(best_model) ## [1] 0.127916 # get optimal parameters best_model@parameters$lambda ## [1] 0.02790355 best_model@parameters$alpha ## [1] 0 3.4.2.3 Feature interpretation h2o provides a built-in function that plots variable importance. To compute variable importance for regularized models, h2o uses the standardized coefficient values (which is the same that we plotted in the glmnet example). You will notice that the the largest influential variables produced by this H2O model differ from the glmnet model. This is because we are assessing the ridge model here, where in the glmnet interpretation section we assessed the lasso and H2O and glmnet use differing approaches to generate the \\(\\lambda\\) search path. However, you will notice that both Overall_Qual.Excellent and Overall_Cond.Fair were also top influencers in the glmnet model suggesting they may have a strong signal regardless of the regularization penalty we use. # get top 25 influential variables h2o.varimp_plot(best_model, num_of_features = 25) Figure 3.10: H2O’s variable importance plot. Provides the same output as plotting the standardized coefficients (h2o.std_coef_plot). Although H2O’s variable importance uses standardized coefficients, a convenient function of H2O is that you can extract the “normal” coefficients which are obtained from the standardized coefficients by reversing the data standardization process (de-scaled, with the intercept adjusted by an added offset) so that they can be applied to data in its original form (i.e. no standardization prior to scoring). For example, every one unit increase in pool size has a $1 decrease in sales price (since our response is log transformed we need to take the exponent \\(e^{-0.000183} = 0.999817\\)). These are not the same as coefficients of a model built on non-standardized data. best_model@model$coefficients_table ## Coefficients: glm coefficients ## names coefficients standardized_coefficients ## 1 Intercept -47.473537 11.713031 ## 2 Neighborhood.Bloomington_Heights -0.019249 -0.019249 ## 3 Neighborhood.Blueste 0.004638 0.004638 ## 4 Neighborhood.Briardale -0.009169 -0.009169 ## 5 Neighborhood.Brookside 0.008538 0.008538 ## ## --- ## names coefficients standardized_coefficients ## 341 Pool_Area -0.000183 -0.005963 ## 342 Misc_Val -0.000038 -0.022608 ## 343 Mo_Sold 0.000444 0.001224 ## 344 Year_Sold -0.007414 -0.009728 ## 345 Longitude -0.303402 -0.007786 ## 346 Latitude 0.948544 0.017592 In the glmnet section we discussed how regularized regression assumes a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. This constant change is given by the given coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. We can illustrate this with a partial dependence plot of the ground living area (square footage) variable. The PDP plot shows that the relationship is monotonic linear (assumes a constant increasing relationships). This PDP plot helps to show the typical values (and one standard error) of our response variable as the square footage of the ground floor living space increases. # partial dependence plots for top 2 influential variables h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Gr_Liv_Area&quot;) Figure 3.11: As the ground living area (square footage) of a home increases, we experience a constant increase in the mean predicted sale price. But what about the two most influential variables (Overall_Qual.Excellent and Overall_Cond.Fair)? These variables are a result of one-hot encoding the original overall quality (Overall_Qual) variable. We can assess a similar plot but must supply the original non-one-hot encoded variable name. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Overall_Qual&quot;) Figure 3.12: The mean predicted sale price for each level of the overall quality variable. Note how the plot does not align with the natural ordering of the predictor variable. Unfortunately, H2O’s function plots the categorical levels in alphabetical order. Alternatively, we can extract the results and plot them in their proper level order to make inference more intuitive. The following shows the marginal effect of the overall quality variable on sales price. It illustrates an interesting finding - the highest and lowest categories do not have the largest marginal sales price effects. It also shows that a house with a very good overall quality has, on average, a $6K higher sale price than a house with only a good overall quality. Alternatively, homes with below average quality receive a substantially lower sale price than homes with average quality. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Overall_Qual&quot;, plot = FALSE) %&gt;% as.data.frame() %&gt;% mutate( Overall_Qual = factor(Overall_Qual, levels = levels(ames$Overall_Qual)), mean_response = exp(mean_response)) %&gt;% ggplot(aes(mean_response, Overall_Qual)) + geom_point() + scale_x_continuous(labels = scales::dollar) + ggtitle(&quot;Average response for each Overall Quality level&quot;) Figure 3.13: The mean predicted sale price for each level of the overall quality variable. This plot now helps to illustrate how the mean predicted sale price changes based on the natural ordering of the overall quality predictor variable. See the model interpretation chapter for more details on variable importance and partial dependence plots. 3.4.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Similar to glmnet, we need to re-transform our predicted values to get a interpretable generalizable error. Our generalizable error is $24,147.60 which is about $1,000 lower than the glmnet model produced. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict ## 1 11.65765 ## 2 11.48417 ## 3 12.50938 ## 4 12.32368 ## 5 13.20056 ## 6 12.69091 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2ORegressionMetrics: glm ## ## MSE: 0.01364287 ## RMSE: 0.1168027 ## MAE: 0.0826363 ## RMSLE: 0.009030871 ## Mean Residual Deviance : 0.01364287 ## R^2 : 0.915491 ## Null Deviance :141.57 ## Null D.o.F. :875 ## Residual Deviance :11.95116 ## Residual D.o.F. :530 ## AIC :-582.0349 # convert predicted values to non-transformed caret::RMSE(as.vector(exp(pred)), ames_test$Sale_Price) ## [1] 24147.6 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) 3.5 Implementation: Binary Classification To illustrate various regularization concepts for a binary classification problem we will use the employee attrition data, where the goal is to predict whether or not an employee attrits (“Yes” vs. “No”). The easiest way to have consistent interpretable results is to recode the response as 1 for the positive class (“Yes”) and 0 for the other class (“No”). attrition &lt;- rsample::attrition %&gt;% mutate(Attrition = recode(Attrition, &quot;Yes&quot; = 1, &quot;No&quot; = 0)) %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) 3.5.1 glmnet Similar to the regression application, for the classification data set we need to perfom some extra processing up-front to prepare for modeling with glmnet. First, many of the features are categorical; consequently, we need to either ordinal encode or one-hot encode these variables so that all features are numeric. Also, glmnet does not use the formula method (y ~ x) so prior to modeling we need to create our feature and target set and convert our features to a matrix. Since our response variable is imbalanced, we use rsample and strat to perform stratified sampling so that both our training and testing data sets have similar proportion of response levels. # one-hot encode our data with model.matrix one_hot &lt;- model.matrix( ~ ., attrition)[, -1] %&gt;% as.data.frame() # Create training and testing sets set.seed(123) split &lt;- initial_split(one_hot, prop = .8, strata = &quot;Attrition&quot;) train &lt;- training(split) test &lt;- testing(split) # separate features from response variable for glmnet train_x &lt;- train %&gt;% select(-Attrition) %&gt;% as.matrix() train_y &lt;- train$Attrition test_x &lt;- test %&gt;% select(-Attrition) %&gt;% as.matrix() test_y &lt;- test$Attrition # check that train &amp; test sets have common response ratio table(train_y) %&gt;% prop.table() ## train_y ## 0 1 ## 0.8385726 0.1614274 table(test_y) %&gt;% prop.table() ## test_y ## 0 1 ## 0.8395904 0.1604096 3.5.1.1 Basic implementation Similar to the regression problem, we apply a regularized classification model with glmnet::glmnet(). The primary difference is that for binary classification models we need to include family = binomial. Remember, the alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)) model. # Apply Ridge regression to attrition data ridge &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.14: Coefficients for our ridge regression model as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). We can also directly access the coefficients for a model using coef and tidy the output with tidy. Here, we check out the top 10 largest absolute coefficient terms when using the smallest and largest lambda values. We see that regardless of a large or small penalty parameter, working overtime and being a Sales Rep has the largest positive influence on the probability of attrition. Whereas being a Research Director and having high job involvement (among others) are the strongest influencers for reducing the probability of attrition. # lambdas applied to penalty parameter ridge$lambda %&gt;% head() ## [1] 86.49535 78.81134 71.80996 65.43056 59.61789 54.32160 # small lambda results in large coefficients coef(ridge)[, 100] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 58 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 OverTimeYes 1.68 ## 2 JobInvolvementVery_High -1.42 ## 3 JobRoleSales_Representative 1.35 ## 4 (Intercept) 1.21 ## 5 BusinessTravelTravel_Frequently 1.17 ## 6 JobRoleResearch_Director -1.05 ## 7 JobInvolvementHigh -0.974 ## 8 EnvironmentSatisfactionVery_High -0.951 ## 9 JobSatisfactionVery_High -0.932 ## 10 WorkLifeBalanceBetter -0.908 ## # ... with 48 more rows # large lambda results in small coefficients coef(ridge)[, 1] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 58 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -1.65e+ 0 ## 2 JobRoleSales_Representative 2.62e-37 ## 3 OverTimeYes 1.95e-37 ## 4 JobRoleResearch_Director -1.55e-37 ## 5 MaritalStatusSingle 1.39e-37 ## 6 JobRoleManager -1.24e-37 ## 7 BusinessTravelTravel_Frequently 1.16e-37 ## 8 JobRoleLaboratory_Technician 9.16e-38 ## 9 JobRoleManufacturing_Director -8.95e-38 ## 10 JobInvolvementVery_High -7.66e-38 ## # ... with 48 more rows However, at this point, we do not understand how much improvement we are experiencing in our loss function across various \\(\\lambda\\) values. 3.5.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation with cv.glmnet. Here we perform a 10-fold CV glmnet model for both a ridge and lasso penalty. By default, cv.glmnet uses deviance as the loss function for binomial classification but you could adjust type.measure to “auc”, “mse”, “mae”, or “class” (missclassification). # Apply CV Ridge regression to attrition data ridge &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) # Apply CV Ridge regression to attrition data lasso &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 1 ) # plot results par(mfrow = c(1, 2)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.15: 10-fold cross validation deviance for a ridge and lasso model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest deviance and the second represents the \\(\\lambda\\) with a deviance within one standard error of the minimum deviance. Our plots above illustrate the 10-fold CV deviance across the \\(\\lambda\\) values. With the ridge model, we don’t see any improvement in the loss function as \\(\\lambda\\) increases but, with the lasso model, we see a slight improvement in the deviance as our penalty \\(\\lambda\\) gets larger, suggesting that a regular logistic regression model likely overfits our data. But as we constrain it further (continue to increase the penalty), our deviance starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases, with the optimal model containing between 38-48 predictor variables. The first and second vertical dashed lines represent the \\(\\lambda\\) value with the minimum deviance and the largest \\(\\lambda\\) value within one standard error of the minimum deviance. # Ridge model min(ridge$cvm) # minimum deviance ## [1] 0.6483054 ridge$lambda.min # lambda for this min deviance ## [1] 0.0104184 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min deviance ## [1] 0.6788463 ridge$lambda.1se # lambda for this deviance ## [1] 0.04615997 # Lasso model min(lasso$cvm) # minimum deviance ## [1] 0.6511349 lasso$lambda.min # lambda for this min deviance ## [1] 0.001737893 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min deviance ## [1] 0.6737481 lasso$lambda.1se # lambda for this deviance ## [1] 0.005307275 We can assess this visually. Here we plot the coefficients across the \\(\\lambda\\) values and the dashed red line represents the \\(\\lambda\\) with the smallest deviance and the dashed blue line represents largest \\(\\lambda\\) that falls within one standard error of the minimum deviance. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy. Above, we saw that both ridge and lasso penalties provide similiar minimum deviance scores; however, these plots illustrate that ridge is still using all 57 variables whereas the lasso model can get a similar deviance by reducing our feature set from 57 down to 48. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 38 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 38 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor. # Ridge model ridge_min &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) # Lasso model lasso_min &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 1 ) par(mfrow = c(1, 2)) # plot ridge model plot(ridge_min, xvar = &quot;lambda&quot;, main = &quot;Ridge penalty\\n\\n&quot;) abline(v = log(ridge$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(ridge$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) # plot lasso model plot(lasso_min, xvar = &quot;lambda&quot;, main = &quot;Lasso penalty\\n\\n&quot;) abline(v = log(lasso$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(lasso$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Figure 3.16: Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. So far we’ve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas alpha \\(\\rightarrow 0\\) will have a heavier ridge penalty applied and alpha \\(\\rightarrow 1\\) will have a heavier lasso penalty. lasso &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1.0) elastic1 &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.25) elastic2 &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.75) ridge &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.0) par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1) plot(lasso, xvar = &quot;lambda&quot;, main = &quot;Lasso (Alpha = 1)\\n\\n\\n&quot;) plot(elastic1, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .25)\\n\\n\\n&quot;) plot(elastic2, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .75)\\n\\n\\n&quot;) plot(ridge, xvar = &quot;lambda&quot;, main = &quot;Ridge (Alpha = 0)\\n\\n\\n&quot;) Figure 3.17: Coefficients for various penalty parameters. Often, the optimal model contains an alpha somewhere between 0-1, thus we want to tune both the \\(\\lambda\\) and the alpha parameters. As we did in the regression section, we create a common fold_id, which just allows us to apply the same CV folds to each model. We then create a tuning grid that searches across a range of alphas from 0-1, and empty columns where we’ll dump our model results into. Use caution when including \\(\\alpha = 0\\) or \\(\\alpha = 1\\) in the grid search. \\(\\alpha = 0\\) will produce a dense solution and it can be very slow (or even impossible) to compute in large N situations. \\(\\alpha = 1\\) has no \\(\\ell_2\\) penalty, so it is therefore less numerically stable and can be very slow as well due to slower convergence. If you experience slow computation, we recommend searching across \\(\\alpha\\) values of 0.1, .25, .5, .75, .9. # maintain the same folds across all models fold_id &lt;- sample(1:10, size = length(train_y), replace=TRUE) # search across a range of alphas tuning_grid &lt;- tibble::tibble( alpha = seq(0, 1, by = .1), dev_min = NA, dev_1se = NA, lambda_min = NA, lambda_1se = NA ) Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error deviance values and their respective \\(\\lambda\\) values. This grid search took 36 seconds. # Warning - this is not fast! See H2O section for faster approach for(i in seq_along(tuning_grid$alpha)) { # fit CV model for each alpha value fit &lt;- cv.glmnet( train_x, train_y, family = &quot;binomial&quot;, alpha = tuning_grid$alpha[i], foldid = fold_id ) # extract MSE and lambda values tuning_grid$dev_min[i] &lt;- fit$cvm[fit$lambda == fit$lambda.min] tuning_grid$dev_1se[i] &lt;- fit$cvm[fit$lambda == fit$lambda.1se] tuning_grid$lambda_min[i] &lt;- fit$lambda.min tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se } tuning_grid ## # A tibble: 11 x 5 ## alpha dev_min dev_1se lambda_min lambda_1se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.656 0.699 0.00949 0.0735 ## 2 0.1 0.656 0.696 0.00685 0.0441 ## 3 0.2 0.655 0.696 0.00599 0.0320 ## 4 0.3 0.655 0.698 0.00481 0.0257 ## 5 0.4 0.655 0.699 0.00434 0.0211 ## 6 0.5 0.655 0.697 0.00381 0.0169 ## 7 0.6 0.655 0.699 0.00349 0.0155 ## 8 0.7 0.656 0.698 0.00299 0.0132 ## 9 0.8 0.656 0.697 0.00287 0.0116 ## 10 0.9 0.656 0.701 0.00255 0.0113 ## 11 1 0.656 0.700 0.00230 0.0102 If we plot the deviance ± one standard error for the optimal \\(\\lambda\\) value for each alpha setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model (alpha = 1) with \\(\\lambda = 0.0023\\), gain the benefits of its feature selection capability and reasonably assume no loss in accuracy. tuning_grid %&gt;% mutate(se = dev_1se - dev_min) %&gt;% ggplot(aes(alpha, dev_min)) + geom_line(size = 2) + geom_ribbon(aes(ymax = dev_min + se, ymin = dev_min - se), alpha = .25) + ggtitle(&quot;Deviance ± one standard error&quot;) Figure 3.18: MSE ± one standard error for different alpha penalty parameters. 3.5.1.3 Feature interpretation 3.5.1.3.1 Variable importance Similar to the regularized regression models, regularized classification models assume a monotonic linear relationship between the predictor variables and the response. The primary difference is in what the linear relationship means. For binary classification models, the linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the log-odds probability of the response variable. This constant change is represented by the given coefficient for a predictor. Log odds are an alternate way of expressing probabilities. The odds of a positive outcome are simply represented as \\(\\frac{prob(positive\\_outcome)}{prob(negative\\_outcome)}\\). So if there is a .2 probability of an employee attriting, the odds ratio is \\(.2 \\div .8 = .25\\). Consequently, the log odds for this employee is \\(log(.25) = -1.386294\\). An employee with 50% change of attriting has a log odds of \\(log(.5 \\div .5) = 0\\) so negative log odds means greater probability of not attriting and positive log odds means greater probability of attriting. Consequently, this makes understanding variable relationships simple with regularized models. Those variables with largest positive coefficients have the strongest influence on increasing the probability of attrition whereas those variables with the largest negative coefficients have the stongest influence on decreasing the probability of attrition. I’ll illustrate with the following models. lasso &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1) ridge &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0) Our ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection but it will typically push most variables to near zero and allow the most influential variables to be more prominent. The following extracts the coefficients of our ridge model and plots predictor coefficients. You can see that some of the variables have coefficients closer to zero but there are also many variables that have a strong positive influence on the probability of attrition (i.e. OverTimeYes, JobRoleSales_Representative, BusinessTravelTravel_Frequently) and there are others that have a strong negative influence on the probability of attrition (i.e. JobInvolvementVery_High, JobRoleResearch_Director, JobInvolvementHigh). coef(ridge, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (ridge penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.19: A ridge penalty will retain all variables but push many coefficients to near zero. Consequently, we retain any minor signals that all features provide but those coefficients with the largest absolute values represent the most influential predictors in our model. Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those features most influential to predictive accuracy. If we select the model with the minimum deviance and plot all variables in that model we see similar results to the ridge model regarding the most influential variables. However, the lasso model has pushed 9 variables to have zero coefficients and has retained the remaining 48, effectively performing automated feature selection. coef(lasso, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (lasso penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.20: A lasso penalty will perform feature selection by pushing coefficients to zero. Consequently, we can view all coefficients to see which features were selected; however, our objective usually is still to identify those features with the strongest signal (largest absolute coefficient values). 3.5.1.3.2 ROC curve We can visualize the ROC curve with the ROCR and pROC packages. Both packages compare the predicted log-odds output (pred) to the actual observed class. ROC curves become more interesting and useful when we compare multiple models, which we will see in later chapters. If you do not include legacy.axes = TRUE in the plot() call for the pROC curve, your x-axis will be reversed. library(ROCR) library(pROC) # predict pred &lt;- predict(ridge, s = ridge$lambda.min, train_x) # plot structure par(mfrow = c(1, 2)) # ROCR plot prediction(pred, train_y) %&gt;% performance(&quot;tpr&quot;, &quot;fpr&quot;) %&gt;% plot(main = &quot;ROCR ROC curve&quot;) #pROC plot roc(train_y, as.vector(pred)) %&gt;% plot(main = &quot;pROC ROC curve&quot;, legacy.axes = TRUE) 3.5.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. Two caveats: You need to supply predict an s parameter with the preferred model’s \\(\\lambda\\) value. For example, here we create a lasso model, which provides me a minimum deviance of 0.648. We use the \\(\\lambda\\) for this model by specifying s = cv_lasso$lambda.min. The default predicted values are the log odds. If you want the probability, include type = &quot;response&quot;. If you want to predict the categorical response, include type = &quot;class&quot;. # optimal model cv_lasso &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1.0) min(cv_lasso$cvm) ## [1] 0.6475805 # predict and get log-odds pred_log_odds &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x) head(pred_log_odds) ## 1 ## 2 -4.11789899 ## 3 0.18751231 ## 14 -3.26688392 ## 25 -2.17385203 ## 39 0.02202369 ## 46 -4.44305986 # predict probability pred_probs &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x, type = &quot;response&quot;) head(pred_probs) ## 1 ## 2 0.01601793 ## 3 0.54674120 ## 14 0.03672490 ## 25 0.10212328 ## 39 0.50550570 ## 46 0.01162321 # predict and get predicted class pred_class &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x, type = &quot;class&quot;) head(pred_class) ## 1 ## 2 &quot;0&quot; ## 3 &quot;1&quot; ## 14 &quot;0&quot; ## 25 &quot;0&quot; ## 39 &quot;1&quot; ## 46 &quot;0&quot; Lastly, to assess various performance metrics on our test data we can use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. We can see that the no information rate is 0.8396. This represents the ratio of non-attrition to attrition rates. The goal is to increase prediction accuracy over and above this rate. We see that our overall accuracy is 0.887. The primary weakness in our model is that for many employees that attrit, we tend to predict non-attrit. This is illustrated by our low sensitivity. The positive argument allows you to specify which value corresponds to the “positive” result. This will impact how you interpret certain metrics that are based on true positive and false negative results (i.e. sensitivity, specificity). caret::confusionMatrix(factor(pred_class), factor(test_y), positive = &quot;1&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 238 25 ## 1 8 22 ## ## Accuracy : 0.8874 ## 95% CI : (0.8455, 0.9212) ## No Information Rate : 0.8396 ## P-Value [Acc &gt; NIR] : 0.013017 ## ## Kappa : 0.5102 ## Mcnemar&#39;s Test P-Value : 0.005349 ## ## Sensitivity : 0.46809 ## Specificity : 0.96748 ## Pos Pred Value : 0.73333 ## Neg Pred Value : 0.90494 ## Prevalence : 0.16041 ## Detection Rate : 0.07509 ## Detection Prevalence : 0.10239 ## Balanced Accuracy : 0.71778 ## ## &#39;Positive&#39; Class : 1 ## 3.5.2 h2o To perform regularized logistic regression with h2o, we first need to initiate our h2o session. h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 1 minutes 59 seconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_thv371 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.43 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we do not need to one-hot encode or standardize our variables as h2o will do this for us. However, we do need to convert our training and test data to h2o objects. # Create training and testing sets set.seed(123) split &lt;- initial_split(attrition, prop = .8, strata = &quot;Attrition&quot;) train &lt;- training(split) test &lt;- testing(split) # convert training data to h2o object train_h2o &lt;- as.h2o(train) # convert test data to h2o object test_h2o &lt;- as.h2o(test) # set the response column to Attrition response &lt;- &quot;Attrition&quot; # set the predictor names predictors &lt;- setdiff(colnames(train), &quot;Attrition&quot;) 3.5.2.1 Basic implementation Similar to our regression problem, we use h2o.glm to perform a regularized logistic regression model. The primary difference is that we need to set family = &quot;binomial&quot; to signal a binary classification problem. As before, by default, h2o.glm performs an elastic net model with alpha = .5 and will perform an automated search across internally generated lambda values. The following performs a default h2o.glm model with alpha = .5 and it performs a 10 fold cross validation (nfolds = 10). # train your model, where you specify alpha (performs 10-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;binomial&quot; ) # print the MSE and AUC for the validation data h2o.mse(h2o_fit1, xval = TRUE) ## [1] 0.09739988 h2o.auc(h2o_fit1, xval = TRUE) ## [1] 0.8345038 We can check out the cross-validated performance results of our model with h2o.performance. You can also get more results information using summary(h2o_fit1). h2o.performance(h2o_fit1, xval = TRUE) ## H2OBinomialMetrics: glm ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.09739988 ## RMSE: 0.3120895 ## LogLoss: 0.3347975 ## Mean Per-Class Error: 0.258244 ## AUC: 0.8345038 ## Gini: 0.6690076 ## R^2: 0.2804837 ## Residual Deviance: 788.1133 ## AIC: 914.1133 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 898 89 0.090172 =89/987 ## 1 81 109 0.426316 =81/190 ## Totals 979 198 0.144435 =170/1177 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.351656 0.561856 156 ## 2 max f2 0.127301 0.631939 267 ## 3 max f0point5 0.497019 0.608696 105 ## 4 max accuracy 0.728176 0.875106 54 ## 5 max precision 0.989718 1.000000 0 ## 6 max recall 0.001056 1.000000 396 ## 7 max specificity 0.989718 1.000000 0 ## 8 max absolute_mcc 0.497019 0.478350 105 ## 9 max min_per_class_accuracy 0.140575 0.757852 257 ## 10 max mean_per_class_accuracy 0.234064 0.762571 209 3.5.2.2 Tuning Next, we’ll use h2o.grid to perform our grid search. The results show that \\(\\alpha = .1\\) performed best; however, the improvement is marginal. This grid search took 7 seconds. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .1)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, family = &quot;binomial&quot;, grid_id = &quot;grid_search_glm_classification&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search_glm_classification&quot;, sort_by = &quot;logloss&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search_glm_classification ## Used hyper parameters: ## - alpha ## Number of models: 11 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## alpha model_ids logloss ## 1 [0.1] grid_search_glm_classification_model_1 0.32729191697214505 ## 2 [0.4] grid_search_glm_classification_model_4 0.3308752559420129 ## 3 [0.2] grid_search_glm_classification_model_2 0.33248215525410824 ## 4 [0.8] grid_search_glm_classification_model_8 0.33249327956862135 ## 5 [0.7] grid_search_glm_classification_model_7 0.3348307314321876 ## 6 [0.5] grid_search_glm_classification_model_5 0.3354479575686281 ## 7 [1.0] grid_search_glm_classification_model_10 0.33563334658821187 ## 8 [0.3] grid_search_glm_classification_model_3 0.33779138839826356 ## 9 [0.0] grid_search_glm_classification_model_0 0.338497245432312 ## 10 [0.6] grid_search_glm_classification_model_6 0.33925009415446955 ## 11 [0.9] grid_search_glm_classification_model_9 0.33927152577597053 We can check out more details of the best performing model. Our AUC (.84) is no better than our default cross-validated model. We can also extract other model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0006. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model, xval = TRUE) ## [1] 0.09538328 h2o.auc(best_model, xval = TRUE) ## [1] 0.8384072 # get optimal parameters best_model@parameters$lambda ## [1] 0.0006111815 best_model@parameters$alpha ## [1] 0.1 3.5.2.3 Feature interpretation 3.5.2.3.1 Feature importance To identify the most influential variables we can use h2o’s variable importance plot. Recall that for a GLM model, variable importance is simply represented by the standardized coefficients. We see that JobInvolvement.Low and JobRole.Sales_Representative have the largest influence in increasing the probability of attrition whereas JobRole.Research_Director and OverTime.No have the largest influence in decreasing the probability of attrition. # get top 25 influential variables h2o.varimp_plot(best_model, num_of_features = 25) Figure 3.21: H2O’s variable importance plot. Provides the same output as plotting the standardized coefficients (h2o.std_coef_plot). To illustrate how the predicted response changes based on these influential variables, we can leverage the partial dependence inforamtion. For example, we can assess the partial dependence plots of the JobInvolvement predictor which shows up at the top of our variable importance plot. We see that mean predicted probability of attrition increases as job involvement decreases; but we can also see the significant increase in probability when an employee has a low job involvement. # partial dependence plots for top 2 influential variables h2o.partialPlot(best_model, data = train_h2o, cols = &quot;JobInvolvement&quot;, plot = FALSE) %&gt;% as.data.frame() %&gt;% mutate(JobInvolvement = factor(JobInvolvement, levels = levels(attrition$JobInvolvement))) %&gt;% ggplot(aes(JobInvolvement, mean_response)) + geom_col() + ggtitle(&quot;Average predicted probability of attrition&quot;) Figure 3.22: There is a significant increase in the predicted probability of attrition as an employee’s level of job involvement decreases to a low level. Similarly, for a continuous variable, we can assess the PDP. For example, age is one of the few continuous predictor variables in this data set and we see that our regularized logistic regression model predicts a continuously decreasing probability of attrition as employees get older. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Age&quot;) Figure 3.23: As an employee gets older, the predicted probability of attrition decreases. 3.5.2.3.2 ROC curve Earlier, we saw that this model produced a 10-fold CV AUC of .84. We can visualize this by plotting the ROC curve using the following: h2o.performance(best_model, xval = TRUE) %&gt;% plot() Figure 3.24: ROC curve for our best performing regularized H2O GLM model. 3.5.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Note how the h2o.predict function provides 3 columns - the predicted class and the probability of each class. We also produce our test set performance results with h2o.performance. If you compare the confusion matrix with the one produced by glmnet you will notice they produce very similar results. The h2o model does a slightly better job predicting true attrition but also does slightly worse in false positives. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict p0 p1 ## 1 0 0.9921441 0.007855913 ## 2 1 0.4230720 0.576927982 ## 3 0 0.9724310 0.027569028 ## 4 0 0.9093784 0.090621612 ## 5 1 0.4451610 0.554838982 ## 6 0 0.9863366 0.013663436 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2OBinomialMetrics: glm ## ## MSE: 0.0936628 ## RMSE: 0.3060438 ## LogLoss: 0.328802 ## Mean Per-Class Error: 0.2543678 ## AUC: 0.8418959 ## Gini: 0.6837917 ## R^2: 0.3045444 ## Residual Deviance: 192.678 ## AIC: 324.678 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 236 10 0.040650 =10/246 ## 1 22 25 0.468085 =22/47 ## Totals 258 35 0.109215 =32/293 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.499766 0.609756 34 ## 2 max f2 0.215378 0.690299 79 ## 3 max f0point5 0.600664 0.677419 26 ## 4 max accuracy 0.600664 0.890785 26 ## 5 max precision 0.991619 1.000000 0 ## 6 max recall 0.003581 1.000000 274 ## 7 max specificity 0.991619 1.000000 0 ## 8 max absolute_mcc 0.499766 0.555889 34 ## 9 max min_per_class_accuracy 0.215378 0.787234 79 ## 10 max mean_per_class_accuracy 0.215378 0.806219 79 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) 3.6 Implementation: Multinomial Classification To illustrate various regularization concepts for a multinomial classification problem we will use the mnist data, where the goal is to predict handwritten numbers ranging from 0-9. # import mnist training and testing data train &lt;- data.table::fread(&quot;../data/mnist_train.csv&quot;, data.table = FALSE) test &lt;- data.table::fread(&quot;../data/mnist_test.csv&quot;, data.table = FALSE) 3.6.1 glmnet Since the mnist data contains all numeric predictors (darkness density ranging from 0-255), we do not need to one-hot encode our feature set. Consequently, we only need to convert our features into a matrix and seperate the response variable (V785). For a multinomial problem our response needs to be either discrete integer values or set as a factor. Since our response values are discrete integer values from 0-9 we can leave them as is. # separate features from response variable for glmnet train_x &lt;- train %&gt;% select(-V785) %&gt;% as.matrix() train_y &lt;- train$V785 test_x &lt;- test %&gt;% select(-V785) %&gt;% as.matrix() test_y &lt;- test$V785 # check the response ratios across the train &amp; test sets table(train_y) %&gt;% prop.table() %&gt;% round(2) ## train_y ## 0 1 2 3 4 5 6 7 8 9 ## 0.10 0.11 0.10 0.10 0.10 0.09 0.10 0.10 0.10 0.10 table(test_y) %&gt;% prop.table() %&gt;% round(2) ## test_y ## 0 1 2 3 4 5 6 7 8 9 ## 0.10 0.11 0.10 0.10 0.10 0.09 0.10 0.10 0.10 0.10 3.6.1.1 Basic implementation To perform a regularized multinomial GLM, we simply change the family parameter to “multinomial”. In this example we perform a full ridge penalty (alpha = 0) model. One difference with a multinomial model is that when you plot the model results, rather than seeing only one plot with the coefficient values across the spectrum of \\(\\lambda\\) values, you will get an individual plot for the coefficients for each response category. # Apply Ridge regression to mnist data ridge &lt;- glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 0 ) par(mfrow = c(2, 5)) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.25: Coefficients for each multinomial response as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Also, the coefficients are stored in a list separated by the response category. We can access these coefficients in a similar manner as we did the regression and binary classification coefficients; however, we need to index for the response category of interest. For example, the following gets the coefficients for response category “9” (note the indexing: coef(ridge)$9). # coefficients for response &quot;9&quot; with small lambda penalty coef(ridge)$`9`[, 100] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 785 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;&quot; -0.428 ## 2 V170 -0.0762 ## 3 V753 0.0505 ## 4 V703 0.0345 ## 5 V780 -0.0317 ## 6 V732 0.0264 ## 7 V726 0.0246 ## 8 V505 0.0220 ## 9 V421 -0.0147 ## 10 V225 -0.0139 ## # ... with 775 more rows # coefficients for response &quot;9&quot; with small lambda penalty coef(ridge)$`9`[, 1] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 785 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;&quot; -7.12e- 3 ## 2 V753 3.18e-38 ## 3 V732 2.39e-38 ## 4 V170 -1.69e-38 ## 5 V16 -1.11e-38 ## 6 V703 8.13e-39 ## 7 V533 -6.68e-39 ## 8 V505 6.46e-39 ## 9 V33 -6.26e-39 ## 10 V752 4.64e-39 ## # ... with 775 more rows 3.6.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation with cv.glmnet as we did with the regression and binary classification examples. However, due to the magnitude of the data slowing glmnet down, rather than perform a full grid search across many alpha settings we only perform a 5-fold CV glmnet model for three alpha values: 1 (ridge), 0.5 (elastic net), and 2 (lasso). As your data set increases, glmnet begins to slow down considerably compared to h2o. Consequently, I reduced k to a 5-fold CV; however, to run a single 5-fold CV on this training set still took 109 minutes! If you parallelize the process with parallel = TRUE (which requires you to use doMC or some other parallelizer) you can achieve some speed improvements. # parallelize the process library(doMC) registerDoMC(cores = 4) # Apply CV Ridge regression to mnist data ridge &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 0, nfolds = 5, parallel = TRUE ) # Apply CV elastic net regression to mnist data elastic &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = .5, nfolds = 5, parallel = TRUE ) # Apply CV lasso regression to mnist data lasso &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 1, nfolds = 5, parallel = TRUE ) # plot results par(mfrow = c(1, 3)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(elastic, main = &quot;Elastic net penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.26: 5-fold cross validation deviance for a ridge, lasso, and elastic net model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest deviance and the second represents the \\(\\lambda\\) with a deviance within one standard error of the minimum deviance. Note how the top of the ridge penalty deviance plot indicates only 717 of the 784 predictors are in the model. The reason for this is not because the ridge model pushed 67 coefficients to zero because a ridge model does not do that. Rather, there are 67 predictors that have zero variance so they are not included in the model (see this with preProcess(train_x, “zv”)). In the performance plots in Figure 3.26 it is difficult to see if the models differ in their minimum error rate (multinomial deviance). But if we look at the minimum deviance (and the largest deviance within one standard error) in the below code chunk, we see that the elastic net provides the optimal performance. Consequently, it appears we could use between 250-300 of the 784 predictors and still achieve optimal performance. # Ridge model min(ridge$cvm) # minimum deviance ## [1] 0.6194693 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min deviance ## [1] 0.6194693 # Elastic net model min(elastic$cvm) # minimum deviance ## [1] 0.5627915 elastic$cvm[elastic$lambda == elastic$lambda.1se] # 1 st.error of min deviance ## [1] 0.5682305 # Lasso model min(lasso$cvm) # minimum deviance ## [1] 0.6557272 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min deviance ## [1] 0.6557272 So the elastic net model is minimizing the multinomial deviance loss function, but how does this translate to the accuracy of this model? We can assess the confusion matrix for this data on the training data. At first glance, the confusion matrix is difficult to discern differences. However, looking at the class statistics and specifically the sensitivity and specificity we can extract some useful insights. First, the specificity is 0.99 across all numbers indicating that the model does well in classifying non-events (basically, no number has a significantly higher false positive rate than the other numbers). Second, looking at the sensitivity, we can see that our model does the best at accurately predicting the numbers 0, 1, and 6. However, it does worst at accurately predicting the numbers 2, 3, 5, 8, and 9. The number 8 has the lowest sensitivity rate and when we look at the confusion matrix we can see that our model often classifies the number 8 as the numbers 5, 3, and 1. pred_class &lt;- predict(elastic, s = elastic$lambda.min, train_x, type = &quot;class&quot;) caret::confusionMatrix(factor(pred_class), factor(test_y)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 5780 1 26 16 11 43 27 10 28 19 ## 1 1 6580 48 28 27 20 15 25 115 25 ## 2 16 33 5452 126 24 38 33 59 56 14 ## 3 8 15 84 5547 9 145 0 19 125 78 ## 4 10 6 65 8 5506 50 30 47 26 135 ## 5 27 23 19 181 9 4882 69 10 129 36 ## 6 34 3 57 17 45 80 5714 3 36 2 ## 7 5 13 67 47 13 16 2 5911 15 157 ## 8 36 58 120 114 31 109 26 15 5251 41 ## 9 6 10 20 47 167 38 2 166 70 5442 ## ## Overall Statistics ## ## Accuracy : 0.9344 ## 95% CI : (0.9324, 0.9364) ## No Information Rate : 0.1124 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9271 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Statistics by Class: ## ## Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 0.97586 0.9760 0.91507 0.90475 0.94249 0.90057 0.96553 0.94350 0.89745 0.91478 ## Specificity 0.99665 0.9943 0.99262 0.99103 0.99304 0.99078 0.99488 0.99377 0.98984 0.99027 ## Pos Pred Value 0.96964 0.9558 0.93181 0.91990 0.93592 0.90659 0.95376 0.94637 0.90519 0.91186 ## Neg Pred Value 0.99735 0.9970 0.99066 0.98918 0.99379 0.99013 0.99622 0.99341 0.98893 0.99062 ## Prevalence 0.09872 0.1124 0.09930 0.10218 0.09737 0.09035 0.09863 0.10442 0.09752 0.09915 ## Detection Rate 0.09633 0.1097 0.09087 0.09245 0.09177 0.08137 0.09523 0.09852 0.08752 0.09070 ## Detection Prevalence 0.09935 0.1147 0.09752 0.10050 0.09805 0.08975 0.09985 0.10410 0.09668 0.09947 ## Balanced Accuracy 0.98625 0.9851 0.95384 0.94789 0.96776 0.94568 0.98020 0.96863 0.94365 0.95252 3.6.1.3 Feature interpretation Interpreting the underlying predictor mechanisms with a multinomial problem is much like the regression and binary classification problems but with a few extra nuances. The first thing to remember is that although we started with 784 predictors, our full ridge model only used 717 because 67 predictors had zero variance. This is not unique to multinomial problems but its important to understand which variables these are because, in an organizational situation, we can assess whether or not we should continue collecting this information. In this example, they primarily represent pixels along the very edge of the images. # zero variance predictor variables that offer no potential signal names(which(sapply(train, var) == 0)) ## [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; &quot;V5&quot; &quot;V6&quot; &quot;V7&quot; &quot;V8&quot; &quot;V9&quot; &quot;V10&quot; &quot;V11&quot; ## [12] &quot;V12&quot; &quot;V17&quot; &quot;V18&quot; &quot;V19&quot; &quot;V20&quot; &quot;V21&quot; &quot;V22&quot; &quot;V23&quot; &quot;V24&quot; &quot;V25&quot; &quot;V26&quot; ## [23] &quot;V27&quot; &quot;V28&quot; &quot;V29&quot; &quot;V30&quot; &quot;V31&quot; &quot;V32&quot; &quot;V53&quot; &quot;V54&quot; &quot;V55&quot; &quot;V56&quot; &quot;V57&quot; ## [34] &quot;V58&quot; &quot;V83&quot; &quot;V84&quot; &quot;V85&quot; &quot;V86&quot; &quot;V112&quot; &quot;V113&quot; &quot;V141&quot; &quot;V142&quot; &quot;V169&quot; &quot;V477&quot; ## [45] &quot;V561&quot; &quot;V645&quot; &quot;V646&quot; &quot;V672&quot; &quot;V673&quot; &quot;V674&quot; &quot;V700&quot; &quot;V701&quot; &quot;V702&quot; &quot;V728&quot; &quot;V729&quot; ## [56] &quot;V730&quot; &quot;V731&quot; &quot;V755&quot; &quot;V756&quot; &quot;V757&quot; &quot;V758&quot; &quot;V759&quot; &quot;V760&quot; &quot;V781&quot; &quot;V782&quot; &quot;V783&quot; ## [67] &quot;V784&quot; However, in a multinomial problem, each response category will not always use all the predictors that have variance. In fact, the below code shows that the number zero only uses 189 predictors whereas the number 3 uses 296. You can set type.multinomial = “grouped” within cv.glmnet() to force all predictors to be in or out together. coef(elastic) %&gt;% purrr::map(tidy) %&gt;% bind_rows(.id = &quot;response&quot;) %&gt;% count(response) ## # A tibble: 10 x 2 ## Response n ## &lt;chr&gt; &lt;int&gt; ## 1 0 189 ## 2 1 206 ## 3 2 281 ## 4 3 296 ## 5 4 272 ## 6 5 264 ## 7 6 267 ## 8 7 278 ## 9 8 228 ## 10 9 283 Moreover, each response category will use the predictors in different ways. Similar to the binary classification problem, the coefficients represent the change in log-odds probability of the response variable for a one unit change in the predictor. For example, below we see that for the response category “0”, a one unit increase in the darkness of the pixel represented by feature V41 causes a 0.0089 log odds increase that the response will be the number “0”. A 0.0089 log odds increase translates to \\(\\frac{e^{0.008912499}}{1+e^{0.008912499}} = 0.5022281\\) probability increase. The below code chunk extracts the coefficients for each response category, combines them all into a single data frame, and removes the intercept (since we just care about the predictor coefficients). # tidy up the coefficients for downstream assessment vi &lt;- coef(elastic) %&gt;% purrr::map(tidy) %&gt;% bind_rows(.id = &quot;response&quot;) %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% select(response, feature = row, coefficient = value) head(vi) ## response feature coefficient ## 1 0 V41 8.912499e-03 ## 2 0 V45 8.816906e-03 ## 3 0 V60 8.381852e-02 ## 4 0 V72 -2.083206e-04 ## 5 0 V106 -1.630429e-03 ## 6 0 V124 1.720361e-05 Similar to the binary classification problem, one of our main concerns is to understand which predictors have the largest influence on each response category. The following identifies the top 10 predictors with the largest absolute coefficients. These predictors represent those that have the largest impact to the probability of that response. For example, as features V60, V225, and V504 increase (those pixels become darker), they have the largest increase in the probability that the response will be “0”. Alternatively, as features V719 and V363 increase, they have the largest decrease in the probability that the response will be “0”. Since a given predictor can have a different coefficient for each response category, each response category can have their own unique variable importance list. vi %&gt;% group_by(response) %&gt;% top_n(10, wt = abs(coefficient)) %&gt;% mutate(predictor = paste(response, feature, sep = &quot;: &quot;)) %&gt;% ggplot(aes(coefficient, reorder(predictor, coefficient), color = coefficient &gt; 0)) + geom_point(show.legend = FALSE) + facet_wrap(~ response, scales = &quot;free_y&quot;, ncol = 5) + ylab(NULL) Figure 3.27: Top 10 most influential predictors for each multinomial response. Predictors with coefficients to the right of zero (blue) increase the probability of that response whereas predictors with coefficients to the left of zero (red) decrease the probability of that response. The above plot identifies thse variables most influential for each given response category. However, we also want to understand which variables are influential across all, or most, of the responses. The below identifies three predictors (V375, V515, V572) that have non-zero coefficients for all the response categories. Plotting these variables allows us to see how the strength and direction of the signal varies across the response categories. These features represent pixels in the images that are used by each handwritten number. # predictors that are influential across all or many of the responses vi %&gt;% count(feature) %&gt;% arrange(desc(n)) ## # A tibble: 653 x 2 ## feature n ## &lt;chr&gt; &lt;int&gt; ## 1 V375 10 ## 2 V515 10 ## 3 V572 10 ## 4 V402 9 ## 5 V239 8 ## 6 V268 8 ## 7 V301 8 ## 8 V324 8 ## 9 V326 8 ## 10 V353 8 ## # ... with 643 more rows vi %&gt;% filter(feature %in% c(&quot;V375&quot;, &quot;V515&quot;, &quot;V572&quot;)) %&gt;% ggplot(aes(coefficient, feature, fill = coefficient &gt; 0)) + geom_col() + coord_flip() + facet_wrap(~ response, ncol = 5) Figure 3.28: Three variables provide signals for all the response categories; however, the strength and direction of the signal can vary across the responses. Similarly, certain predictors may only provide a signal for only one response category. We also want to assess these as they provide a unique signal for a particular response. For example, feature V170 is only influential for response “3” and increases the probability by 0.525. # only 82 predictors that play a role in just one response singles &lt;- vi %&gt;% count(feature) %&gt;% filter(n == 1) %&gt;% .$feature vi %&gt;% filter(feature %in% singles) %&gt;% arrange(desc(abs(coefficient))) %&gt;% slice(1:10) %&gt;% mutate(prob = exp(coefficient) / (1 + exp(coefficient))) ## response feature coefficient prob ## 1 3 V170 0.10146013 0.5253433 ## 2 0 V60 0.08381852 0.5209424 ## 3 0 V225 0.07995005 0.5199769 ## 4 7 V780 0.05972853 0.5149277 ## 5 7 V534 0.03223708 0.5080586 ## 6 7 V778 0.02278233 0.5056953 ## 7 2 V392 0.02252209 0.5056303 ## 8 2 V82 0.02225574 0.5055637 ## 9 8 V335 0.01925485 0.5048136 ## 10 6 V88 0.01859690 0.5046491 Remember that these linear models assume a monotonic linear relationship between the predictors and the response; meaning that for a given response category, the relationship represented by the coefficient is constant. Therefore, global and local model interpreation will be the same. See more in the machine learning interpretability chapter. 3.6.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. Similar to the binary classification problem: You need to supply predict an s parameter with the preferred model’s \\(\\lambda\\) value. The default predicted values are the log odds. If you want the probability, include type = &quot;response&quot;. If you want to predict the categorical response, include type = &quot;class&quot;. Additionally, the predicted output is in the form of an array. Consequently, to assess the predicted values for the first five observations across all response categories, index with the following: # predict and get log-odds pred_log_odds &lt;- predict(elastic, s = elastic$lambda.min, test_x) pred_log_odds[1:5, , 1] ## 0 1 2 3 4 5 6 7 8 9 ## [1,] -5.432659 -1.339061 -2.333135 0.7305298 -3.606102 0.5498658 -6.628586 -6.8034856 6.3175820 0.353073 ## [2,] -8.302420 -5.464651 -3.410668 7.5492386 -4.806773 3.3335312 -4.941261 -0.1618101 0.7612956 2.373736 ## [3,] 7.266914 -8.224763 1.052633 1.6484586 -8.055438 -9.2497608 2.820177 -12.5006341 8.8967193 -10.473515 ## [4,] 8.415969 -10.700918 1.233072 -5.6888200 -2.364087 -0.6202491 5.928128 -7.3944607 0.2065292 -5.634506 ## [5,] -6.395731 7.228976 2.144250 -0.1299773 -4.381965 -2.0272380 -2.086397 -1.8958485 1.1838882 -2.138867 # predict probability pred_probs &lt;- predict(elastic, s = elastic$lambda.min, test_x, type = &quot;response&quot;) pred_probs[1:5, , 1] ## 0 1 2 3 4 5 6 7 8 9 ## [1,] 7.808164e-06 4.681405e-04 0.0001732428 3.708412e-03 4.850791e-05 3.095470e-03 2.361373e-06 1.982470e-06 0.9899515761 2.542498e-03 ## [2,] 1.277183e-07 2.181128e-06 0.0000170104 9.784562e-01 4.211081e-06 1.444385e-02 3.681174e-06 4.382031e-04 0.0011029991 5.531581e-03 ## [3,] 1.633925e-01 3.056908e-08 0.0003268920 5.931549e-04 3.620925e-08 1.096811e-08 1.914428e-03 4.249083e-10 0.8337729101 3.225981e-09 ## [4,] 9.222861e-01 4.597347e-09 0.0007004437 6.906115e-07 1.919319e-05 1.097706e-04 7.663202e-02 1.254537e-07 0.0002509293 7.291585e-07 ## [5,] 1.198730e-06 9.905039e-01 0.0061317946 6.308170e-04 8.980257e-06 9.460923e-05 8.917459e-05 1.078935e-04 0.0023469721 8.461622e-05 # predict and get predicted class pred_class &lt;- predict(elastic, s = elastic$lambda.min, test_x, type = &quot;class&quot;) head(pred_class) ## 1 ## [1,] &quot;8&quot; ## [2,] &quot;3&quot; ## [3,] &quot;8&quot; ## [4,] &quot;0&quot; ## [5,] &quot;1&quot; ## [6,] &quot;5&quot; Lastly, to assess various performance metrics on our test data we can use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. We can see that the overall accuracy rate is 0.9281 and our model does well predicting “0”, “1”, and “6” but poorly predicting “2”, “3”, “5”, “8”, and “9”. caret::confusionMatrix(factor(pred_class), factor(test_y)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 959 0 7 3 1 8 10 2 6 10 ## 1 0 1114 8 1 1 2 3 9 7 8 ## 2 1 2 929 16 4 2 4 22 6 1 ## 3 1 2 15 921 1 33 2 5 21 10 ## 4 0 0 7 0 919 11 7 5 10 25 ## 5 7 1 3 29 0 779 14 0 24 5 ## 6 6 4 12 2 12 14 913 0 9 0 ## 7 5 2 11 10 6 8 3 954 10 22 ## 8 1 10 36 20 8 31 2 1 871 6 ## 9 0 0 4 8 30 4 0 30 10 922 ## ## Overall Statistics ## ## Accuracy : 0.9281 ## 95% CI : (0.9229, 0.9331) ## No Information Rate : 0.1135 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9201 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 0.9786 0.9815 0.9002 0.9119 0.9358 0.8733 0.9530 0.9280 0.8943 0.9138 ## Specificity 0.9948 0.9956 0.9935 0.9900 0.9928 0.9909 0.9935 0.9914 0.9873 0.9904 ## Pos Pred Value 0.9533 0.9662 0.9412 0.9110 0.9339 0.9037 0.9393 0.9253 0.8834 0.9147 ## Neg Pred Value 0.9977 0.9976 0.9886 0.9901 0.9930 0.9876 0.9950 0.9917 0.9886 0.9903 ## Prevalence 0.0980 0.1135 0.1032 0.1010 0.0982 0.0892 0.0958 0.1028 0.0974 0.1009 ## Detection Rate 0.0959 0.1114 0.0929 0.0921 0.0919 0.0779 0.0913 0.0954 0.0871 0.0922 ## Detection Prevalence 0.1006 0.1153 0.0987 0.1011 0.0984 0.0862 0.0972 0.1031 0.0986 0.1008 ## Balanced Accuracy 0.9867 0.9885 0.9469 0.9509 0.9643 0.9321 0.9733 0.9597 0.9408 0.9521 3.6.2 h2o To perform regularized multinomial logistic regression with h2o, we first need to initiate our h2o session. h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 minutes 366 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_thv371 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.43 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Since our data is already split into a training test set, to prepare for modeling, we just need to convert our training and test data to h2o objects and identify the response and predictor variables. One key difference compared to prior classification procedures, to perform a multinomial modeling with h2o we need to convert the response variable to a factor. # convert training data to h2o object train_h2o &lt;- train %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # convert test data to h2o object test_h2o &lt;- test %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # set the response column to V785 response &lt;- &quot;V785&quot; # set the predictor names predictors &lt;- setdiff(colnames(train), response) 3.6.2.1 Basic implementation Similar to our regression and binary classification problem, we use h2o.glm to perform a regularized multinomial logistic regression model. The primary difference is that we need to set family = &quot;multinomial&quot; to signal a multinomial classification problem. As before, by default, h2o.glm performs an elastic net model with alpha = .5 and will perform an automated search across internally generated lambda values. The following performs a default multinomial h2o.glm model with alpha = .5 and it performs a 5 fold cross validation (nfolds = 10). When you are working with large data sets, h2o provides a far more efficient approach for regularized models. Whereas glmnet took over an hour to perform a 5-fold cross validated regularized model on the mnist data, h2o took less than 2.5 minutes! # train your model, where you specify alpha (performs 5-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 5, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;multinomial&quot; ) We can check out the cross-validated performance results of our model with h2o.performance. You can also get more results information using summary(h2o_fit1). One unique output worth discussing is the “Top-10 Hit Ratios” table. The first line of this table (k = 1, hit_ratio = 0.9213) represents the mean accuracy of our model across the 5-fold validated set. However, the second line tells us that when we take those missed predictions and use the second highest predicted probability we get a mean accuracy of 96.85% (we get an additional \\((0.9685 - 0.9213) \\times 60000 = 2832\\) observations correct.) After only 4 reapplications we are able to achieve 99% accuracy. h2o.performance(h2o_fit1, xval = TRUE) ## H2OMultinomialMetrics: glm ## ** Reported on cross-validation data. ** ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## Cross-Validation Set Metrics: ## ===================== ## ## Extract cross-validation frame with `h2o.getFrame(&quot;filea18f736ccac9_sid_8725_4&quot;)` ## MSE: (Extract with `h2o.mse`) 0.07378137 ## RMSE: (Extract with `h2o.rmse`) 0.2716273 ## Logloss: (Extract with `h2o.logloss`) 0.2843358 ## Mean Per-Class Error: 0.07972688 ## Null Deviance: (Extract with `h2o.nulldeviance`) 276156.9 ## Residual Deviance: (Extract with `h2o.residual_deviance`) 34193.91 ## R^2: (Extract with `h2o.r2`) 0.9911615 ## AIC: (Extract with `h2o.aic`) NaN ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,xval = TRUE)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.921300 ## 2 2 0.968500 ## 3 3 0.984300 ## 4 4 0.991517 ## 5 5 0.995133 ## 6 6 0.997200 ## 7 7 0.998400 ## 8 8 0.999183 ## 9 9 0.999800 ## 10 10 1.000000 3.6.2.2 Tuning Since h2o is much faster than glmnet, we can perform a grid search across a wider range of alpha parameters. Here, I assess alphas equal to 0, 0.25, 0.5, 0.75, and 1 using h2o.grid to perform our grid search. The results show that \\(\\alpha = .25\\) performed best. The results also show that 4 out of 5 models have minor differences in the Log Loss loss function; however, \\(\\alpha = .0\\) (full ridge penalty) definitely performs the worst. This grid search took 20 minutes. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .25)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, family = &quot;multinomial&quot;, grid_id = &quot;grid_search_glm_multinomial&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search_glm_multinomial&quot;, sort_by = &quot;logloss&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search_glm_multinomial ## Used hyper parameters: ## - alpha ## Number of models: 5 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## alpha model_ids logloss ## 1 [0.25] grid_search_glm_multinomial_model_1 0.2826176758945207 ## 2 [1.0] grid_search_glm_multinomial_model_4 0.2833413952701668 ## 3 [0.75] grid_search_glm_multinomial_model_3 0.2834683548256082 ## 4 [0.5] grid_search_glm_multinomial_model_2 0.2855257887768779 ## 5 [0.0] grid_search_glm_multinomial_model_0 0.31518212134903734 We can check out more details of the best performing model. Our AUC (.84) is no better than our default cross-validated model. We can also extract other model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0006. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model, xval = TRUE) ## [1] 0.0733229 h2o.rmse(best_model, xval = TRUE) ## [1] 0.270782 # get optimal parameters best_model@parameters$lambda ## [1] 0.000673668 best_model@parameters$alpha ## [1] 0.25 We can also assess the confusion matrix for our optimal model. Our overall accuracy is 0.9146 (\\(1-0.0854\\)), which is slightly less than the mean 5-fold CV accuracy produced by the glmnet model (0.9344). Similar to our glmnet results, our optimal model is doing a good job predicting “0”, “1”, and “6”, but is poorly predicting “2”, “3”, “5”, and “8”. h2o.confusionMatrix(best_model) ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 5776 1 14 9 10 28 36 6 38 5 0.0248 = 147 / 5,923 ## 1 1 6581 32 15 6 26 3 14 55 9 0.0239 = 161 / 6,742 ## 2 28 53 5449 85 64 16 57 67 121 18 0.0854 = 509 / 5,958 ## 3 18 31 119 5553 7 180 16 49 109 49 0.0943 = 578 / 6,131 ## 4 11 34 24 8 5504 7 44 12 30 168 0.0579 = 338 / 5,842 ## 5 50 32 31 146 52 4855 78 17 121 39 0.1044 = 566 / 5,421 ## 6 31 18 31 0 35 62 5710 4 25 2 0.0351 = 208 / 5,918 ## 7 11 35 58 16 46 8 4 5905 12 170 0.0575 = 360 / 6,265 ## 8 33 151 51 128 26 134 34 15 5211 68 0.1094 = 640 / 5,851 ## 9 25 27 11 76 142 30 3 156 38 5441 0.0854 = 508 / 5,949 ## Totals 5984 6963 5820 6036 5892 5346 5985 6245 5760 5969 0.0669 = 4,015 / 60,000 3.6.2.3 Feature interpretation Similar to glmnet, we can extract useful information from our coefficients to interpret influential variables in our predictors. First, we need to do a little clean up of our coefficient table. The coefficient table provides both the coefficient estimate and the standard error; however, what follows only focuses on the coefficient estimate. # clean up coefficient information vi &lt;- best_model@model$coefficients_table %&gt;% as.data.frame() %&gt;% tidyr::gather(response, coefficient, -names) %&gt;% filter( names != &quot;Intercept&quot;, !(stringr::str_detect(response, &quot;std&quot;))) %&gt;% mutate(response = stringr::str_remove(response, &quot;coefs_class_&quot;)) head(vi) ## names response coefficient ## 1 V13 0 1.637796e-04 ## 2 V14 0 2.447977e-05 ## 3 V15 0 1.202897e-04 ## 4 V16 0 2.886954e-03 ## 5 V33 0 1.270802e-03 ## 6 V34 0 3.842376e-04 Unlike glmnet, h2o forces all predictors to be either in or out across all response categories (there are options to remove some of the collinear columns; however, this will still produce a consistent number of predictors across all response categories). Consequently, we see that all 717 features are present for each response. count(vi, response) ## # A tibble: 10 x 2 ## response n ## &lt;chr&gt; &lt;int&gt; ## 1 0 717 ## 2 1 717 ## 3 2 717 ## 4 3 717 ## 5 4 717 ## 6 5 717 ## 7 6 717 ## 8 7 717 ## 9 8 717 ## 10 9 717 Similar to glment, we can use this information to identify the top 10 predictors with the largest absolute coefficients for each response category. These predictors represent those that have the largest impact to the probability of that response. Comparing to the glmnet variable importance plots earlier, features V60, V225, and V504 are the top 3 features that have the largest increase in the probability that the response will be “0” (although their ordering differs slightly). However, the features that have the largest decrease in the probability of response “0” differ from before. This may be a result of the different penalty parameter that this optimal model has compared to what we used with glmnet (\\(\\alpha=0.25\\) versus \\(\\alpha=0.5\\)). vi %&gt;% group_by(response) %&gt;% top_n(10, wt = abs(coefficient)) %&gt;% mutate(predictor = paste(response, names, sep = &quot;: &quot;)) %&gt;% ggplot(aes(coefficient, reorder(predictor, coefficient), color = coefficient &gt; 0)) + geom_point(show.legend = FALSE) + facet_wrap(~ response, scales = &quot;free_y&quot;, ncol = 5) + ylab(NULL) Figure 3.29: Top 10 most influential predictors for each multinomial response. Predictors with coefficients to the right of zero (blue) increase the probability of that response whereas predictors with coefficients to the left of zero (red) decrease the probability of that response. Remember that these linear models assume a monotonic linear relationship between the predictors and the response; meaning that for a given response category, the relationship represented by the coefficient is constant. Therefore, global and local model interpreation will be the same. See more in the machine learning interpretability chapter. 3.6.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Note how the h2o.predict function provides 11 columns - the predicted class and the probability of each class. We also produce our test set performance results with h2o.performance. If you compare the confusion matrix with the one produced by glmnet you will notice they produce very similar results. The h2o model has an overall accuracy of 0.9254 (\\(1-0.0746\\)) whereas the glmnet model achieved an accuracy of 0.9281 on the test set. And both models have about the same level of accuracy across individual response categories. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 ## 1 8 1.202141e-05 5.972906e-04 1.974938e-04 3.407596e-03 7.913304e-05 2.536697e-03 1.125671e-05 5.718130e-06 0.9905660926 2.586700e-03 ## 2 3 3.049209e-07 2.484003e-05 5.767582e-06 9.805161e-01 6.366074e-06 1.372732e-02 8.326104e-06 4.850814e-04 0.0009518292 4.274033e-03 ## 3 8 4.521685e-01 1.335448e-07 6.741594e-04 7.897412e-05 1.444127e-07 8.704872e-07 3.896986e-03 5.145982e-09 0.5431802637 3.238420e-09 ## 4 0 8.901589e-01 4.027749e-07 9.102679e-04 5.754058e-07 3.002048e-05 1.979771e-04 1.081991e-01 1.176814e-06 0.0005003709 1.131257e-06 ## 5 1 7.789492e-06 9.859295e-01 9.676005e-03 8.276910e-04 1.749899e-05 1.464323e-04 2.368448e-04 1.720208e-04 0.0029125347 7.370373e-05 ## 6 5 1.597256e-06 2.661786e-08 1.713028e-08 8.928673e-11 4.543192e-05 9.399342e-01 1.337817e-06 3.112696e-07 0.0600170128 2.307191e-08 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2OMultinomialMetrics: glm ## ## Test Set Metrics: ## ===================== ## ## MSE: (Extract with `h2o.mse`) 0.06887707 ## RMSE: (Extract with `h2o.rmse`) 0.2624444 ## Logloss: (Extract with `h2o.logloss`) 0.2682622 ## Mean Per-Class Error: 0.07570879 ## Null Deviance: (Extract with `h2o.nulldeviance`) 46020.38 ## Residual Deviance: (Extract with `h2o.residual_deviance`) 5365.243 ## R^2: (Extract with `h2o.r2`) 0.9917859 ## AIC: (Extract with `h2o.aic`) NaN ## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 963 0 0 1 0 5 5 4 2 0 0.0173 = 17 / 980 ## 1 0 1112 2 2 0 1 4 2 12 0 0.0203 = 23 / 1,135 ## 2 7 9 926 17 9 3 13 9 36 3 0.1027 = 106 / 1,032 ## 3 4 1 17 921 1 24 2 11 23 6 0.0881 = 89 / 1,010 ## 4 1 3 3 2 919 0 14 3 7 30 0.0642 = 63 / 982 ## 5 8 2 1 33 10 774 17 12 31 4 0.1323 = 118 / 892 ## 6 11 3 6 1 7 15 910 3 2 0 0.0501 = 48 / 958 ## 7 2 9 22 6 6 0 0 951 1 31 0.0749 = 77 / 1,028 ## 8 7 12 6 19 11 24 11 12 861 11 0.1160 = 113 / 974 ## 9 10 8 1 10 28 5 0 24 6 917 0.0912 = 92 / 1,009 ## Totals 1013 1159 984 1012 991 851 976 1031 981 1002 0.0746 = 746 / 10,000 ## ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.925400 ## 2 2 0.970200 ## 3 3 0.983900 ## 4 4 0.991300 ## 5 5 0.995600 ## 6 6 0.997400 ## 7 7 0.998600 ## 8 8 0.999300 ## 9 9 0.999700 ## 10 10 1.000000 # shutdown h2o h2o.removeAll() h2o.shutdown(prompt = FALSE) 3.7 Learning More This serves as an introduction to regularized regression; however, it just scrapes the surface. Regularized regression approaches have been extended to other parametric (i.e. Cox proportional hazard, poisson, support vector machines) and non-parametric (i.e. Least Angle Regression, the Bayesian Lasso, neural networks) models. The following are great resources to learn more (listed in order of complexity): Applied Predictive Modeling Practical Machine Learning with H2o Introduction to Statistical Learning The Elements of Statistical Learning Statistical Learning with Sparsity References "],
["random-forest.html", "Chapter 4 Random Forest 4.1 Prerequisites 4.2 Advantages &amp; Disadvantages 4.3 The Idea 4.4 Implementation: Regression 4.5 Implementation: Binary Classification 4.6 Implementation: Multinomial Classification 4.7 Learning More", " Chapter 4 Random Forest Random forests are a modification of decision trees and bagging that builds a large collection of de-correlated trees to reduce overfitting (aka variance). They have become a very popular “out-of-the-box” learning algorithm that enjoys good predictive performance and easy hyperparameter tuning. Many modern implementations of random forests algorithms exist; however, Leo Breiman’s algorithm (Breiman 2001) has largely become the authoritative procedure. This chapter will cover the fundamentals of random forests. 4.1 Prerequisites Any tutorial on random forests (RF) should also include a review of decision trees, as these are models that are ensembled together to create the random forest model – or put another way, the “trees that comprise the forest.” Much of the complexity and detail of the random forest algorithm occurs within the individual decision trees and therefore it’s important to understand decision trees to understand the RF algorithm as a whole. Therefore, before proceeding, it is recommended that you read through http://uc-r.github.io/regression_trees prior to continuing. This chapter leverages the following packages. Some of these packages play a supporting role; however, the emphasis is on how to implement random forests with the ranger (Wright and Ziegler 2017) and h2o packages. library(rsample) # data splitting library(ranger) # a fast c++ implementation of the random forest algorithm library(h2o) # a java-based platform library(vip) # visualize feature importance library(pdp) # visualize feature effects library(ggplot2) # supports visualization library(dplyr) # basic data transformation 4.2 Advantages &amp; Disadvantages Advantages: Typically have very good performance. Remarkably good “out-of-the box” - very little tuning required. Built-in validation set - don’t need to sacrifice data for extra validation. Does not overfit. No data pre-processing required - often works great with categorical and numerical values as is. Robust to outliers. Handles missing data - imputation not required. Provide automatic feature selection. Disadvantages: Can become slow on large data sets. Although accurate, often cannot compete with the accuracy of advanced boosting algorithms. Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.). 4.3 The Idea Random forests are built on the same fundamental principles as decision trees and bagging (check out this tutorial if you need a refresher on these techniques). Bagging trees introduces a random component in to the tree building process that reduces the variance of a single tree’s prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. For example, if we create six decision trees with different bootstrapped samples of the Boston housing data (Harrison Jr and Rubinfeld 1978), we see that the top of the trees all have a very similar structure. Although there are 15 predictor variables to split on, all six trees have both lstat and rm variables driving the first few splits. Figure 4.1: Six decision trees based on different bootstrap samples. This characteristic is known as tree correlation and prevents bagging from optimally reducing variance of the predictive values. In order to reduce variance further, we need to minimize the amount of correlation between the trees. This can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways: Bootstrap: similar to bagging, each tree is grown to a bootstrap resampled data set, which makes them different and somewhat decorrelates them. Split-variable randomization: each time a split is to be performed, the search for the split variable is limited to a random subset of m of the p variables. Typical default values are \\(m = \\frac{p}{3}\\) (regression trees) and \\(m = \\sqrt{p}\\) (classification trees) but this should be considered a tuning parameter. When \\(m = p\\), the randomization amounts to using only step 1 and is the same as bagging. The basic algorithm for a regression or classification random forest can be generalized to the following: 1. Given training data set 2. Select number of trees to build (ntrees) 3. for i = 1 to ntrees do 4. | Generate a bootstrap sample of the original data 5. | Grow a regression or classification tree to the bootstrapped data 6. | for each split do 7. | | Select m variables at random from all p variables 8. | | Pick the best variable/split-point among the m 9. | | Split the node into two child nodes 10. | end 11. | Use typical tree model stopping criteria to determine when a tree is complete (but do not prune) 12. end Since the algorithm randomly selects a bootstrap sample to train on and predictors to use at each split, tree correlation will be lessened beyond bagged trees. 4.3.1 OOB error vs. test set error Similar to bagging, a natural benefit of the bootstrap resampling process is that random forests have an out-of-bag (OOB) sample that provides an efficient and reasonable approximation of the test error. This provides a built-in validation set without any extra work on your part, and you do not need to sacrifice any of your training data to use for validation. This makes identifying the number of trees required to stablize the error rate during tuning more efficient; however, as illustrated below some difference between the OOB error and test error are expected. Figure 2.1: Random forest out-of-bag error versus validation error. Furthermore, many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not. If you are comparing multiple models to one-another, you’d want to score each on the same validation set to compare performance. Also, although technically it is possible to compute certain metrics such as root mean squared logarithmic error (RMSLE) on the OOB sample, it is not built in to all packages. So if you are looking to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation. 4.3.2 Tuning Random forests are fairly easy to tune since there are only a handful of tuning parameters. Typically, the primary concern when starting out is tuning the number of candidate variables to select from at each split. However, there are a few additional hyperparameters that we should be aware of. Although the argument names may differ across packages, these hyperparameters should be present: Number of trees_: We want enough trees to stabalize the error but using too many trees is unncessarily inefficient, especially when using large data sets. Number of variables to randomly sample as candidates at each split (often referred to as mtry): When mtry \\(=p\\) the model equates to bagging. When mtry \\(=1\\) the split variable is completely random, so all variables get a chance but can lead to overly biased results. A common suggestion is to start with 5 values evenly spaced across the range from 2 to p. Sample size: the number of samples to train on. The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample. Lower sample sizes can reduce the training time but may introduce more bias than necessary. Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range. Node size: minimum number of samples within the terminal nodes. Controls the complexity of the trees. Smaller node size allows for deeper, more complex trees and a larger node size results in shallower trees. This is another bias-variance tradeoff where deeper trees introduce more variance (risk of overfitting) and shallower trees introduce more bias (risk of not fully capturing unique patters and relatonships in the data). Number of terminal nodes: Another way to control the complexity of the trees. More nodes equates to deeper, more complex trees and less nodes result in shallower trees. 4.3.3 Package implementation There are over 20 random forest packages in R.3 The oldest and most well known implementation of the Random Forest algorithm in R is the randomForest package. randomForest is not a recommended package because as your data sets grow in size randomForest does not scale well (although you can parallelize with foreach). Instead, we recommend you use the ranger and h2o packages. Since randomForest does not scale well to many of the data set sizes that organizations analyze, we will demonstrate how to implement the random forest algorithm with two fast, efficient, and highly recommended packages: ranger: a C++ implementation of Brieman’s random forest algorithm and particularly well suited for high dimensional data. The original paper describing ranger and providing benchmarking to other packages can be found here. Features include4: Classification, regression, probability estimation and survival forests are supported. Multi-threaded capabilities for optimal speed. Excellent speed and support for high-dimensional or wide data. Not as fast for “tall &amp; skinny” data (many rows, few columns). GPL-3 licensed. h2o: The h2o R package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with a fairly comprehensive online resource that includes methodology and code documentation along with tutorials. Features include: Automated feature pre-processing (one-hot encode &amp; standardization). Built-in cross validation. Built-in grid search capabilities. Provides automatic early stopping for faster grid searches. Supports the following distributions: “guassian”, “binomial”, “multinomial”, “poisson”, “gamma”, “tweedie”. Uses histogram approximations of continuous variables for speedup on “long data” (many rows). Distributed and parallelized computation on either a single node or a multi-node cluster. Model export in plain Java code for deployment in production environments. 4.4 Implementation: Regression To illustrate various random forest concepts for a regression problem we will use the Ames, IA housing data, where our intent is to predict Sale_Price. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Tree-based algorithms typically perform very well without preprocessing the data (i.e. one-hot encoding, normalizing, standardizing). 4.4.1 ranger 4.4.1.1 Basic implementation ranger::ranger uses the formula method for specifying our model. Below we apply the default ranger model specifying to model Sale_Price as a function of all features in our data set. The key arguments to the ranger call are: formula: formula specification data: training data num.trees: number of trees in the forest mtry: randomly selected predictor variables at each split. Default is \\(\\texttt{floor}(\\sqrt{\\texttt{number of features}})\\); however, for regression problems the preferred mtry to start with is \\(\\texttt{floor}(\\frac{\\texttt{number of features}}{3}) = \\texttt{floor}(\\frac{92}{3}) = 30\\) respect.unordered.factors: specifies how to treat unordered factor variables. We recommend setting this to “order” for regression. See Friedman, Hastie, and Tibshirani (2001), chapter 9.2.4 for details. seed: because this is a random algorithm, you will set the seed to get reproducible results By default, ranger will provide the computation status and estimated remaining time; however, to reduce output in this tutorial this is turned off with verbose = FALSE. As the model results show, averaging across all 500 trees provides an OOB \\(MSE = 615848303\\) (\\(RMSE \\approx 24816\\)). # number of features features &lt;- setdiff(names(ames_train), &quot;Sale_Price&quot;) # perform basic random forest model m1_ranger &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = floor(length(features) / 3), respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) # look at results m1_ranger ## Ranger result ## ## Call: ## ranger(formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = floor(length(features)/3), respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 2054 ## Number of independent variables: 80 ## Mtry: 26 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 615848303 ## R squared (OOB): 0.9013317 # compute RMSE (RMSE = square root of MSE) sqrt(m1_ranger$prediction.error) ## [1] 24816.29 One of the benefits of tree-based methods is they do not require preprocessing steps such as normalization and standardization of the response and/or predictor variables. However, because these methods do not require these steps does not mean you should not assess their impact. Sometimes normalizing and standardizing the data can improve performance. In the following code we compare a basic random forest model on unprocessed data to one on processed data (normalized, standardized, and zero variance features removded). # create validation set set.seed(123) split2 &lt;- initial_split(ames_train, prop = .8, strata = &quot;Sale_Price&quot;) train_tran &lt;- training(split2) validation &lt;- testing(split2) #-------------------------Unprocessed variables-------------------------# # number of features in unprocessed data m &lt;- length(setdiff(names(train_tran), &quot;Sale_Price&quot;)) # perform basic random forest model on unprocessed data m1_ranger_unprocessed &lt;- ranger( formula = Sale_Price ~ ., data = train_tran, num.trees = 500, mtry = m, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) #--------------------------Processed variables--------------------------# # preprocess features feature_process &lt;- caret::preProcess( train_tran[, features], method = c(&quot;YeoJohnson&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;zv&quot;) ) train_tran &lt;- predict(feature_process, train_tran) # preprocess response train_tran$Sale_Price &lt;- log(train_tran$Sale_Price) # number of features in processed data m &lt;- length(setdiff(names(train_tran), &quot;Sale_Price&quot;)) # perform basic random forest model on processed data m1_ranger_processed &lt;- ranger( formula = Sale_Price ~ ., data = train_tran, num.trees = 500, mtry = m, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) We can now apply each model to the validation set. For the second (preprocessed) model, we re-transform our predicted values back to the normal units and we compute the RMSE for both. Now we see that our original model on unpreprocessed data is performing just as well as, if not better than, the second model on the processed data. # apply unpreprocessed model m1_pred &lt;- predict(m1_ranger_unprocessed, validation) caret::RMSE(m1_pred$predictions, validation$Sale_Price) ## [1] 22302.02 # preprocess features valid_tran &lt;- predict(feature_process, validation) # apply preprocessed model m1_tran_pred &lt;- predict(m1_ranger_processed, valid_tran) m1_processed_pred &lt;- expm1(m1_tran_pred$predictions) caret::RMSE(m1_processed_pred, validation$Sale_Price) ## [1] 24281.47 4.4.1.2 Tuning With the ranger function we can tune various hyperparameters mentioned in the general tuning section. For example, the following model adjusts: num.trees: increase number of trees to 750 mtry: reduce number of predictor variables to randomly select at each split to 20 min.node.size: reduce minimum node size to 3 (default is 5 for regression) sample.fraction: increase training set to 70% m2_ranger &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 750, mtry = 20, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = .70 ) # RMSE sqrt(m2_ranger$prediction.error) ## [1] 25298.61 # model results m2_ranger ## Ranger result ## ## Call: ## ranger(formula = Sale_Price ~ ., data = ames_train, num.trees = 750, mtry = 20, respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = 0.7) ## ## Type: Regression ## Number of trees: 750 ## Sample size: 2054 ## Number of independent variables: 80 ## Mtry: 20 ## Target node size: 3 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 640019634 ## R squared (OOB): 0.8974591 We can continue to adjust these settings individually to identify the optimal combination; however, this becomes tedious when you want to explore a larger grid search. To perform a larger grid search across several hyperparameters we’ll need to create a grid and loop through each hyperparameter combination and evaluate the model. First we want to construct our grid of hyperparameters. We’re going to search across 80 different models with varying number of trees, mtry, minimum node size, and sample size. # hyperparameter grid search hyper_grid &lt;- expand.grid( num.trees = seq(250, 500, 750), mtry = seq(20, 40, by = 5), node_size = seq(1, 10, by = 3), sample_size = c(.55, .632, .70, .80), OOB_RMSE = 0 ) # total number of combinations nrow(hyper_grid) ## [1] 80 # hyperparameter grid head(hyper_grid) ## num.trees mtry node_size sample_size OOB_RMSE ## 1 250 20 1 0.55 0 ## 2 250 25 1 0.55 0 ## 3 250 30 1 0.55 0 ## 4 250 35 1 0.55 0 ## 5 250 40 1 0.55 0 ## 6 250 20 4 0.55 0 We can now loop through each hyperparameter combination. Note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. This full grid search ran for about 2.5 minutes before completing. Larger grid searches like these can become time consuming as your data set increases in dimensions. The h2o package provides alternative approaches to search through larger grid spaces. Our OOB RMSE ranges between ~25021-26089. Our top 10 performing models all have RMSE values in the low 25000 range and the results show that we can use a smaller number of trees than the default and models with slighly larger sample size appear to perform best. At first glance, no definitive evidence suggests that altering mtry or node_size have a sizable impact. for(i in 1:nrow(hyper_grid)) { # train model model &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, respect.unordered.factors = &#39;order&#39;, seed = 123, verbose = FALSE, mtry = hyper_grid$mtry[i], min.node.size = hyper_grid$node_size[i], sample.fraction = hyper_grid$sample_size[i] ) # add OOB error to grid hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error) } hyper_grid %&gt;% dplyr::arrange(OOB_RMSE) %&gt;% head(10) ## num.trees mtry node_size sample_size OOB_RMSE ## 1 250 35 1 0.800 25020.85 ## 2 250 35 4 0.800 25044.19 ## 3 250 25 4 0.700 25093.52 ## 4 250 25 1 0.700 25117.58 ## 5 250 20 4 0.800 25122.44 ## 6 250 40 4 0.800 25133.63 ## 7 250 40 1 0.800 25134.92 ## 8 250 40 7 0.800 25140.17 ## 9 250 30 1 0.800 25152.47 ## 10 250 40 4 0.632 25159.07 The above grid search helps to focus where we can further refine our model tuning. As a next step, we would perform additional grid searches that focus in on a refined grid space for sample size and also try a few additional settings of mtry and min.node.size to rule out their effects on performance. However, for brevity we will leave this as an exercise for the reader. 4.4.1.3 Feature interpretation Whereas regularized regression assumes a monotonic linear relationship between features and the response, random forests make no such assumption. Moreover, random forests do not have coefficients to base these relationships on. Consequently, with random forests we can understand the relationship between the features and the response using variable importance plots and partial dependence plots. Additional model interpretability approaches will be discussed in the Model Interpretability chapter. 4.4.1.3.1 Feature importance Whereas regularized models used the standardized coefficients to signal importance, random forests have, historically, applied two different approaches to measure variable importance. Impurity: At each split in each tree, compute the improvement in the split-criterion (MSE for regression). Then average the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important. Permutation: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly “shaking up” of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important. To compute these variable importance measures with ranger, you must include the importance argument. Once you’ve identified the optimal parameter values from the grid search, you will want to re-run your model with these hyperparameter values. # re-run model with impurity-based variable importance m3_ranger_impurity &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 250, mtry = 35, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;impurity&#39; ) # re-run model with permutation-based variable importance m3_ranger_permutation &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 250, mtry = 35, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;permutation&#39; ) For both options, you can directly access the variable importance values with model_name$variable.importance. However, here we will plot the variable importance using the vip package. Typically, you will not see the same variable importance order between the two options; however, you will often see similar variables at the top of the plots. Consquently, in this example, we can comfortably state that there appears to be enough evidence to suggest that three variables stand out as most influential: Overall_Qual Gr_Liv_Area Neighborhood Looking at the next ~10 variables in both plots, you will also see some commonality in influential variables (i.e. Garage_Cars, Bsmt_Qual, Year_Built). p1 &lt;- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle(&quot;Impurity-based variable importance&quot;) p2 &lt;- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle(&quot;Permutation-based variable importance&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 4.2: Top 25 most important variables based on impurity (left) and permutation (right). 4.4.1.3.2 Feature effects After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. Unlike linear approaches, random forests do not assume a linear relationship. Consequently, we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves. PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. For example, consider the Gr_Liv_Area variable. In the h20 regularized regression section (3.4.2.3), we saw that the linear model assumed a continously increasing relationship between Gr_Liv_Area and Sale_Price. However, the PDP plot below displays a non-linear relationship where Sale_Price appears to not be influenced by Gr_Liv_Area values below 750 sqft or above 3500 sqft. # partial dependence of Sale_Price on Gr_Liv_Area m3_ranger_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50) %&gt;% autoplot(rug = TRUE, train = ames_train) Figure 4.3: The mean predicted sale price as the above ground living area increases. Additionally, if we assess the relationship between the Overall_Qual predictor and Sale_Price, we see a continual increase as the overall quality increases. This is more intutive than the results we saw in the regularized regression section (3.4.2.3). This may be an indication that the coefficients were biased in the regularized regression models. # partial dependence of Sale_Price on Overall_Qual m3_ranger_impurity %&gt;% partial(pred.var = &quot;Overall_Qual&quot;, train = as.data.frame(ames_train)) %&gt;% autoplot() Figure 4.4: The mean predicted sale price for each level of the overall quality variable. Individual conditional expectation (ICE) curves (Goldstein et al. 2015) are an extension of PDP plots but, rather than plot the average marginal effect on the response variable, we plot the change in the predicted response variable for each observation as we vary each predictor variable. Below shows the regular ICE curve plot (left) and the centered ICE curves (right). When the curves have a wide range of intercepts and are consequently “stacked” on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results. The plots below show that marginal changes in Gr_Liv_Area have a fairly homogenous effect on our response variable. As Gr_Liv_Area increases, the vast majority of observations show a similar increasing effect on the predicted Sale_Price value. The primary differences is in the magnitude of the increasing effect. However, in the centered ICE plot you see evidence of a few observations that display a different pattern. These observations would be worth looking at more closely. # ice curves of Sale_Price on Gr_Liv_Area ice1 &lt;- m3_ranger_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50, ice = TRUE) %&gt;% autoplot(rug = TRUE, train = ames_train, alpha = 0.2) + ggtitle(&quot;Non-centered ICE plot&quot;) ice2 &lt;- m3_ranger_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50, ice = TRUE) %&gt;% autoplot(rug = TRUE, train = ames_train, alpha = 0.2, center = TRUE) + ggtitle(&quot;Centered ICE plot&quot;) gridExtra::grid.arrange(ice1, ice2, nrow = 1) Figure 4.5: Non-centered (left) and centered (right) individual conditional expectation curve plots illustrate how changes in above ground square footage influences predicted sale price for all observations. Both PDPs and ICE curves should be assessed for the most influential variables as they help to explain the underlying patterns in the data that the random forest model is picking up. Check out the Model Interpretation chapter to learn more about visualizing your machine learning models. 4.4.1.4 Predicting Once you’ve found your optimal model, predicting new observations with the ranger model follows the same procedure as most R models. We can apply the predict function and supply it the optimal model and the new data set we’d like to predict on. The result is a list object that includes several attributes about the model used to predict (i.e. number of trees &amp; predictor variables, sample size, tree type). The predicted values we are most concerned with are contained in the predict_object$predictions list item. # predict on test set predict_ranger &lt;- predict(m3_ranger_impurity, ames_test) # predict object str(predict_ranger) ## List of 5 ## $ predictions : num [1:876] 130020 157044 224076 251814 376064 ... ## $ num.trees : num 250 ## $ num.independent.variables: num 80 ## $ num.samples : int 876 ## $ treetype : chr &quot;Regression&quot; ## - attr(*, &quot;class&quot;)= chr &quot;ranger.prediction&quot; # predicted values head(predict_ranger$predictions) ## [1] 130020.1 157044.1 224076.1 251813.8 376064.3 365273.1 We can use these predicted values to assess the final generalization error, which is slightly lower than our models OOB sample RMSE: # final model OOB RMSE sqrt(m3_ranger_impurity$prediction.error) ## [1] 25197.67 # generalization error caret::RMSE(predict_ranger$predictions, ames_test$Sale_Price) ## [1] 25148.62 4.4.2 h20 To perform a random forest model with h2o, we first need to initiate our h2o session. h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpbIIvCw/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpbIIvCw/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 403 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_ply740 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we need to convert our training and test data to h2o objects. # convert training data to h2o object train_h2o &lt;- as.h2o(ames_train) # convert test data to h2o object test_h2o &lt;- as.h2o(ames_test) # set the response column to Sale_Price response &lt;- &quot;Sale_Price&quot; # set the predictor names predictors &lt;- setdiff(colnames(ames_train), response) 4.4.2.1 Basic implementation To perform a random forest model with h2o we use h2o::h2o.randomForest. Keep in mind that h2o uses the name method for specifying our model. Below we apply the default h2o.randomForest model specifying to model Sale_Price as a function of all features in our data set. h2o.randomForest has many arguments that can be adjusted; however, often the default settings perform very well. To start with, a few key arguments in h2o.randomForest to understand include: x: names of the predictor variables y: name of the response variable training_frame: training data ntrees: number of trees in the forest (default is 50) mtries: randomly selected predictor variables at each split. Default is \\(\\texttt{floor}(\\frac{\\texttt{number of features}}{3}) = \\texttt{floor}(\\frac{92}{3}) = 30\\) for regression. categorical_encoding: Decides the encoding scheme for categorical variables. Typically choose one of “Enum” or “SortByResponse” (categorical_encoding = 'SortByResponse' performs similar procedure as respect.unordered.factors = 'order'). For these data we do not see any difference in performance between the two. seed: because this is a random algorithm, you will set the seed to get reproducible results h2o can provide the computation status; however, this feature is turned off by default but can be turned on with verbose = FALSE. As the model results show, averaging across all 250 trees provides an OOB \\(RMSE \\approx 24541\\)). These are pretty similar to what we found with the default ranger model. # perform basic random forest model m1_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = train_h2o, ntrees = 250, seed = 123 ) # look at results ## m1_h2o ## Model Details: ## ============== ## ## H2ORegressionModel: drf ## Model ID: DRF_model_R_1532981766487_1 ## Model Summary: ## ## ## H2ORegressionMetrics: drf ## ** Reported on training data. ** ## ** Metrics reported on Out-Of-Bag training samples ** ## ## MSE: 602273377 ## RMSE: 24541.26 ## MAE: 15060.85 ## RMSLE: 0.1406867 ## Mean Residual Deviance : 602273377 One of the benefits of h2o is it allows us to include stopping_ arguments, which will stop the modeling automatically once the RMSE metric on the OOB samples stops improving by a certain value (say 1%) for a specified number of consecutive trees. This helps us to identify the number of trees required to stabilize our error metric. Below we see that 49 trees are sufficient. # perform basic random forest model m2_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = train_h2o, ntrees = 500, seed = 123, stopping_metric = &quot;RMSE&quot;, # stopping mechanism stopping_rounds = 10, # number of rounds stopping_tolerance = 0.005 # looking for 0.5% improvement ) # look at results m2_h2o ## Model Details: ## ============== ## ## H2ORegressionModel: drf ## Model ID: DRF_model_R_1532981766487_2 ## Model Summary: ## number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves ## 1 49 49 758326 20 20 20.00000 1175 1273 1226.34690 ## ## ## H2ORegressionMetrics: drf ## ** Reported on training data. ** ## ** Metrics reported on Out-Of-Bag training samples ** ## ## MSE: 670676969 ## RMSE: 25897.43 ## MAE: 15741.89 ## RMSLE: 0.1434247 ## Mean Residual Deviance : 670676969 4.4.2.2 Tuning h2o.randomForest provides many tuning options. The more common tuning options can be categorized into three purposes: Controlling how big your random forest will be: ntrees: how many trees in the forest max_depth: maximum depth to which each tree will be built (default is 20) Controlling the random components of the model: mtries: number of predictor variables to randomly select at each split sample_rate: Row sample rate per tree (default is 63.2%) Controlling how the splitting is done min_rows: minimum number of observations for a leaf in order to split (default is 1) There are additional tuning parameters in each of the above categories but their defaults are typically sufficient. You can read about the options here. We can tune these hyperparameters individually; however, a major benefit of h2o is it provides two different approaches for hyperparameter grid searches: Full cartesian grid search: examine every combination of hyperparameter settings that we specify, Random grid search: jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). 4.4.2.2.1 Full cartesian grid search First, we can try a comprehensive (full cartesian) grid search, which means we will examine every combination of hyperparameter settings that we specify in hyper_grid.h2o. Here, we search across 320 hyperparameter combinations. You can include ntrees as a hyperparameter; however, its more efficient to set ntrees to a high value and then use early stopping to stop each model once improvement is no longer obtained. This comprehensive grid search took 56 minutes. The results show a minimum RMSE of $23,792 (slightly less than our optimal ranger model), when max_depth = 25, min_rows = 1, mtries = 25, and sample_rate = 80%. Looking at the top 5 models it appears that the primary driving parameters for minimizing MSE are min_rows (smaller is better), mtries (smaller is better), and sample_rate (larger is better). # hyperparameter grid hyper_grid.h2o &lt;- list( mtries = seq(20, 40, by = 5), max_depth = seq(15, 30, by = 5), min_rows = seq(1, 10, by = 3), sample_rate = c(.55, .632, .70, .80) ) # build grid search grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_full_grid&quot;, x = predictors, y = response, training_frame = train_h2o, hyper_params = hyper_grid.h2o, ntrees = 500, seed = 123, stopping_metric = &quot;RMSE&quot;, stopping_rounds = 10, stopping_tolerance = 0.005, search_criteria = list(strategy = &quot;Cartesian&quot;) ) # collect the results and sort by our model performance metric of choice full_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_full_grid&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE ) print(full_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_full_grid ## Used hyper parameters: ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 291 ## Number of failed models: 29 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## max_depth min_rows mtries sample_rate model_ids mse ## 1 25 1.0 25 0.8 rf_full_grid_model_258 5.660696269742714E8 ## 2 30 1.0 25 0.8 rf_full_grid_model_259 5.66075155855145E8 ## 3 20 1.0 25 0.8 rf_full_grid_model_257 5.66146259929908E8 ## 4 30 1.0 20 0.8 rf_full_grid_model_243 5.665494562778755E8 ## 5 25 1.0 20 0.8 rf_full_grid_model_242 5.665555943619757E8 ## ## --- ## max_depth min_rows mtries sample_rate model_ids mse ## 286 25 10.0 35 0.55 rf_full_grid_model_62 8.195309220382509E8 ## 287 20 10.0 35 0.55 rf_full_grid_model_61 8.195309220382509E8 ## 288 30 10.0 30 0.632 rf_full_grid_model_127 8.266536241339123E8 ## 289 25 10.0 30 0.632 rf_full_grid_model_126 8.266536241339123E8 ## 290 20 10.0 30 0.632 rf_full_grid_model_125 8.266536241339123E8 ## 291 15 10.0 30 0.632 rf_full_grid_model_124 8.266536241339123E8 4.4.2.2.2 Random discrete grid search Because of the combinatorial explosion, each additional hyperparameter that gets added to our grid search has a huge effect on the time to complete. Consequently, h2o provides an additional grid search path called “RandomDiscrete”, which will jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). Although using a random discrete search path will likely not find the optimal model, it typically does a good job of finding a very good model. This comprehensive grid search took 30 minutes. For example, the following code searches the same grid search performed above. We create a random grid search that will stop if none of the last 10 models have managed to have a 0.1% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 1800 seconds (30 minutes). Our grid search assessed 191 models before stopping due to time. The best model (max_depth = 25, min_rows = 1, mtries = 40, and sample_rate = 0.8) achived an RMSE \\(\\approx \\$23,751\\). So although our random search on assessed about half the number of models as the full grid search, the more efficient random search found a near-optimal model relatively speaking. # random grid search criteria search_criteria &lt;- list( strategy = &quot;RandomDiscrete&quot;, stopping_metric = &quot;mse&quot;, stopping_tolerance = 0.001, stopping_rounds = 10, max_runtime_secs = 60*30 ) # build grid search random_grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_random_grid&quot;, x = predictors, y = response, training_frame = train_h2o, hyper_params = hyper_grid.h2o, ntrees = 500, seed = 123, stopping_metric = &quot;RMSE&quot;, stopping_rounds = 10, stopping_tolerance = 0.005, search_criteria = search_criteria ) # collect the results and sort by our model performance metric of choice random_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_random_grid&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE ) print(random_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_random_grid ## Used hyper parameters: ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 191 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## max_depth min_rows mtries sample_rate model_ids mse ## 1 25 1.0 40 0.8 rf_random_grid_model_131 5.624310085353142E8 ## 2 15 1.0 40 0.8 rf_random_grid_model_180 5.63276905670922E8 ## 3 25 1.0 25 0.8 rf_random_grid_model_174 5.660696269742714E8 ## 4 20 1.0 25 0.8 rf_random_grid_model_17 5.66146259929908E8 ## 5 20 1.0 20 0.8 rf_random_grid_model_144 5.66631854910229E8 ## ## --- ## max_depth min_rows mtries sample_rate model_ids mse ## 186 20 10.0 40 0.55 rf_random_grid_model_52 8.163555317753098E8 ## 187 30 10.0 30 0.8 rf_random_grid_model_50 8.182002205674793E8 ## 188 20 10.0 35 0.55 rf_random_grid_model_23 8.195309220382509E8 ## 189 30 10.0 35 0.55 rf_random_grid_model_97 8.195309220382509E8 ## 190 20 10.0 30 0.632 rf_random_grid_model_44 8.266536241339123E8 ## 191 30 7.0 30 0.8 rf_random_grid_model_190 1.3296523023487797E9 Once we’ve identifed the best model we can extract it with: # Grab the model_id for the top model, chosen by validation error best_model_id &lt;- random_grid_perf@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) 4.4.2.3 Feature interpretation 4.4.2.3.1 Feature importance Once you’ve identified and selected the optimally tuned model, you can visualize variable importance with h2o.varimp_plot. h2o.varimp_plot computes variable importance “by calculating the relative influence of each variable: whether that variable was selected during splitting in the tree building process and how much the squared error (over all trees) improved as a result.”5 This is equivalent to the impurity approach used by ranger. The most important variables are relatively similar to those found with the ranger model. h2o.varimp_plot(best_model, num_of_features = 25) Figure 4.6: Variable importance plot provided by the h2o package. If you prefer the plotting provided by the vip package, you can also use vip::vip on any h2o model as well. vip(best_model, num_features = 25, bar = FALSE) Figure 4.7: Variable importance plot provided by the vip package. 4.4.2.3.2 Feature effects As with ranger, we can also assess PDP plots. h2o provides the h2o.partialPlot function to plot PDPs. Although it does not allow you to plot individual ICE curves, it does plot the standard error of the mean response across all observations along with automatically showing you the values of the predictor variable (by default it selects 20 values but can be adjusted with nbins), mean response, and standard error of the response. The partial dependence plot for Gr_Liv_Area follows a similar non-linear trend as we saw with the ranger model. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Gr_Liv_Area&quot;) Figure 4.8: h2o’s partial dependence plot of the Gr_Liv_Area predictor variable based on the optimal h2o model. ## PartialDependence: Partial Dependence Plot of model rf_random_grid_model_131 on column &#39;Gr_Liv_Area&#39; ## Gr_Liv_Area mean_response stddev_response ## 1 334.000000 161930.609931 63443.765820 ## 2 613.368421 162062.529779 63315.589233 ## 3 892.736842 162452.265339 63060.091227 ## 4 1172.105263 167315.601270 61620.417433 ## 5 1451.473684 176012.418557 59599.978531 ## 6 1730.842105 185785.468561 64362.546404 ## 7 2010.210526 195264.075938 71031.464662 ## 8 2289.578947 201998.046236 73740.380295 ## 9 2568.947368 206241.151641 74596.263303 ## 10 2848.315789 209652.185877 75251.557070 ## 11 3127.684211 210836.830736 74650.327138 ## 12 3407.052632 211658.584116 75789.811261 ## 13 3686.421053 211833.145557 76111.081331 ## 14 3965.789474 211755.931914 75912.587977 ## 15 4245.157895 211461.025415 75172.912270 ## 16 4524.526316 211422.033670 75065.495828 ## 17 4803.894737 211373.627191 74921.530746 ## 18 5083.263158 211373.486977 74921.244177 ## 19 5362.631579 211369.474903 74913.694244 ## 20 5642.000000 211369.474903 74913.694244 If you prefer getting actual ICE curves, we can use the pdp package. However, since pdp does not have an explicit method for h2o objects we need to create a prediction function and use the pred.fun argument: # build custom prediction function pfun &lt;- function(object, newdata) { as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]] } # compute ICE curves prod.ice &lt;- partial( best_model, pred.var = &quot;Gr_Liv_Area&quot;, train = ames_train, pred.fun = pfun, grid.resolution = 20 ) p1 &lt;- autoplot(prod.ice, alpha = 0.2) + ggtitle(&quot;Non-centered ICE curves&quot;) p2 &lt;- autoplot(prod.ice, alpha = 0.2, center = TRUE) + ggtitle(&quot;Centered ICE curves&quot;) gridExtra::grid.arrange(p1, p2, ncol = 2) Figure 4.9: pdp’s ICE curves of the Gr_Liv_Area predictor variable based on the optimal h2o model. 4.4.2.4 Predicting Finally, if you are satisfied with your final model we can predict values for an unseen data set a couple different ways. We can also quickly assess the model’s performance on our test set with h2o.performance. We see a similar generalizable error as we saw with the ranger model. # predict new values with base R predict() predict(best_model, test_h2o) ## predict ## 1 129214.6 ## 2 155571.0 ## 3 224196.0 ## 4 250114.6 ## 5 359731.0 ## 6 362031.2 ## ## [876 rows x 1 column] # predict new values with h2o.predict() h2o.predict(best_model, newdata = test_h2o) ## predict ## 1 129214.6 ## 2 155571.0 ## 3 224196.0 ## 4 250114.6 ## 5 359731.0 ## 6 362031.2 ## ## [876 rows x 1 column] # assess performance on test data h2o.performance(best_model, newdata = test_h2o) ## H2ORegressionMetrics: drf ## ## MSE: 661358432 ## RMSE: 25716.89 ## MAE: 15628.89 ## RMSLE: 0.1249999 ## Mean Residual Deviance : 661358432 # shut down h2o h2o.shutdown(prompt = FALSE) ## [1] TRUE 4.5 Implementation: Binary Classification To illustrate random forests concepts for a binary classification problem we will continue with the employee attrition data. attrition &lt;- rsample::attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) %&gt;% mutate(Attrition = relevel(Attrition, ref = &quot;Yes&quot;)) # Create training and testing sets set.seed(123) split &lt;- initial_split(attrition, prop = .8, strata = &quot;Attrition&quot;) attrit_train &lt;- training(split) attrit_test &lt;- testing(split) Tree-based algorithms typically perform very well without preprocessing the data (i.e. one-hot encoding, normalizing, standardizing). 4.5.1 ranger 4.5.1.1 Basic implementation We apply ranger::ranger just as we did in the regression setting. However, note that the default mtry is \\(\\texttt{floor}(\\sqrt{\\texttt{number of features}})\\), which is a good starting point for classification problems (we changed it to \\(mtry = \\texttt{floor}(\\frac{\\texttt{number of features}}{3})\\) in the regression setting). As long as your response variable is encoded as a character or factor, ranger will automatically perform a classification random forest model. As the model results show, majority voting across all 500 trees provides an OOB error rate of 13.59%. # perform basic random forest model m1_ranger &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 500, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) # look at results m1_ranger ## Ranger result ## ## Call: ## ranger(formula = Attrition ~ ., data = attrit_train, num.trees = 500, respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123) ## ## Type: Classification ## Number of trees: 500 ## Sample size: 1177 ## Number of independent variables: 30 ## Mtry: 5 ## Target node size: 1 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error: 13.59 % # look at confusion matrix m1_ranger$confusion.matrix ## predicted ## true Yes No ## Yes 36 154 ## No 6 981 The default ranger classification model does not provide probability estimates. If you want to predict the probabilities then use probability = TRUE. When using this option, the OOB prediction error changes from misclassification rate to MSE. One of the benefits of tree-based methods is they do not require preprocessing steps such as normalization and standardization of the response and/or predictor variables. However, because these methods do not require these steps does not mean you should not assess their impact. Sometimes normalizing and standardizing the data can improve performance. In the following code we compare a basic random forest probability model with unprocessed features to one with processed features (normalized, standardized, and zero variance features removed). # create validation set set.seed(123) split2 &lt;- initial_split(attrit_train, prop = .8, strata = &quot;Attrition&quot;) train_tran &lt;- training(split2) validation &lt;- testing(split2) #-------------------------Unprocessed variables-------------------------# # perform basic random forest model on unprocessed data m1_ranger_unprocessed &lt;- ranger( formula = Attrition ~ ., data = train_tran, num.trees = 500, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, probability = TRUE ) #--------------------------Processed variables--------------------------# features &lt;- setdiff(names(attrit_train), &quot;Attrition&quot;) # preprocess features feature_process &lt;- caret::preProcess( train_tran[, features], method = c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;zv&quot;) ) train_tran &lt;- predict(feature_process, train_tran) # perform basic random forest model on processed data m1_ranger_processed &lt;- ranger( formula = Attrition ~ ., data = train_tran, num.trees = 500, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, probability = TRUE ) We can now apply each model to the validation set and we see that the feature processing has no impact on our area under the curve. # apply unpreprocessed model m1_pred &lt;- predict(m1_ranger_unprocessed, validation) roc &lt;- pROC::roc(validation$Attrition, m1_pred$predictions[, 1]) pROC::auc(roc) ## Area under the curve: 0.8586 # preprocess features valid_tran &lt;- predict(feature_process, validation) # apply preprocessed model m1_tran_pred &lt;- predict(m1_ranger_processed, valid_tran) roc &lt;- pROC::roc(validation$Attrition, m1_tran_pred$predictions[, 1]) pROC::auc(roc) ## Area under the curve: 0.8591 4.5.1.2 Tuning With the ranger function we can tune various hyperparameters mentioned in the general tuning section. For example, the following model adjusts: num.trees: increase number of trees to 750 mtry: increase the number of predictor variables to randomly select at each split to 20 min.node.size: increase minimum node size to 3 (default is 1 for classification) sample.fraction: increase training set to 70% m2_ranger &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 750, mtry = 20, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = .70 ) # misclassification rate m2_ranger$prediction.error ## [1] 0.1393373 # model results m2_ranger ## Ranger result ## ## Call: ## ranger(formula = Attrition ~ ., data = attrit_train, num.trees = 750, mtry = 20, respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = 0.7) ## ## Type: Classification ## Number of trees: 750 ## Sample size: 1177 ## Number of independent variables: 30 ## Mtry: 20 ## Target node size: 3 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error: 13.93 % We can continue to adjust these settings individually to identify the optimal combination; however, this become tedious when you want to explore a larger grid search. Similar to the regression setting, to perform a larger grid search across several hyperparameters we need to create a grid and loop through each hyperparameter combination and evaluate the model. First we want to construct our grid of hyperparameters. We’re going to search across 200 different models with varying number of trees, mtry, minimum node size, and sample size. I also vary the split rule, which determines when and how to split into branches. # hyperparameter grid search hyper_grid &lt;- expand.grid( num.trees = seq(250, 500, 750), mtry = seq(5, 25, by = 5), node_size = seq(1, 10, by = 3), sample_size = c(.55, .632, .70, .80, 1), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;), OOB_error = 0 ) # total number of combinations nrow(hyper_grid) ## [1] 200 # hyperparameter grid head(hyper_grid) ## num.trees mtry node_size sample_size splitrule OOB_error ## 1 250 5 1 0.55 gini 0 ## 2 250 10 1 0.55 gini 0 ## 3 250 15 1 0.55 gini 0 ## 4 250 20 1 0.55 gini 0 ## 5 250 25 1 0.55 gini 0 ## 6 250 5 4 0.55 gini 0 We can now loop through each hyperparameter combination. Note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. This full grid search took about 90 seconds to compute. Our OOB classification error ranges between ~0.1308-0.1487. Our top 10 performing models all have classification error rates in the lower 0.13 range. The results show that all the top 10 models use less trees, larger mtry than the default (\\(\\text{floor}\\big(\\sqrt{\\text{number of features}}\\big) = 5\\), and a sample size less than 100. However, no definitive patterns are observed with the other hyperparameters. for(i in 1:nrow(hyper_grid)) { # train model model &lt;- ranger( formula = Attrition ~ ., data = attrit_train, respect.unordered.factors = &#39;order&#39;, seed = 123, verbose = FALSE, mtry = hyper_grid$mtry[i], min.node.size = hyper_grid$node_size[i], sample.fraction = hyper_grid$sample_size[i], splitrule = hyper_grid$splitrule[i] ) # add OOB error to grid hyper_grid$OOB_error[i] &lt;- model$prediction.error } hyper_grid %&gt;% dplyr::arrange(OOB_error) %&gt;% head(10) ## num.trees mtry node_size sample_size splitrule OOB_error ## 1 250 25 1 0.800 gini 0.1308411 ## 2 250 20 7 0.632 gini 0.1325404 ## 3 250 25 10 0.800 extratrees 0.1325404 ## 4 250 25 4 0.700 gini 0.1333900 ## 5 250 25 7 0.700 gini 0.1333900 ## 6 250 20 7 0.550 extratrees 0.1333900 ## 7 250 25 10 0.700 extratrees 0.1333900 ## 8 250 25 4 0.800 extratrees 0.1333900 ## 9 250 25 1 0.700 gini 0.1342396 ## 10 250 10 7 1.000 gini 0.1342396 The above grid search helps to focus where we can further refine our model tuning. As a next step, we would perform additional grid searches; however, for brevity we will leave this as an exercise for the reader. 4.5.1.3 Feature interpretation 4.5.1.3.1 Feature importance As in the regression setting, once we’ve found our optimal hyperparameter settings we can re-run our model and set the importance argument to “impurity” and/or “permutation”. The following applies both settings so that we can compare and contrast the influential variables each method identifies. m3_ranger_impurity &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 250, mtry = 25, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;impurity&#39; ) m3_ranger_permutation &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 250, mtry = 25, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;permutation&#39; ) Plotting the top 25 influential variables using both variable importance methods results in a common theme among the top 3 variables - MonthlyIncome, Age, and OverTime appear to have strong influence on our results. We also saw OverTime as an influential variable using regularized regression. Looking at the next dozen important variables, we see similar results across variable importance approaches but just in different order (i.e. TotalWorkingYears, JobRole, NumCompaniesWorked). Some of these were influential variables in the regularized regression models and some were not; suggesting our random forest model is picking up different patterns and logic in our data. p1 &lt;- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle(&quot;Impurity-based variable importance&quot;) p2 &lt;- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle(&quot;Permutation-based variable importance&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 4.10: Top 25 most important variables based on impurity (left) and permutation (right). 4.5.1.3.2 Feature effects After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. This is important considering random forests allow us to pick up non-linear, non-monotonic relationships. For this we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves. However, to generate PDPs and ICE curves we need to run a probability model (probability = TRUE) so that we can extract the class probabilities. To produce a PDP with binary classification problems, we need to create a custom prediction function that will return a vector of the mean predicted probability for the response class of interest (in this example we want the probabilities for Attrition = “Yes”). We supply this custom prediction function within the pdp::partial function call. Our PDPs illustrate a strong increase in the probability of attrition for employees that work overtime. Also, note that non-linear relationship between the probability of attrition and monthly income and age. The MonthlyIncome plot shows an increase in probability as monthly income reaches $10,000 but then flatlines until employees make about $20,000 per month. Similiarly with age, as employees get older they tend to become more stable; however, this changes after the age of 45 where an increase of age tends to increase the probablity of attrition (recall in Section 3.5.2.3 that we saw how the regularized models assumed a constantly decreasing relationships between Age and the probability of attrition). # probability model m3_ranger_prob &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 250, mtry = 25, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, probability = TRUE, importance = &#39;impurity&#39; ) # custom prediction function custom_pred &lt;- function(object, newdata) { pred &lt;- predict(object, newdata) avg &lt;- mean(pred$predictions[, 1]) return(avg) } # partial dependence of OverTime p1 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;OverTime&quot;, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train) # partial dependence of MonthlyIncome p2 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;MonthlyIncome&quot;, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train) # partial dependence of Age p3 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;Age&quot;, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train) gridExtra::grid.arrange(p1, p2, p3, nrow = 1) Figure 4.11: Partial dependence plots of our top 3 influential variables. Note the non-linear, non-monotonic relationship our random forest model is picking up for MonthlyIncome and Age. We can extract more insights with centered ICE curves. Although the PDP illustrates an increase in the average probability of attrition for employees who work overtime, the ICE curves illustrate that this is not the case for all employees. A fair amount of the observations actually experience a decrease in probability when they work overtime. This likely suggests an intereaction effect with other variables (we will discuss how to tease out interactions in the Model Interpretability chapter). To produce ICE curves with binary classification problems, we need to create a custom prediction function that will return a vector of the predicted probabilities for the response class of interest. # custom prediction function custom_pred &lt;- function(object, newdata) { pred &lt;- predict(object, newdata) avg &lt;- pred$predictions[, 1] return(avg) } # ICE curves for top 3 influential variables p1 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;OverTime&quot;, ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train, alpha = 0.2) p2 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;MonthlyIncome&quot;, ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train, alpha = 0.2) p3 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;Age&quot;, ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train, alpha = 0.2) gridExtra::grid.arrange(p1, p2, p3, nrow = 1) Figure 4.12: Centered ICE curves for our top 3 influential variables. 4.5.1.3.3 ROC curve As in the regularize regression chapter, we can visualize the ROC curve with the ROCR and pROC packages. Both packages compare the predicted probability output to the actual observed class so we need to use our probability ranger model m3_ranger_prob. The predicted probabilities for our model are accessible at ranger_model$predictions and we want to index for the class of interest. library(ROCR) library(pROC) # plot structure par(mfrow = c(1, 2)) # ROCR plot prediction(m3_ranger_prob$predictions[, 1], attrit_train$Attrition) %&gt;% performance(&quot;tpr&quot;, &quot;fpr&quot;) %&gt;% plot(main = &quot;ROCR ROC curve&quot;) #pROC plot roc(attrit_train$Attrition, m3_ranger_prob$predictions[, 1]) %&gt;% plot(main = &quot;pROC ROC curve&quot;, legacy.axes = TRUE) Figure 4.13: ROC curve for our ranger random forest model based on the training data. 4.5.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. If you use a probability model, the predicted values will be probabilities for each class. If you use a non-probability model, the predicted values will be the class. # predict a probability model pred_probs &lt;- predict(m3_ranger_prob, attrit_test) head(pred_probs$predictions) ## Yes No ## [1,] 0.124 0.876 ## [2,] 0.472 0.528 ## [3,] 0.056 0.944 ## [4,] 0.064 0.936 ## [5,] 0.296 0.704 ## [6,] 0.124 0.876 # predict a non-probability model pred_class &lt;- predict(m3_ranger_impurity, attrit_test) head(pred_class$predictions) ## [1] No No No No No No ## Levels: Yes No Lastly, to assess various performance metrics on our test data we use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. If you compare the results to the regularized regression model you will notice that our random forest model does not provide additional predictive performance. You need to supply caret::confusionMatrix with the predicted class. Consequently, if you use the probability model which predicts probabilities then you will need perform an extract step that creates a predicted class based on the probabilities (i.e. ifelse(probability &gt;= .5, “Yes”, “No”)). caret::confusionMatrix(factor(pred_class$predictions), attrit_test$Attrition, positive = &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 13 5 ## No 34 241 ## ## Accuracy : 0.8669 ## 95% CI : (0.8226, 0.9036) ## No Information Rate : 0.8396 ## P-Value [Acc &gt; NIR] : 0.1145 ## ## Kappa : 0.3415 ## Mcnemar&#39;s Test P-Value : 7.34e-06 ## ## Sensitivity : 0.27660 ## Specificity : 0.97967 ## Pos Pred Value : 0.72222 ## Neg Pred Value : 0.87636 ## Prevalence : 0.16041 ## Detection Rate : 0.04437 ## Detection Prevalence : 0.06143 ## Balanced Accuracy : 0.62814 ## ## &#39;Positive&#39; Class : Yes ## 4.5.2 h20 To perform a binary classification random forest with h2o, we first need to initiate our h2o session. # launch h2o h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 729 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_ply740 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we need to convert our training and test data to h2o objects. # convert training data to h2o object attrit_train_h2o &lt;- as.h2o(attrit_train) # convert test data to h2o object attrit_test_h2o &lt;- as.h2o(attrit_test) # set the response column to Attrition response &lt;- &quot;Attrition&quot; # set the predictor names predictors &lt;- setdiff(colnames(attrit_train), &quot;Attrition&quot;) 4.5.2.1 Basic implementation Similar to our regression problem, we use h2o::h2o.randomForest to perform a random forest model with h2o. Most of the default parameter settings in h2o.randomForest do not change between a regression and classification problem. However, mtries (how many predictor variables are randomly selected at each split) defaults to \\(\\texttt{floor}(\\sqrt{p})\\). As long as your response variable is encoded as a character or factor, h2o will apply a binomial or multinomial classification model. Alternatively, you can specify the response distribution with the distribution argument. The following performs a default h2o.randomForest model with 250 trees and a 10 fold cross validation. As the model results show, averaging across all 250 trees provides an OOB \\(AUC = 0.8\\). # perform basic random forest model m1_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = attrit_train_h2o, ntrees = 250, seed = 123, nfolds = 10, keep_cross_validation_predictions = TRUE ) # look at results h2o.performance(m1_h2o, xval = TRUE) ## H2OBinomialMetrics: drf ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.1076479 ## RMSE: 0.3280973 ## LogLoss: 0.3600018 ## Mean Per-Class Error: 0.2996321 ## AUC: 0.795918 ## Gini: 0.591836 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## No Yes Error Rate ## No 915 72 0.072948 =72/987 ## Yes 100 90 0.526316 =100/190 ## Totals 1015 162 0.146134 =172/1177 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.304000 0.511364 81 ## 2 max f2 0.164000 0.607055 130 ## 3 max f0point5 0.352000 0.579365 65 ## 4 max accuracy 0.356000 0.869159 64 ## 5 max precision 0.754667 1.000000 0 ## 6 max recall 0.016000 1.000000 177 ## 7 max specificity 0.754667 1.000000 0 ## 8 max absolute_mcc 0.352000 0.438268 65 ## 9 max min_per_class_accuracy 0.180000 0.727457 124 ## 10 max mean_per_class_accuracy 0.220000 0.741097 108 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` The results above used all 250 trees but to make sure we are providing enough trees to stabilize the OOB error we can include automatic stopping. Also, when dealing with classification problems, if your response variable is significantly imbalanced, you can achieve additional predictive accuracy by over/under sampling. We can over/under sample our classes to achieve balanced class counts by incorporating the balance_classes argument. However, in this example we do not achieve any performance improvement by balancing our attrition classes. You will, typically, achieve performance improvements by over/under sampling when you binary response variable has a 90/10 or worse class imbalance. # perform basic random forest model m2_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = attrit_train_h2o, ntrees = 500, seed = 123, nfolds = 10, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, stopping_metric = &quot;AUC&quot;, # stopping mechanism stopping_rounds = 10, # number of rounds stopping_tolerance = 0 # stops after trees add no improvement ) # look at results h2o.performance(m2_h2o, xval = TRUE) ## H2OBinomialMetrics: drf ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.129077 ## RMSE: 0.3592728 ## LogLoss: 0.4325378 ## Mean Per-Class Error: 0.2804218 ## AUC: 0.7969045 ## Gini: 0.593809 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## No Yes Error Rate ## No 875 112 0.113475 =112/987 ## Yes 85 105 0.447368 =85/190 ## Totals 960 217 0.167375 =197/1177 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.086292 0.515971 143 ## 2 max f2 0.044498 0.602837 239 ## 3 max f0point5 0.135658 0.574205 78 ## 4 max accuracy 0.135658 0.869159 78 ## 5 max precision 0.570324 1.000000 0 ## 6 max recall 0.008554 1.000000 383 ## 7 max specificity 0.570324 1.000000 0 ## 8 max absolute_mcc 0.135658 0.424441 78 ## 9 max min_per_class_accuracy 0.052971 0.721378 216 ## 10 max mean_per_class_accuracy 0.056358 0.730742 207 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` 4.5.2.2 Tuning As discussed in the regression section of this chapter, h2o.randomForest provides several tunable hyperparameters, for which we can perform a full (aka full cartesian) or stochastic (aka random discrete) grid search across. 4.5.2.2.1 Full cartesian grid search First, we can try a comprehensive (full cartesian) grid search, which means we will examine every combination of hyperparameter settings that we specify in hyper_grid.h2o. Here, we search across 360 models. To speed up the grid search I dropped it down to 5-fold cross validation; however, this grid search still took 30 minutes. As an alternative, you could create a single validation frame (see validation_frame in ?h2o.grid) to score against rather than perform k-fold cross validation. The results show a maximum AUC of 1 but don’t get too excited about this. When you over/under sample with balance_classes = TRUE, we are essentially bootstrapping extra samples of the observations with Attrition = Yes. This means our up-sampled observations will have many of the same values across the features. This makes it easier to over exaggerate the predictive performance during our training. A few characteristics we notice from our results suggest that predictive accuracy is maximized when balance_classes = TRUE, max_depth is larger, min_rows is smaller, and sample_rate \\(&lt; 1\\). # hyperparameter grid hyper_grid.h2o &lt;- list( mtries = c(2, 5, 10, 15), max_depth = seq(10, 30, by = 5), min_rows = c(1, 3, 5), sample_rate = c(.632, .8, .95), balance_classes = c(TRUE, FALSE) ) # build grid search grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_full_grid&quot;, x = predictors, y = response, training_frame = attrit_train_h2o, hyper_params = hyper_grid.h2o, search_criteria = list(strategy = &quot;Cartesian&quot;), ntrees = 500, seed = 123, nfolds = 5, keep_cross_validation_predictions = TRUE, stopping_metric = &quot;AUC&quot;, stopping_rounds = 10, stopping_tolerance = 0 ) # collect the results and sort by our model performance metric of choice full_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_full_grid&quot;, sort_by = &quot;auc&quot;, decreasing = TRUE ) print(full_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_full_grid ## Used hyper parameters: ## - balance_classes ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 720 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by decreasing auc ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 1 true 25 1.0 2 0.632 rf_full_grid_model_6 1.0 ## 2 true 30 1.0 10 0.95 rf_full_grid_model_308 1.0 ## 3 true 30 1.0 2 0.632 rf_full_grid_model_8 1.0 ## 4 true 30 1.0 5 0.8 rf_full_grid_model_158 1.0 ## 5 true 15 1.0 10 0.8 rf_full_grid_model_182 1.0 ## ## --- ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 715 false 10 1.0 10 0.95 rf_full_grid_model_301 0.6200193262072915 ## 716 false 25 3.0 5 0.95 rf_full_grid_model_287 0.6037350054525626 ## 717 false 10 1.0 15 0.95 rf_full_grid_model_331 0.6013023735068234 ## 718 false 10 1.0 2 0.95 rf_full_grid_model_241 0.5981035997865863 ## 719 false 25 1.0 10 0.95 rf_full_grid_model_307 0.5896310022558814 ## 720 false 15 1.0 2 0.95 rf_full_grid_model_243 0.5875399042298484 4.5.2.2.2 Random discrete grid search Rather than perform a full grid search, we could’ve sped up the search process by using the random discrete grid search. The following performs a random search across the same 360 hyperparameter combinations, stopping if none of the last 10 models have managed to have a 0.01% improvement in AUC compared to the best model before that. I cut the grid search off after 1200 seconds (20 minutes) if a final approximately optimal model is not found. Our grid search assessed 171 of the 360 models before stopping and the best model achieved an AUC of 0.812. # random grid search criteria search_criteria &lt;- list( strategy = &quot;RandomDiscrete&quot;, stopping_metric = &quot;AUC&quot;, stopping_tolerance = 0.0001, stopping_rounds = 10, max_runtime_secs = 60*20 ) # build grid search random_grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_random_grid&quot;, x = predictors, y = response, training_frame = attrit_train_h2o, hyper_params = hyper_grid.h2o, search_criteria = search_criteria, ntrees = 500, seed = 123, nfolds = 5, keep_cross_validation_predictions = TRUE, stopping_metric = &quot;AUC&quot;, stopping_rounds = 10, stopping_tolerance = 0 ) # collect the results and sort by our model performance metric of choice random_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_random_grid&quot;, sort_by = &quot;auc&quot;, decreasing = TRUE ) print(random_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_random_grid ## Used hyper parameters: ## - balance_classes ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 171 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by decreasing auc ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 1 false 30 3.0 2 0.95 rf_random_grid_model_39 0.8121580547112462 ## 2 false 10 3.0 2 0.95 rf_random_grid_model_163 0.8115421532554791 ## 3 false 20 1.0 5 0.95 rf_random_grid_model_101 0.7974137471337919 ## 4 false 30 3.0 2 0.632 rf_random_grid_model_73 0.7966458699941342 ## 5 false 15 5.0 2 0.8 rf_random_grid_model_9 0.7965312216711993 ## ## --- ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 166 true 20 3.0 15 0.95 rf_random_grid_model_113 0.7474084146536555 ## 167 true 30 3.0 15 0.8 rf_random_grid_model_83 0.7469284914413694 ## 168 true 10 1.0 15 0.95 rf_random_grid_model_107 0.74409961072895 ## 169 true 25 1.0 15 0.95 rf_random_grid_model_63 0.7314962939263051 ## 170 true 20 1.0 15 0.95 rf_random_grid_model_142 0.7314962939263051 ## 171 true 30 1.0 15 0.95 rf_random_grid_model_61 0.7290326881032368 Once we’ve identifed the best set of hyperparameters, we can extract the model. For the remaining examples I will use the optimal model from the random grid search. # Grab the model_id for the top model, chosen by validation error best_model_id &lt;- random_grid_perf@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) 4.5.2.3 Feature interpretation 4.5.2.3.1 Feature importance Assessing the variable importance, we see similar results as with the ranger model with the most influential variables including OverTime, MonthlyIncome, Age, and JobRole. # plot top 25 influential variables vip(best_model, num_features = 25, bar = FALSE) 4.5.2.3.2 Feature effects As with ranger, we can also assess PDPs and ICE curves. The following looks at three of the most influential variables in our model (OverTime, MonthlyIncome and Age). Our centered ICE curves help to illustrate the marginal increasing or decreasing effect on the predicted probability of attrition. In all three plots we see groups of observations going in opposite directions (i.e. at age 35 many employees experience an increase in the probability of attrition while many others a decrease). This indicates interaction effects with other features, which we will explore more in the Model Interpretability chapter. These plots illustrate similar attributes that we saw in the ranger PDP/ICE curve plots. pfun &lt;- function(object, newdata) { as.data.frame(predict(object, newdata = as.h2o(newdata)))[[3L]] } # JobRole partial dependencies ot.ice &lt;- partial( best_model, pred.var = &quot;OverTime&quot;, train = attrit_train, pred.fun = pfun ) # MonthlyIncome partial dependencies income.ice &lt;- partial( best_model, pred.var = &quot;MonthlyIncome&quot;, train = attrit_train, pred.fun = pfun, grid.resolution = 20 ) # Age partial dependencies age.ice &lt;- partial( best_model, pred.var = &quot;Age&quot;, train = attrit_train, pred.fun = pfun, grid.resolution = 20 ) p1 &lt;- autoplot(ot.ice, alpha = 0.1, center = TRUE) p2 &lt;- autoplot(income.ice, alpha = 0.1, center = TRUE) p3 &lt;- autoplot(age.ice, alpha = 0.1, center = TRUE) gridExtra::grid.arrange(p1, p2, p3, nrow = 1) Figure 4.14: ICE curves for OverTime, MonthlyIncome, and Age. 4.5.2.3.3 ROC curve Visualizing our ROC curve helps to illustrate our cross-validated AUC of 0.812. h2o.performance(best_model, xval = TRUE) %&gt;% plot() Figure 4.15: ROC curve for our ranger random forest model based on the training data. 4.5.2.4 Predicting Finally, if you are satisfied with your final model we can predict values for an unseen data set a couple different ways. Both predict and h2o.predict will provide the predicted class and the probability of each class. We can also quickly assess the model’s performance on our test set with h2o.performance. # predict new values with base R predict() predict(best_model, attrit_test_h2o) ## predict No Yes ## 1 No 0.9205128 0.07948718 ## 2 Yes 0.6083333 0.39166666 ## 3 No 0.9163170 0.08368298 ## 4 No 0.8282505 0.17174950 ## 5 No 0.7910256 0.20897436 ## 6 No 0.9775641 0.02243590 ## ## [293 rows x 3 columns] # predict new values with h2o.predict() h2o.predict(best_model, newdata = attrit_test_h2o) ## predict No Yes ## 1 No 0.9205128 0.07948718 ## 2 Yes 0.6083333 0.39166666 ## 3 No 0.9163170 0.08368298 ## 4 No 0.8282505 0.17174950 ## 5 No 0.7910256 0.20897436 ## 6 No 0.9775641 0.02243590 ## ## [293 rows x 3 columns] # assess performance on test data h2o.performance(best_model, newdata = attrit_test_h2o) ## H2OBinomialMetrics: drf ## ## MSE: 0.1017999 ## RMSE: 0.319061 ## LogLoss: 0.3423053 ## Mean Per-Class Error: 0.2296316 ## AUC: 0.8374849 ## Gini: 0.6749697 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## No Yes Error Rate ## No 222 24 0.097561 =24/246 ## Yes 17 30 0.361702 =17/47 ## Totals 239 54 0.139932 =41/293 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.254258 0.594059 53 ## 2 max f2 0.160211 0.667752 118 ## 3 max f0point5 0.382280 0.646259 24 ## 4 max accuracy 0.382280 0.883959 24 ## 5 max precision 0.739642 1.000000 0 ## 6 max recall 0.022436 1.000000 286 ## 7 max specificity 0.739642 1.000000 0 ## 8 max absolute_mcc 0.254258 0.511808 53 ## 9 max min_per_class_accuracy 0.194833 0.744681 94 ## 10 max mean_per_class_accuracy 0.160211 0.777634 118 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` h2o.shutdown(prompt = FALSE) ## [1] TRUE 4.6 Implementation: Multinomial Classification To illustrate various random forest concepts for a multinomial classification problem we will continue with the mnist data, where the goal is to predict handwritten numbers ranging from 0-9. # import mnist training and testing data train &lt;- data.table::fread(&quot;../data/mnist_train.csv&quot;, data.table = FALSE) test &lt;- data.table::fread(&quot;../data/mnist_test.csv&quot;, data.table = FALSE) 4.6.1 ranger To use ranger on a multiclassification problem we need our response variable to be encoded as a character or factor. Since our response variable for the mnist data (V785) contains numeric responses (integers ranging from 0-9), we need to convert these to a factor. # convert response variable to a factor train &lt;- mutate(train, V785 = factor(V785)) test &lt;- mutate(test, V785 = factor(V785)) 4.6.1.1 Basic implementation Once our response variable is properly encoded as a factor or character, ranger::ranger will automatically apply a random forest model with multinomial terminal nodes without you having to specify. Consequently, the following applies a default random forest model with 500 trees. Since all the predictors in the mnist data set are numeric we do not need to worry about setting the respect.unordered.factors parameter. As your data set grows in size, random forests can become slow. Parallel processing can speed up the process and, by default, ranger will use the number of CPUs available (you can manually set the number of CPUs to use with num.threads). However, even so, on the mnist data the default ranger model takes three minutes to train. The results show an OOB error of 2.98%, which is already more accurate than the tuned regularized regression results (see Section 3.6). This basic default model took a little over 3 minutes to train. # perform basic random forest model m1_ranger &lt;- ranger( formula = V785 ~ ., data = train, num.trees = 500, verbose = FALSE, seed = 123 ) # look at results m1_ranger ## Ranger result ## ## Call: ## ranger(formula = V785 ~ ., data = train, num.trees = 500, seed = 123) ## ## Type: Classification ## Number of trees: 500 ## Sample size: 60000 ## Number of independent variables: 784 ## Mtry: 28 ## Target node size: 1 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error: 2.98 % # look at confusion matrix m1_ranger$confusion.matrix ## predicted ## true 0 1 2 3 4 5 6 7 8 9 ## 0 5855 1 7 2 3 6 18 0 28 3 ## 1 1 6646 36 11 9 4 5 13 9 8 ## 2 24 9 5787 23 22 2 14 37 36 4 ## 3 9 6 73 5840 1 62 8 44 62 26 ## 4 8 11 9 0 5683 0 23 10 12 86 ## 5 21 5 8 61 9 5223 41 5 27 21 ## 6 20 11 5 1 9 30 5824 0 18 0 ## 7 3 22 55 6 28 0 0 6069 10 72 ## 8 8 27 30 32 20 36 26 4 5610 58 ## 9 21 9 13 65 55 19 4 45 42 5676 4.6.1.2 Tuning As in the binary classification section, we will tune the various hyperparameters of the ranger function. The following creates a hyperparameter grid of 160 model combinations with varying number of trees (num_trees), number of variables to possibly split at (mtry), terminal node size (node_size), sample size (sample_size), and the split rule (splitrule). hyper_grid &lt;- expand.grid( num_trees = c(250, 500), mtry = seq(15, 35, by = 5), node_size = seq(1, 10, by = 3), sample_size = c(.632, .80), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;), OOB_error = 0 ) nrow(hyper_grid) Our OOB classification error ranges between 0.0302-0.0383. Our top 10 performing models all have classification error rates in the low 0.03 range. The results show that most of the top 10 models use higher mtry values, smaller min.node.size, and include a stochastic nature with sample.fraction \\(&gt; 1\\). We also see that the two top models use the extratrees split rule versus the gini; however, the improvement is marginal. This grid search took 5 hours and 38 minutes! for(i in 1:nrow(hyper_grid)) { # train model model &lt;- ranger( formula = V785 ~ ., data = train, seed = 123, verbose = FALSE, num.trees = hyper_grid$num_trees[i], mtry = hyper_grid$mtry[i], min.node.size = hyper_grid$node_size[i], sample.fraction = hyper_grid$sample_size[i], splitrule = hyper_grid$splitrule[i] ) # add OOB error to grid hyper_grid$OOB_error[i] &lt;- model$prediction.error } hyper_grid %&gt;% dplyr::arrange(OOB_error) %&gt;% head(10) ## num.trees mtry node_size sample_size splitrule OOB_error ## 1 250 35 1 0.8 extratrees 0.03021667 ## 2 500 35 1 0.8 extratrees 0.03021667 ## 3 250 35 1 0.8 gini 0.03046667 ## 4 500 35 1 0.8 gini 0.03046667 ## 5 250 35 4 0.8 gini 0.03061667 ## 6 500 35 4 0.8 gini 0.03061667 ## 7 250 30 1 0.8 gini 0.03065000 ## 8 500 30 1 0.8 gini 0.03065000 ## 9 250 20 1 0.8 gini 0.03078333 ## 10 500 20 1 0.8 gini 0.03078333 The above grid search helps to focus where we can further refine our model tuning. As a next step, we could perform additional grid searches focusing on additional ranges of these parameters. However, for brevity we leave this as an exercise for the reader. 4.6.1.3 Feature interpretation 4.6.1.3.1 Feature importance As in the regression and binary classification setting, once we’ve found our optimal hyperparameter settings we can re-run our model and set the importance argument to “impurity” and/or “permutation”. The following applies both settings so that we can compare and contrast the influential variables each method identifies. # re-run model with impurity-based variable importance m3_ranger_impurity &lt;- ranger( formula = V785 ~ ., data = train, num.trees = 250, mtry = 35, min.node.size = 1, sample.fraction = .80, splitrule = &quot;extratrees&quot;, importance = &#39;impurity&#39;, verbose = FALSE, seed = 123 ) # re-run model with permutation-based variable importance m3_ranger_permutation &lt;- ranger( formula = V785 ~ ., data = train, num.trees = 250, mtry = 35, min.node.size = 1, sample.fraction = .80, splitrule = &quot;extratrees&quot;, importance = &#39;permutation&#39;, verbose = FALSE, seed = 123 ) In the regularized regression chapter, we saw that 67 predictors were not used because they contained zero variance. For random forests, we can assess how many and which variables were not used for any splits within our random forest model to improve impurtiy. This signals those variables that do not provide any increase in the accuracy within each terminal node. For our m3_ranger_impurity model there are 102 variables not used for any splits. We can do the same with the m3_ranger_permutation model; however, variables with zero importance represent those variables that when scrambled, still do not hurt the performance of our model. We see there are 153 variables that have zero importance for the permutation-based approach. # how many variables not used for a split which(m3_ranger_impurity$variable.importance == 0) %&gt;% length() ## [1] 95 # how many variables where randomizing their values does not hurt performance which(m3_ranger_permutation$variable.importance == 0) %&gt;% length() ## [1] 153 Alternatively, to find important variables we can extract the top 10 influential variables as we did in the regression and binary classification problems. Unlike, in the regularized regression chapter, we cannot extract the most influential variables for each class. This is due to how variable importance is calculated differently between the two. However, shortly we will see how to understand the relationship an influential variable and the different classes of our response variable. Figure 4.16 illustrates the top 25 influential variables in our random forest model. We see many of the same variables towards the top albeit in differing order (i.e. V379, V351, V462, V407) signaling that these variables appear influential regardless of the importance measure used. p1 &lt;- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle(&quot;Impurity-based variable importance&quot;) p2 &lt;- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle(&quot;Permutation-based variable importance&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 4.16: Top 25 most important variables based on impurity (left) and permutation (right). 4.6.1.3.2 Feature effects After the most relevant variables have been identified, we can assess the relationship between these influential predictors and the response variable with PDP plots and ICE curves. As with the binary classification model, to generate PDPs and ICE curves we need to use the probability model (probability = TRUE) so that can extract the class probabilities. To produce a PDP with multi-nomial classification problems, we need to create a custom prediction function that will return a data frame of the mean predicted probability for each response class. We supply this custom prediction function to the pdp::partial function call. In this example, we assess the PDP of each response category with variable V379, which ranked first as the most influential variable. We can see those response categories where this variable has a large impact (stronger changes in the predicted value \\(\\hat y\\) as V379 changes) versus those that are less influnced by this predictor (mostly flat-lined plots). # probability model m3_ranger_prob &lt;- ranger( formula = V785 ~ ., data = train, num.trees = 250, mtry = 35, min.node.size = 1, sample.fraction = .80, importance = &#39;impurity&#39;, probability = TRUE, verbose = FALSE, seed = 123, ) # custom prediction function custom_pred &lt;- function(object, newdata) { pred &lt;- predict(object, newdata)$predictions avg &lt;- purrr::map_df(as.data.frame(pred), mean) return(avg) } # partial dependence of V379 pd &lt;- partial(m3_ranger_prob, pred.var = &quot;V379&quot;, pred.fun = custom_pred, train = train) ggplot(pd, aes(V379, yhat, color = factor(yhat.id))) + geom_line(show.legend = FALSE) + facet_wrap(~ yhat.id, nrow = 2) + expand_limits(y = 0) Figure 4.17: Partial dependence plots of our most influential variable (V379) across the 10 response levels. This variable appears to be most influential in predicting the number 0, 1, 3, and 7. 4.6.1.4 Predicting Finally, if you are satisfied with your final model we can predict values for an unseen data set with predict. If using a non-probability model then your predicted outputs will be a vector containing the predicted class for each observation. If using a probability model then your predicted outputs will be the probability of each class. In both cases, the result of predict is a list with the actual predictions contained in object$predictions. # predict class as the output pred_class &lt;- predict(m3_ranger_impurity, test) head(pred_class$predictions) ## [1] 8 3 8 0 1 5 ## Levels: 0 1 2 3 4 5 6 7 8 9 # predict probability as the output pred_prob &lt;- predict(m3_ranger_prob, test) pred_prob$predictions[1:5, ] ## 0 1 2 3 4 5 6 7 8 9 ## [1,] 0.000 0.000 0.004 0.000 0.004 0.004 0.000 0.000 0.988 0.000 ## [2,] 0.000 0.012 0.000 0.868 0.020 0.024 0.004 0.024 0.008 0.040 ## [3,] 0.140 0.000 0.052 0.120 0.004 0.168 0.088 0.000 0.400 0.028 ## [4,] 0.912 0.004 0.004 0.000 0.000 0.024 0.040 0.000 0.008 0.008 ## [5,] 0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 Lastly, to assess various performance metrics on our test data we can use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. We can see that the overall accuracy rate of 0.9702 is significantly higher than our accuracy of \\(\\approx 0.93\\) for both regularized regression models. We also see our model is doing a much better job of predicting “2”, “3”, “5”, “8”, and “9” which were all poorly predicted by the regularized regression models. caret::confusionMatrix(factor(pred_class$predictions), factor(test$V785)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 969 0 6 0 1 3 7 1 3 6 ## 1 0 1123 0 0 0 0 3 4 0 5 ## 2 1 2 996 7 2 0 0 20 4 2 ## 3 0 4 8 974 0 10 0 1 9 8 ## 4 0 0 3 0 955 0 2 1 5 8 ## 5 2 1 0 10 0 863 4 0 5 1 ## 6 4 3 3 0 6 6 940 0 1 1 ## 7 1 0 10 10 1 1 0 983 3 6 ## 8 3 1 6 7 2 6 2 2 935 8 ## 9 0 1 0 2 15 3 0 16 9 964 ## ## Overall Statistics ## ## Accuracy : 0.9702 ## 95% CI : (0.9667, 0.9734) ## No Information Rate : 0.1135 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9669 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 ## Sensitivity 0.9888 0.9894 0.9651 0.9644 0.9725 0.9675 ## Specificity 0.9970 0.9986 0.9958 0.9956 0.9979 0.9975 ## Pos Pred Value 0.9729 0.9894 0.9632 0.9606 0.9805 0.9740 ## Neg Pred Value 0.9988 0.9986 0.9960 0.9960 0.9970 0.9968 ## Prevalence 0.0980 0.1135 0.1032 0.1010 0.0982 0.0892 ## Detection Rate 0.0969 0.1123 0.0996 0.0974 0.0955 0.0863 ## Detection Prevalence 0.0996 0.1135 0.1034 0.1014 0.0974 0.0886 ## Balanced Accuracy 0.9929 0.9940 0.9804 0.9800 0.9852 0.9825 ## Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 0.9812 0.9562 0.9600 0.9554 ## Specificity 0.9973 0.9964 0.9959 0.9949 ## Pos Pred Value 0.9751 0.9685 0.9619 0.9545 ## Neg Pred Value 0.9980 0.9950 0.9957 0.9950 ## Prevalence 0.0958 0.1028 0.0974 0.1009 ## Detection Rate 0.0940 0.0983 0.0935 0.0964 ## Detection Prevalence 0.0964 0.1015 0.0972 0.1010 ## Balanced Accuracy 0.9893 0.9763 0.9779 0.9751 4.6.2 h2o To perform regularized multinomial logistic regression with h2o, we first need to initiate our h2o session. h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 6 seconds 149 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_ply740 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.39 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Since our data is already split into a training test set, to prepare for modeling, we just need to convert our training and test data to h2o objects and identify the response and predictor variables. One key difference compared to prior classification procedures, to perform a multinomial modeling with h2o we need to convert the response variable to a factor. # convert training data to h2o object train_h2o &lt;- train %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # convert test data to h2o object test_h2o &lt;- test %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # set the response column to V785 response &lt;- &quot;V785&quot; # set the predictor names predictors &lt;- setdiff(colnames(train), response) 4.6.2.1 Basic implementation Following the previous H2O random forest sections, we use h2o.randomForest; however, we need to set distribution = &quot;multinomial&quot;. The following trains a 5-fold cross-validated random forest with default settings but we increase the number of trees to 500 and incorporate early stopping (stopping if the logloss objective function does not improve over the past 10 additional trees). # train your model, where you specify alpha (performs 5-fold CV) h2o_fit1 &lt;- h2o.randomForest( x = predictors, y = response, training_frame = train_h2o, distribution = &quot;multinomial&quot;, ntrees = 500, seed = 123, nfolds = 5, keep_cross_validation_predictions = TRUE, stopping_metric = &quot;logloss&quot;, stopping_rounds = 10, stopping_tolerance = 0 ) Our results show that early stopping kicks in at 397 trees and our cross-validated logloss is 0.241 with an average accuracy of 0.967 (similar to our ranger model performance). Looking at the confusion matrix, we see that our model does the best at predicting the numbers 0, 1, 2, and 6 but performs the worst when predicting the numbers 3, 8, and 9. h2o_fit1 ## Model Details: ## ============== ## ## H2OMultinomialModel: drf ## Model ID: DRF_model_R_1533607021291_1 ## Model Summary: ## number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves ## 1 397 3970 49473256 15 20 19.79975 395 1599 984.52640 ## ## ## H2OMultinomialMetrics: drf ## ** Reported on training data. ** ## ** Metrics reported on Out-Of-Bag training samples ** ## ## Training Set Metrics: ## ===================== ## ## Extract training frame with `h2o.getFrame(&quot;file12e3866a06dc9_sid_a8e8_2&quot;)` ## MSE: (Extract with `h2o.mse`) 0.06306652 ## RMSE: (Extract with `h2o.rmse`) 0.2511305 ## Logloss: (Extract with `h2o.logloss`) 0.2298843 ## Mean Per-Class Error: 0.03121473 ## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 5851 1 6 1 4 5 16 2 34 3 0.0122 = 72 / 5,923 ## 1 2 6636 44 15 11 0 8 10 9 7 0.0157 = 106 / 6,742 ## 2 24 9 5782 35 22 0 8 26 45 7 0.0295 = 176 / 5,958 ## 3 4 6 79 5839 5 39 6 52 62 39 0.0476 = 292 / 6,131 ## 4 5 10 14 1 5674 0 23 9 14 92 0.0288 = 168 / 5,842 ## 5 24 6 9 51 7 5216 55 8 26 19 0.0378 = 205 / 5,421 ## 6 16 8 3 0 7 50 5809 0 25 0 0.0184 = 109 / 5,918 ## 7 7 22 50 6 21 2 0 6068 9 80 0.0314 = 197 / 6,265 ## 8 10 28 31 36 23 42 34 8 5590 49 0.0446 = 261 / 5,851 ## 9 20 9 10 65 57 14 3 61 35 5675 0.0461 = 274 / 5,949 ## Totals 5963 6735 6028 6049 5831 5368 5962 6244 5849 5971 0.0310 = 1,860 / 60,000 ## ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.969000 ## 2 2 0.990133 ## 3 3 0.995917 ## 4 4 0.998217 ## 5 5 0.999067 ## 6 6 0.999300 ## 7 7 0.999500 ## 8 8 0.999600 ## 9 9 0.999683 ## 10 10 1.000000 ## ## ## ## H2OMultinomialMetrics: drf ## ** Reported on cross-validation data. ** ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## Cross-Validation Set Metrics: ## ===================== ## ## Extract cross-validation frame with `h2o.getFrame(&quot;file12e3866a06dc9_sid_a8e8_2&quot;)` ## MSE: (Extract with `h2o.mse`) 0.06701525 ## RMSE: (Extract with `h2o.rmse`) 0.258873 ## Logloss: (Extract with `h2o.logloss`) 0.2413819 ## Mean Per-Class Error: 0.03299157 ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,xval = TRUE)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.967250 ## 2 2 0.990100 ## 3 3 0.995750 ## 4 4 0.998100 ## 5 5 0.999117 ## 6 6 0.999600 ## 7 7 0.999850 ## 8 8 0.999900 ## 9 9 0.999950 ## 10 10 1.000000 ## ## ## Cross-Validation Metrics Summary: ## mean sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_valid ## accuracy 0.9672536 0.0015155711 0.97094333 0.96793 0.9645565 0.96675926 0.96607906 ## err 0.03274637 0.0015155711 0.029056698 0.03206997 0.035443455 0.033240765 0.033920962 ## err_count 393.0 18.820202 349.0 385.0 430.0 395.0 406.0 ## logloss 0.24139565 0.002679856 0.23648557 0.24019359 0.23914783 0.24380569 0.24734557 ## max_per_class_error 0.052197315 0.002844122 0.04480135 0.054357205 0.055464927 0.05107084 0.05529226 ## mean_per_class_accuracy 0.96702486 0.0015379025 0.97068304 0.96770495 0.96408165 0.96661985 0.9660349 ## mean_per_class_error 0.032975126 0.0015379025 0.029316958 0.032295052 0.035918355 0.033380147 0.033965122 ## mse 0.06701857 9.877497E-4 0.065272674 0.066462636 0.06653778 0.06733827 0.06948148 ## r2 0.9919707 1.3911341E-4 0.9921999 0.99204344 0.9920536 0.99194384 0.9916128 ## rmse 0.25886548 0.0019011803 0.25548518 0.25780347 0.25794917 0.25949618 0.2635934 4.6.2.2 Tuning To find the near-optimal model we’ll perform a grid search over the common hyperparameters. Since the mnist data is large, a full cartesian grid search takes significant time. Consequently, the following jumps straight to performing a random discrete grid search across 144 hyperparameter combinations of varying mtry, max_depth, min_rows, and sample_rate. Our grid search seeks to optimize the logloss cost function but will stop search if the last 10 models do not improve by 0.01% or if training time reaches 3 hours. This grid search ran for 3 hours and only evaluated 6 models! # create training &amp; validation frame split &lt;- h2o.splitFrame(train_h2o, ratios = .75) train_h2o_v2 &lt;- split[[1]] valid_h2o &lt;- split[[2]] # create hyperparameter grid hyper_grid.h2o &lt;- list( mtries = c(20, 28, 35, 45), max_depth = seq(10, 40, by = 10), min_rows = c(1, 3, 5), sample_rate = c(.632, .8, .95) ) # random grid search criteria search_criteria &lt;- list( strategy = &quot;RandomDiscrete&quot;, stopping_metric = &quot;logloss&quot;, stopping_tolerance = 0.0001, stopping_rounds = 10, max_runtime_secs = 60*60*3 ) # build grid search random_grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;mnist_random_rf_grid&quot;, x = predictors, y = response, training_frame = train_h2o_v2, validation_frame = valid_h2o, distribution = &quot;multinomial&quot;, hyper_params = hyper_grid.h2o, search_criteria = search_criteria, ntrees = 500, seed = 123, stopping_metric = &quot;logloss&quot;, stopping_rounds = 10, stopping_tolerance = 0.0001 ) # collect the results and sort by our model performance metric of choice sorted_grid &lt;- h2o.getGrid(&quot;mnist_random_rf_grid&quot;, sort_by = &quot;logloss&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: mnist_random_rf_grid ## Used hyper parameters: ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 6 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## max_depth min_rows mtries sample_rate model_ids logloss ## 1 30 1.0 45 0.8 mnist_random_rf_grid_model_4 0.21216531556485124 ## 2 30 3.0 35 0.95 mnist_random_rf_grid_model_5 0.22417350102146627 ## 3 30 5.0 35 0.95 mnist_random_rf_grid_model_0 0.23136887656231625 ## 4 30 5.0 35 0.8 mnist_random_rf_grid_model_1 0.24136703713063676 ## 5 30 5.0 28 0.8 mnist_random_rf_grid_model_2 0.2509863265325691 ## 6 10 1.0 20 0.632 mnist_random_rf_grid_model_3 0.35384506436712415 Although are model only evaluated six models, the logloss was reduced to 0.21 (compared to 0.24 in our initial model) but our accuracy of 3% is not much lower than before. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # check out confusion matrix h2o.confusionMatrix(best_model) ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 4391 1 5 0 3 3 7 1 30 2 0.0117 = 52 / 4,443 ## 1 1 4920 34 8 8 0 6 8 4 5 0.0148 = 74 / 4,994 ## 2 21 6 4365 23 15 2 5 24 31 4 0.0291 = 131 / 4,496 ## 3 3 5 59 4315 5 28 6 42 50 28 0.0498 = 226 / 4,541 ## 4 6 8 11 1 4255 0 20 10 12 66 0.0305 = 134 / 4,389 ## 5 17 1 7 32 7 4016 46 2 27 15 0.0369 = 154 / 4,170 ## 6 6 4 3 0 6 37 4325 0 19 0 0.0170 = 75 / 4,400 ## 7 4 12 48 7 21 0 0 4470 5 55 0.0329 = 152 / 4,622 ## 8 7 17 22 31 14 26 20 6 4196 37 0.0411 = 180 / 4,376 ## 9 11 6 8 47 35 11 0 48 20 4271 0.0417 = 186 / 4,457 ## Totals 4467 4980 4562 4464 4369 4123 4435 4611 4394 4483 0.0304 = 1,364 / 44,888 Since we used a simple validation procedure to find the near-optimal parameter settings, we need to rerun the model with best parameters. here I perform a 5-fold CV model to get a more robust measure of our model’s performance with the identified parameter settings. We see our CV logloss and accuracy rate are 0.219 and 3.1% respectively. h2o_final &lt;- h2o.randomForest( x = predictors, y = response, training_frame = train_h2o, distribution = &quot;multinomial&quot;, ntrees = 500, mtries = 45, max_depth = 30, min_rows = 1, sample_rate = 0.8, seed = 123, nfolds = 5, keep_cross_validation_predictions = TRUE, stopping_metric = &quot;logloss&quot;, stopping_rounds = 10, stopping_tolerance = 0 ) h2o.performance(h2o_final, xval = TRUE) ## H2OMultinomialMetrics: drf ## ** Reported on cross-validation data. ** ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## Cross-Validation Set Metrics: ## ===================== ## ## MSE: (Extract with `h2o.mse`) 0.05791091 ## RMSE: (Extract with `h2o.rmse`) 0.2406469 ## Logloss: (Extract with `h2o.logloss`) 0.2188368 ## Mean Per-Class Error: 0.03141614 ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,xval = TRUE)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.968817 ## 2 2 0.989500 ## 3 3 0.994550 ## 4 4 0.996567 ## 5 5 0.997400 ## 6 6 0.997717 ## 7 7 0.997917 ## 8 8 0.997933 ## 9 9 0.997933 ## 10 10 1.000000 4.6.2.3 Feature interpretation In the regularized regression chapter, we saw that 67 predictors were not used because they contained zero variance and in our ranger model there were 102 variables not used for any splits. Similarly, for our h2o model we can assess how many variables have zero importance. Since h2o uses the impurity method for variable importance, this implies that these variables were not used for any splits. We see there are 23 variables that have zero importance. # how many variables not used for a split h2o.varimp(h2o_final) %&gt;% filter(percentage == 0) ## variable relative_importance scaled_importance percentage ## 1 V13 0 0 0 ## 2 V16 0 0 0 ## 3 V33 0 0 0 ## 4 V34 0 0 0 ## 5 V59 0 0 0 ## 6 V114 0 0 0 ## 7 V197 0 0 0 ## 8 V281 0 0 0 ## 9 V309 0 0 0 ## 10 V337 0 0 0 ## 11 V365 0 0 0 ## 12 V393 0 0 0 ## 13 V449 0 0 0 ## 14 V533 0 0 0 ## 15 V588 0 0 0 ## 16 V589 0 0 0 ## 17 V617 0 0 0 ## 18 V618 0 0 0 ## 19 V644 0 0 0 ## 20 V754 0 0 0 ## 21 V761 0 0 0 ## 22 V762 0 0 0 ## 23 V780 0 0 0 Alternatively, to find important variables we can extract the top 25 influential variables using vip. Figure 4.18 illustrates the top 25 influential variables in our h2o random forest model. We see many of the same variables that we saw with the ranger model but in slightly different order (i.e. V407, V379, V406, V435, v324). vip(h2o_final, num_features = 25, bar = FALSE) Figure 4.18: Top 25 most important variables based on impurity. 4.6.2.3.1 Feature effects Similar to the ranger section, next we assess the relationship between these influential predictors and the response variable with PDP plots. In this example, we assess the PDP of each response category with variable V407, which ranked first as the most influential variable. We can see those response categories where this variable has a large impact (stronger changes in the predicted value \\(\\hat y\\) as V407 changes) versus those that are less influnced by this predictor (mostly flat-lined plots). To produce a PDP with multi-classification problems, we need to create a custom prediction function that will return a data frame of the mean predicted probability for each response class. We supply this custom prediction function # custom prediction function custom_pred &lt;- function(object, newdata) { pred &lt;- predict(object, as.h2o(newdata))[[-1]] avg &lt;- purrr::map_df(as.data.frame(pred), mean) return(avg) } # partial dependence of V407 pd &lt;- partial(h2o_final, pred.var = &quot;V407&quot;, pred.fun = custom_pred, train = train) ggplot(pd, aes(V407, yhat, color = factor(yhat.id))) + geom_line(show.legend = FALSE) + facet_wrap(~ yhat.id, nrow = 2) Figure 4.19: Partial dependence plots of our most influential variable (V407) across the 10 response levels. This variable appears to be most influential in predicting the number 0, 1, 7, and 8. 4.6.2.4 Predicting Lastly, we can predict values for an unseen data set with predict or h2o.predict. Both options produce the same output, the predicted class and the predicted probability for each class. We also produce our test set performance results with h2o.performance. Comparing to our ranger model we see a similar overall error rate of just under 3%. Also, similar to our ranger model, h2o does a much better job predicting across all number relative to regularize regression; however, our random forest models still lack predictive accuracy for numbers 2, 7, 8, and 9 compared to when predicting the numbers 0, 1, and 6. Maybe we can improve upon this with more advanced algorithms 🤷. Note the Top-10 Hit Ratios. With our regularized regression h2o model, it took 4 “guesses” or tries until we achieved a 99% accuracy rate. With our random forest model, it only takes 2 tries for our model to accurately predict 99% on our test data! # predict with `predict` pred1 &lt;- predict(h2o_final, test_h2o) head(pred1) ## predict p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 ## 1 8 0.0000000 0.00000000 0.0000574597 0.02325448 0.00000000 0.01162724 0.00000000 0.000000e+00 0.9650608 0.00000000 ## 2 3 0.0000000 0.00000000 0.0002050103 0.87078918 0.03225145 0.01075048 0.00000000 2.150097e-02 0.0000000 0.06450290 ## 3 8 0.1711703 0.00000000 0.0270268942 0.08108068 0.00000000 0.09909861 0.07207172 4.912864e-06 0.5495468 0.00000000 ## 4 0 0.8247423 0.00000000 0.0103092784 0.00000000 0.00000000 0.04123711 0.11340206 0.000000e+00 0.0000000 0.01030928 ## 5 1 0.0000000 0.99974901 0.0001758430 0.00000000 0.00000000 0.00000000 0.00000000 7.514232e-05 0.0000000 0.00000000 ## 6 5 0.0000000 0.01030917 0.0000000000 0.00000000 0.01030917 0.81442428 0.02061834 1.069344e-05 0.1340192 0.01030917 # predict with `h2o.predict` pred2 &lt;- h2o.predict(h2o_final, test_h2o) head(pred2) ## predict p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 ## 1 8 0.0000000 0.00000000 0.0000574597 0.02325448 0.00000000 0.01162724 0.00000000 0.000000e+00 0.9650608 0.00000000 ## 2 3 0.0000000 0.00000000 0.0002050103 0.87078918 0.03225145 0.01075048 0.00000000 2.150097e-02 0.0000000 0.06450290 ## 3 8 0.1711703 0.00000000 0.0270268942 0.08108068 0.00000000 0.09909861 0.07207172 4.912864e-06 0.5495468 0.00000000 ## 4 0 0.8247423 0.00000000 0.0103092784 0.00000000 0.00000000 0.04123711 0.11340206 0.000000e+00 0.0000000 0.01030928 ## 5 1 0.0000000 0.99974901 0.0001758430 0.00000000 0.00000000 0.00000000 0.00000000 7.514232e-05 0.0000000 0.00000000 ## 6 5 0.0000000 0.01030917 0.0000000000 0.00000000 0.01030917 0.81442428 0.02061834 1.069344e-05 0.1340192 0.01030917 # assess performance h2o.performance(h2o_final, newdata = test_h2o) ## H2OMultinomialMetrics: drf ## ## Test Set Metrics: ## ===================== ## ## MSE: (Extract with `h2o.mse`) 0.05332193 ## RMSE: (Extract with `h2o.rmse`) 0.2309154 ## Logloss: (Extract with `h2o.logloss`) 0.2025038 ## Mean Per-Class Error: 0.02985019 ## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 971 0 0 0 0 1 3 1 3 1 0.0092 = 9 / 980 ## 1 0 1124 3 2 1 1 3 0 1 0 0.0097 = 11 / 1,135 ## 2 6 0 992 9 4 0 0 8 13 0 0.0388 = 40 / 1,032 ## 3 1 0 5 978 0 3 1 11 7 4 0.0317 = 32 / 1,010 ## 4 2 0 2 0 951 0 6 0 4 17 0.0316 = 31 / 982 ## 5 3 0 1 8 1 865 7 2 4 1 0.0303 = 27 / 892 ## 6 5 2 0 1 2 5 939 0 4 0 0.0198 = 19 / 958 ## 7 1 4 23 3 0 0 0 986 2 9 0.0409 = 42 / 1,028 ## 8 5 1 7 7 2 4 3 5 934 6 0.0411 = 40 / 974 ## 9 7 5 3 11 9 1 1 5 4 963 0.0456 = 46 / 1,009 ## Totals 1001 1136 1036 1019 970 880 963 1018 976 1001 0.0297 = 297 / 10,000 ## ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.970300 ## 2 2 0.990700 ## 3 3 0.994800 ## 4 4 0.996100 ## 5 5 0.997000 ## 6 6 0.997400 ## 7 7 0.997500 ## 8 8 0.997500 ## 9 9 0.997600 ## 10 10 1.000000 4.7 Learning More Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. Because of their more simplistic tuning nature and the fact that they require very little, if any, feature pre-processing they are often one of the first go-to algorithms when facing a predictive modeling problem. To learn more I would start with the following resources listed in order of complexity: Practical Machine Learning with H2O An Introduction to Statistical Learning Applied Predictive Modeling Computer Age Statistical Inference The Elements of Statistical Learning References "],
["gradient-boosting-machines.html", "Chapter 5 Gradient Boosting Machines 5.1 Package Requirements 5.2 Advantages &amp; Disadvantages 5.3 The Idea 5.4 Implementation: Regression 5.5 Implementation: Binary Classification 5.6 Implementation: Multinomial Classification 5.7 Learning More", " Chapter 5 Gradient Boosting Machines Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions. Whereas random forests (Chapter @ref(random_forest)) build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms. This chapter will cover the fundamentals to understanding and implementing GBMs. 5.1 Package Requirements This chapter leverages the following packages. Some of these packages play a supporting role; however, our focus is on demonstrating how to implement GBMs with the gbm (others 2017), xgboost (T. Chen et al. 2018), and h2o packages and discuss the pros and cons to each. library(rsample) # data splitting library(gbm) # original implementation of gbm library(xgboost) # a faster implementation of gbm library(h2o) # a java-based platform library(vip) # visualize feature importance library(pdp) # visualize feature effects library(ggplot2) # model visualization 5.2 Advantages &amp; Disadvantages Advantages: Often provides predictive accuracy that cannot be beat. Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible. No data pre-processing required - often works great with categorical and numerical values as is. Handles missing data - imputation not required. Disdvantages: GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize. Computationally expensive - GBMs often require many trees (&gt;1000) which can be time and memory exhaustive. The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning. Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, local variable importance, etc.). 5.3 The Idea Several supervised machine learning models are founded on a single predictive model such as linear regression, penalized models, naive Bayes, support vector machines. Alternatively, other approaches such as bagging and random forests are built on the idea of building an ensemble of models where each individual model predicts the outcome and then the ensemble simply averages the predicted values. The family of boosting methods is based on a different, constructive strategy of ensemble formation. The main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far. Figure 5.1: Sequential ensemble approach. Let’s discuss each component of the previous sentence in closer detail because they are important. Base-learning models: Boosting is a framework that iteratively improves any weak learning model. Many gradient boosting applications allow you to “plug in” various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner. Consequently, this chapter will discuss boosting in the context of decision trees. Training weak models: A weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used. Combining many weak models (versus strong ones) has a few benefits: Speed: Constructing weak models is computationally cheap. Accuracy improvement: Weak models allow the algorithm to learn slowly; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well. Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation). Sequential training with respect to errors: Boosted trees are grown sequentially; each tree is grown using information from previously grown trees. The basic algorithm for boosted regression trees can be generalized to the following where x represents our features and y represents our response: Fit a decision tree to the data: \\(F_1(x) = y\\), We then fit the next decision tree to the residuals of the previous: \\(h_1(x) = y - F_1(x)\\), Add this new tree to our algorithm: \\(F_2(x) = F_1(x) + h_1(x)\\), Fit the next decision tree to the residuals of \\(F_2\\): \\(h_2(x) = y - F_2(x)\\), Add this new tree to our algorithm: \\(F_3(x) = F_2(x) + h_1(x)\\), Continue this process until some mechanism (i.e. cross validation) tells us to stop. The basic algorithm for boosted decision trees can be generalized to the following where the final model is simply a stagewise additive model of b individual trees: \\[ f(x) = \\sum^B_{b=1}f^b(x) \\tag{1} \\] To illustrate the behavior, assume the following x and y observations. The blue sine wave represents the true underlying function and the points represent observations that include some irriducible error (noise). The boosted prediction illustrates the adjusted predictions after each additional sequential tree is added to the algorithm. Initially, there are large errors which the boosted algorithm improves upon immediately but as the predictions get closer to the true underlying function you see each additional tree make small improvements in different areas across the feature space where errors remain. Towards the end of the gif, the predicted values nearly converge to the true underlying function. Figure 5.2: Boosted regression tree predictions (courtesy of Brandon Greenwell) 5.3.1 Gradient descent Many algorithms, including decision trees, focus on minimizing the residuals and, therefore, emphasize the MSE loss function. The algorithm discussed in the previous section outlines the approach of sequentially fitting regression trees to minimize the errors. This specific approach is how gradient boosting minimizes the mean squared error (MSE) loss function. However, often we wish to focus on other loss functions such as mean absolute error (MAE) or to be able to apply the method to a classification problem with a loss function such as deviance. The name gradient boosting machine comes from the fact that this procedure can be generalized to loss functions other than MSE. Gradient boosting is considered a gradient descent algorithm. Gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are a downhill skier racing your friend. A good strategy to beat your friend to the bottom is to take the path with the steepest slope. This is exactly what gradient descent does - it measures the local gradient of the loss (cost) function for a given set of parameters (\\(\\Theta\\)) and takes steps in the direction of the descending gradient. Once the gradient is zero, we have reached the minimum. Figure 5.3: Gradient descent is the process of gradually decreasing the cost function (i.e. MSE) by tweaking parameters iteratively until you have reached a minimum. Image courtesy of Géron (2017). Gradient descent can be performed on any loss function that is differentiable. Consequently, this allows GBMs to optimize different loss functions as desired (see Friedman, Hastie, and Tibshirani (2001), p. 360 for common loss functions). An important parameter in gradient descent is the size of the steps which is determined by the learning rate. If the learning rate is too small, then the algorithm will take many iterations to find the minimum. On the other hand, if the learning rate is too high, you might jump across the minimum and end up further away than when you started. Figure 5.4: A learning rate that is too small will require many iterations to find the minimum. A learning rate too big may jump over the minimum. Image courtesy of Géron (2017). Moreover, not all cost functions are convex (bowl shaped). There may be local minimas, plateaus, and other irregular terrain of the loss function that makes finding the global minimum difficult. Stochastic gradient descent can help us address this problem by sampling a fraction of the training observations (typically without replacement) and growing the next tree using that subsample. This makes the algorithm faster but the stochastic nature of random sampling also adds some random nature in descending the loss function gradient. Although this randomness does not allow the algorithm to find the absolute global minimum, it can actually help the algorithm jump out of local minima and off plateaus and get near the global minimum. Figure 5.5: Stochastic gradient descent will often find a near-optimal solution by jumping out of local minimas and off plateaus. Image courtesy of Géron (2017). As we’ll see in the next section, there are several hyperparameter tuning options that allow us to address how we approach the gradient descent of our loss function. 5.3.2 Tuning Part of the beauty and challenges of GBMs is that they offer several tuning parameters. The beauty in this is GBMs are highly flexible. The challenge is that they can be time consuming to tune and find the optimal combination of hyperparamters. The most common hyperparameters that you will find in most GBM implementations include: Number of trees: The total number of trees to fit. GBMs often require many trees; however, unlike random forests GBMs can overfit so the goal is to find the optimal number of trees that minimize the loss function of interest with cross validation. Depth of trees: The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often \\(d = 1\\) works well, in which case each tree is a stump consisting of a single split. More commonly, d is greater than 1 but it is unlikely \\(d &gt; 10\\) will be required. Learning rate: Controls how quickly the algorithm proceeds down the gradient descent. Smaller values reduce the chance of overfitting but also increases the time to find the optimal fit. This is also called shrinkage. Subsampling: Controls whether or not you use a fraction of the available training observations. Using less than 100% of the training observations means you are implementing stochastic gradient descent. This can help to minimize overfitting and keep from getting stuck in a local minimum or plateau of the loss function gradient. Throughout this chapter you’ll be exposed to additional hyperparameters that are specific to certain packages and can improve performance and/or the efficiency of training and tuning models. 5.3.3 Package implementation There are many packages that implement GBMs and GBM variants. You can find a fairly comprehensive list here and at the CRAN Machine Learning Task View. However, the most popular implementations which we will cover in this post include: gbm: The gbm R package is an implementation of extensions to Freund and Schapire’s AdaBoost algorithm and Friedman’s gradient boosting machine. This is the original R implementation of GBM. A presentation is available here by Mark Landry. Features include6: Stochastic GBM. Supports up to 1024 factor levels. Supports Classification and regression trees. Includes methods for: least squares absolute loss t-distribution loss quantile regression logistic multinomial logistic Poisson Cox proportional hazards partial likelihood AdaBoost exponential loss Huberized hinge loss Learning to Rank measures (LambdaMart) Out-of-bag estimator for the optimal number of iterations is provided. Easy to overfit since early stopping functionality is not automated in thispackage. If internal cross-validation is used, this can be parallelized to all cores on the machine. Currently undergoing a major refactoring &amp; rewrite (and has been for sometime). GPL-2/3 License. xgboost: A fast and efficient gradient boosting framework with a C++ backend). Many resource are available here. The xgboost package is quite popular on Kaggle for data mining competitions. Features include: Stochastic GBM with column and row sampling (per split and per tree) for better generalization. Includes efficient linear model solver and tree learning algorithms. Parallel computation on a single machine. Supports various objective functions, including regression, classification and ranking. The package is made to be extensible, so that users are also allowed to define their own objectives easily. Apache 2.0 License. h2o: A powerful java-based interface that provides parallel distributed algorithms and efficient productionalization. Resources regarding h2o’s GBM implementation include a Tuning guide by Arno Candel](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd) and a Vignette. Features include: Distributed and parallelized computation on either a single node or a multi-node cluster. Automatic early stopping based on convergence of user-specified metrics to user-specied relative tolerance. Stochastic GBM with column and row sampling (per split and per tree) for better generalization. Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace). Grid search for hyperparameter optimization and model selection. Data-distributed, which means the entire dataset does not need to fit into memory on a single node, hence scales to any size training set. Uses histogram approximations of continuous variables for speedup. Uses dynamic binning - bin limits are reset at each tree level based on the split bins’ min and max values discovered during the last pass. Uses squared error to determine optimal splits. Distributed implementation details outlined in a blog post by Cliff Click. Unlimited factor levels. Multiclass trees (one for each class) built in parallel with each other. Apache 2.0 Licensed. Model export in plain Java code for deployment in production environments. 5.4 Implementation: Regression To illustrate various GBM concepts for a regression problem we will continue with the Ames, IA housing data, where our intent is to predict Sale_Price. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Tree-based methods tend to perform well on unprocessed data (i.e. without normalizing, centering, scaling features). In this chapter I focus on how to implement GBMs with various packages. Although I do not pre-process the data, realize that you can improve model performance by spending time processing variable attributes. 5.4.1 gbm 5.4.1.1 Basic implementation gbm has two primary training functions - gbm::gbm and gbm::gbm.fit. The primary difference is that gbm::gbm uses the formula interface to specify your model whereas gbm::gbm.fit requires the separated x and y matrices. When working with many variables it is more efficient to use the matrix rather than formula interface. The default settings in gbm includes a learning rate (shrinkage) of 0.001. This is a very small learning rate and typically requires a large number of trees to find the minimum MSE. However, gbm uses a default number of trees of 100, which is rarely sufficient. Consequently, I crank it up to 5,000 trees. The default depth of each tree (interaction.depth) is 1, which means we are ensembling a bunch of stumps. Lastly, I also include cv.folds to perform a 5 fold cross validation. The model took 48 seconds to run and the results show that our MSE loss function is minimized with 5,000 trees. # for reproducibility set.seed(123) # train GBM model gbm.fit &lt;- gbm( formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = ames_train, n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, cv.folds = 5, n.cores = NULL, # will use all cores by default verbose = FALSE ) # print results print(gbm.fit) ## gbm(formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = ames_train, ## n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, ## cv.folds = 5, verbose = FALSE, n.cores = NULL) ## A gradient boosted model with gaussian loss function. ## 5000 iterations were performed. ## The best cross-validation iteration was 5000. ## There were 80 predictors of which 27 had non-zero influence. The output object is a list containing several modelling and results information. We can access this information with regular indexing; I recommend you take some time to dig around in the object to get comfortable with its components. Here, we see that the minimum CV RMSE is $33,079.61 but the plot also illustrates that the CV error (MSE) is still decreasing at 5,000 trees. # get MSE and compute RMSE sqrt(min(gbm.fit$cv.error)) ## [1] 33079.61 # plot loss function as a result of n trees added to the ensemble gbm.perf(gbm.fit, method = &quot;cv&quot;) Figure 5.6: Training and cross-validated MSE as n trees are added to the GBM algorithm. In this case, the small learning rate is resulting in very small incremental improvements which means many trees are required. In fact, with the default learning rate and tree depth settings, the CV error is still reducing after 10,000 trees! 5.4.1.2 Tuning However, rarely do the default settings suffice. We could tune parameters one at a time to see how the results change. For example, here, I increase the learning rate to take larger steps down the gradient descent, reduce the number of trees (since we are reducing the learning rate), and increase the depth of each tree from using a single split to 3 splits. Our RMSE ($23,813.34) is lower than our initial model and the optimal number of trees required was 964. # for reproducibility set.seed(123) # train GBM model gbm.fit2 &lt;- gbm( formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = ames_train, n.trees = 5000, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5, n.cores = NULL, # will use all cores by default verbose = FALSE ) # find index for n trees with minimum CV error min_MSE &lt;- which.min(gbm.fit2$cv.error) # get MSE and compute RMSE sqrt(gbm.fit2$cv.error[min_MSE]) ## [1] 23813.34 # plot loss function as a result of n trees added to the ensemble gbm.perf(gbm.fit2, method = &quot;cv&quot;) Figure 5.7: Training and cross-validated MSE as n trees are added to the GBM algorithm. We can see that are new hyperparameter settings result in a much quicker progression down the gradient descent than our initial model. However, a better option than manually tweaking hyperparameters one at a time is to perform a grid search which iterates over every combination of hyperparameter values and allows us to assess which combination tends to perform well. To perform a manual grid search, first we want to construct our grid of hyperparameter combinations. We’re going to search across 81 models with varying learning rates and tree depth. I also vary the minimum number of observations allowed in the trees terminal nodes (n.minobsinnode) and introduce stochastic gradient descent by allowing bag.fraction &lt; 1. # create hyperparameter grid hyper_grid &lt;- expand.grid( shrinkage = c(.01, .1, .3), interaction.depth = c(1, 3, 5), n.minobsinnode = c(5, 10, 15), bag.fraction = c(.65, .8, 1), optimal_trees = 0, # a place to dump results min_RMSE = 0 # a place to dump results ) # total number of combinations nrow(hyper_grid) ## [1] 81 We loop through each hyperparameter combination and apply 5,000 trees. However, to speed up the tuning process, instead of performing 5-fold CV I train on 75% of the training observations and evaluate performance on the remaining 25%. When using train.fraction it will take the first XX% of the data so its important to randomize your rows in case their is any logic behind the ordering of the data (i.e. ordered by neighbhorhood). Our grid search revealed a few important attributes. First, our top model has better performance than our previously fitted model above and any of the other models covered in Chapters 3 and 4, with an RMSE of $20,390.55. Second, looking at the top 10 models we see that: all the top models used a learning rate of 0.1 or smaller; small incremental steps down the gradient descent appears to work best, all the top models used deeper trees (interaction.depth = 5); there are likely stome important interactions that the deeper trees are able to capture, most of the models with a learning rate of 0.01 used nearly all the trees meaning they had just enough trees to converge to their global minimum. Searching this entire grid took 36 minutes. # randomize data random_index &lt;- sample(1:nrow(ames_train), nrow(ames_train)) random_train &lt;- ames_train[random_index, ] # grid search for(i in 1:nrow(hyper_grid)) { # reproducibility set.seed(123) # train model gbm.tune &lt;- gbm( formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = random_train, n.trees = 5000, interaction.depth = hyper_grid$interaction.depth[i], shrinkage = hyper_grid$shrinkage[i], n.minobsinnode = hyper_grid$n.minobsinnode[i], bag.fraction = hyper_grid$bag.fraction[i], train.fraction = .75, n.cores = NULL, # will use all cores by default verbose = FALSE ) # add min training error and trees to grid hyper_grid$optimal_trees[i] &lt;- which.min(gbm.tune$valid.error) hyper_grid$min_RMSE[i] &lt;- sqrt(min(gbm.tune$valid.error)) } hyper_grid %&gt;% dplyr::arrange(min_RMSE) %&gt;% head(10) ## shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees min_RMSE ## 1 0.01 5 5 0.80 4911 20390.55 ## 2 0.01 5 5 0.65 4726 20588.02 ## 3 0.10 5 10 0.80 500 20758.72 ## 4 0.01 5 10 0.80 4897 20761.53 ## 5 0.01 5 5 1.00 5000 20997.68 ## 6 0.10 5 5 1.00 665 21277.84 ## 7 0.10 5 5 0.80 514 21277.90 ## 8 0.01 5 10 0.65 4987 21310.94 ## 9 0.01 5 15 0.80 4990 21456.17 ## 10 0.01 5 10 1.00 4960 21481.65 These results help us to zoom into areas where we can refine our search. In practice, tuning is an iterative process so you would likely refine this search grid to analyze a search space around the top models. For example, I would likely search the following values in my next grid search: learning rate: 0.1, 0.05, 0.01, 0.005 interaction depth: 3, 5, 7 along with the previously assessed values for n.minobsinnode and bag.fraction. Also, since we used nearly all 5000 trees when the learning rate was 0.01, I would increase this to ensure there are enough trees for learning rate \\(=0.005\\). Once we have found our top model we train a model with those specific parameters. I’ll use the top model in our grid search and since the model converged at 4911 trees I train a model with that many trees. # for reproducibility set.seed(123) # train GBM model gbm.fit.final &lt;- gbm( formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = ames_train, n.trees = 4342, interaction.depth = 5, shrinkage = 0.01, n.minobsinnode = 5, bag.fraction = .80, train.fraction = 1, n.cores = NULL, # will use all cores by default verbose = FALSE ) 5.4.1.3 Feature interpretation Similar to random forests, GBMs make no assumption regarding the linearity and monoticity of the predictor-response relationship. So as we did in the random forest chapter (Chapter 4) we can understand the relationship between the features and the response using variable importance plots and partial dependence plots. Additional model interpretability approaches will be discussed in the Model Interpretability chapter. 5.4.1.3.1 Feature importance After re-running our final model we likely want to understand the variables that have the largest influence on our response variable. The summary method for gbm will output a data frame and a plot that shows the most influential variables. cBars allows you to adjust the number of variables to show (in order of influence). The default method for computing variable importance is with relative influence but your options include: method = relative.influence: At each split in each tree, gbm computes the improvement in the split-criterion (MSE for regression). gbm then averages the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important. method = permutation.test.gbm: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly “shaking up” of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important. par(mfrow = c(1, 2), mar = c(5, 10, 1, 1)) # relative influence approach summary(gbm.fit.final, cBars = 10, method = relative.influence, las = 2) # permutation approach summary(gbm.fit.final, cBars = 10, method = permutation.test.gbm, las = 2) Figure 5.8: Top 10 influential variables using the relative influence (left) and permutation (right) approach. We can see common themes among the top variables although in differing order. 5.4.1.3.2 Feature effects After the most relevant variables have been identified, we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves to better understand the relationship between the predictors and response. Here we plot two of the most influential variables (Gr_Liv_Area and Overall_Qual). We see that both predictor non-linear relationships with the sale price. As in Chapter (???)(random-forest), you can produce ICE curves by incorporating ice = TRUE and center = TRUE (for centered ICE curves). p1 &lt;- gbm.fit.final %&gt;% partial( pred.var = &quot;Gr_Liv_Area&quot;, n.trees = gbm.fit.final$n.trees, grid.resolution = 50 ) %&gt;% autoplot(rug = TRUE, train = ames_train) p2 &lt;- gbm.fit.final %&gt;% partial( pred.var = &quot;Overall_Qual&quot;, n.trees = gbm.fit.final$n.trees, train = data.frame(ames_train) ) %&gt;% autoplot() gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 5.9: The mean predicted sale price as Gr_Liv_Area and Overall_Qual change in value. 5.4.1.4 Predicting Once you’ve found your optimal model, predicting new observations with the gbm model follows the same procedure as most R models. We simply use the predict function; however, we also need to supply the number of trees to use (see ?predict.gbm for details). We see that our RMSE for our test set is right in line with the optimal RMSE obtained during our grid search and far better than any model to-date. # predict values for test data pred &lt;- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, ames_test) # results caret::RMSE(pred, ames_test$Sale_Price) ## [1] 20859.01 5.4.2 xgboost The xgboost package only works with matrices that contain all numeric variables; consequently, we need to one hot encode our data. Throughout this book we’ve illustrated different ways to do this in R (i.e. Matrix::sparse.model.matrix, caret::dummyVars) but here we will use the vtreat package. vtreat is a robust package for data prep and helps to eliminate problems caused by missing values, novel categorical levels that appear in future data sets that were not in the training data, etc. However, vtreat is not very intuitive. I will not explain the functionalities but you can find more information here, here, and here. The following applies vtreat to one-hot encode the training and testing data sets. # variable names features &lt;- setdiff(names(ames_train), &quot;Sale_Price&quot;) # Create the treatment plan from the training data treatplan &lt;- vtreat::designTreatmentsZ(ames_train, features, verbose = FALSE) # Get the &quot;clean&quot; variable names from the scoreFrame new_vars &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% dplyr::filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% magrittr::use_series(varName) # Prepare the training data features_train &lt;- vtreat::prepare(treatplan, ames_train, varRestriction = new_vars) %&gt;% as.matrix() response_train &lt;- ames_train$Sale_Price # Prepare the test data features_test &lt;- vtreat::prepare(treatplan, ames_test, varRestriction = new_vars) %&gt;% as.matrix() response_test &lt;- ames_test$Sale_Price # dimensions of one-hot encoded data dim(features_train) ## [1] 2054 211 dim(features_test) ## [1] 876 211 5.4.2.1 Basic implementation xgboost provides different training functions (i.e. xgb.train which is just a wrapper for xgboost). However, to train an xgboost model we typically want to use xgb.cv, which incorporates cross-validation. The following trains a basic 5-fold cross validated XGBoost model with 1,000 trees. There are many parameters available in xgb.cv but the ones you have become more familiar with in this chapter include the following default values: learning rate (eta): 0.3 tree depth (max_depth): 6 minimum node size (min_child_weight): 1 percent of training data to sample for each tree (subsample –&gt; equivalent to gbm’s bag.fraction): 100% This model took nearly 2 minutes to run. The reason xgboost seems slower than gbm is since we one-hot encoded our data, xgboost is searching across 211 features where gbm uses non-one-hot encoded which means it was only searching across 80 features. # reproducibility set.seed(123) xgb.fit1 &lt;- xgb.cv( data = features_train, label = response_train, nrounds = 1000, nfold = 5, objective = &quot;reg:linear&quot;, # for regression models verbose = 0 # silent, ) The xgb.fit1 object contains lots of good information. In particular we can assess the xgb.fit1$evaluation_log to identify the minimum RMSE and the optimal number of trees for both the training data and the cross-validated error. We can see that the training error continues to decreasing through 980 trees where the RMSE nearly reaches 0; however, the cross validated error reaches a minimum RMSE of $26,758.30 with only 98 trees. # get number of trees that minimize error xgb.fit1$evaluation_log %&gt;% dplyr::summarise( ntrees.train = which.min(train_rmse_mean), rmse.train = min(train_rmse_mean), ntrees.test = which.min(test_rmse_mean), rmse.test = min(test_rmse_mean), ) ## ntrees.train rmse.train ntrees.test rmse.test ## 1 980 0.05009 98 26758.3 # plot error vs number trees xgb.fit1$evaluation_log %&gt;% tidyr::gather(error, RMSE, train_rmse_mean, test_rmse_mean) %&gt;% ggplot(aes(iter, RMSE, color = error)) + geom_line() Figure 5.10: Training (blue) and cross-validation (red) error for each additional tree added to the GBM algorithm. The CV error is quickly minimized with 98 trees while the training error reduces to near zero over 980 trees. A nice feature provided by xgb.cv is early stopping. This allows us to tell the function to stop running if the cross validated error does not improve for n continuous trees. For example, the above model could be re-run with the following where we tell it stop if we see no improvement for 10 consecutive trees. This feature will help us speed up the tuning process in the next section. This reduced our training time from 2 minutes to 8 seconds! # reproducibility set.seed(123) xgb.fit2 &lt;- xgb.cv( data = features_train, label = response_train, nrounds = 1000, nfold = 5, objective = &quot;reg:linear&quot;, # for regression models verbose = 0, # silent, early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees ) # plot error vs number trees xgb.fit2$evaluation_log %&gt;% tidyr::gather(error, RMSE, train_rmse_mean, test_rmse_mean) %&gt;% ggplot(aes(iter, RMSE, color = error)) + geom_line() Figure 5.11: Early stopping allows us to stop training once we experience no additional improvement on our cross-validated error. 5.4.2.2 Tuning To tune the XGBoost model we pass parameters as a list object to the params argument. The most common parameters include: eta:controls the learning rate max_depth: tree depth min_child_weight: minimum number of observations required in each terminal node subsample: percent of training data to sample for each tree colsample_bytrees: percent of columns to sample from for each tree For example, if we wanted to specify specific values for these parameters we would extend the above model with the following parameters. # create parameter list params &lt;- list( eta = .1, max_depth = 5, min_child_weight = 2, subsample = .8, colsample_bytree = .9 ) # reproducibility set.seed(123) # train model xgb.fit3 &lt;- xgb.cv( params = params, data = features_train, label = response_train, nrounds = 1000, nfold = 5, objective = &quot;reg:linear&quot;, # for regression models verbose = 0, # silent, early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees ) # assess results xgb.fit3$evaluation_log %&gt;% dplyr::summarise( ntrees.train = which.min(train_rmse_mean), rmse.train = min(train_rmse_mean), ntrees.test = which.min(test_rmse_mean), rmse.test = min(test_rmse_mean), ) ## ntrees.train rmse.train ntrees.test rmse.test ## 1 122 7954.668 112 24547.28 To perform a large search grid, we can follow the same procedure we did with gbm. We create our hyperparameter search grid along with columns to dump our results in. Here, I create a pretty large search grid consisting of 108 different hyperparameter combinations to model. # create hyperparameter grid hyper_grid &lt;- expand.grid( eta = c(.05, .1, .15), max_depth = c(3, 5, 7), min_child_weight = c(5, 10, 15), subsample = c(.65, .8), colsample_bytree = c(.9, 1), optimal_trees = 0, # a place to dump results min_RMSE = 0 # a place to dump results ) nrow(hyper_grid) ## [1] 108 Now I apply the same for loop procedure to loop through and apply an xgboost model for each hyperparameter combination and dump the results in the hyper_grid data frame. Our minimum RMSE ($23,316.40) is a little higher than the gbm model, likely a result of one-hot encoding our data and how the models treat these dummy coded variables differently. This full search grid took 34 minutes to run! # grid search for(i in 1:nrow(hyper_grid)) { # create parameter list params &lt;- list( eta = hyper_grid$eta[i], max_depth = hyper_grid$max_depth[i], min_child_weight = hyper_grid$min_child_weight[i], subsample = hyper_grid$subsample[i], colsample_bytree = hyper_grid$colsample_bytree[i] ) # reproducibility set.seed(123) # train model xgb.tune &lt;- xgb.cv( params = params, data = features_train, label = response_train, nrounds = 5000, nfold = 5, objective = &quot;reg:linear&quot;, # for regression models verbose = 0, # silent, early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees ) # add min training error and trees to grid hyper_grid$optimal_trees[i] &lt;- which.min(xgb.tune$evaluation_log$test_rmse_mean) hyper_grid$min_RMSE[i] &lt;- min(xgb.tune$evaluation_log$test_rmse_mean) } hyper_grid %&gt;% dplyr::arrange(min_RMSE) %&gt;% head(10) ## eta max_depth min_child_weight subsample colsample_bytree optimal_trees min_RMSE ## 1 0.05 7 5 0.65 1.0 481 23316.40 ## 2 0.05 5 5 0.65 1.0 355 23515.42 ## 3 0.05 5 5 0.80 1.0 469 23856.31 ## 4 0.05 5 5 0.65 0.9 312 23888.34 ## 5 0.05 7 5 0.80 1.0 660 23904.30 ## 6 0.15 3 5 0.65 1.0 227 23909.78 ## 7 0.05 3 5 0.80 1.0 567 23967.56 ## 8 0.05 3 10 0.80 1.0 665 23998.05 ## 9 0.15 3 5 0.65 0.9 230 24001.86 ## 10 0.05 5 10 0.80 1.0 624 24033.78 After assessing the results you would likely perform a few more grid searches to hone in on the parameters that appear to influence the model the most. In fact, here is a link to a great blog post that discusses a strategic approach to tuning with xgboost. However, for brevity, we’ll just assume the top model in the above search is the globally optimal model. Once you’ve found the optimal model, we can fit our final model with xgb.train or xgboost. # parameter list params &lt;- list( eta = 0.05, max_depth = 7, min_child_weight = 5, subsample = 0.65, colsample_bytree = 1 ) # train final model xgb.fit.final &lt;- xgboost( params = params, data = features_train, label = response_train, nrounds = 481, objective = &quot;reg:linear&quot;, verbose = 0 ) 5.4.2.3 Feature interpretation 5.4.2.3.1 Feature importance xgboost provides built-in variable importance plotting. First, you need to create the importance matrix with xgb.importance and then feed this matrix into xgb.plot.importance (or xgb.ggplot.importance for a ggplot output). xgboost provides 3 variable importance measures: Gain: the relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model. This is synonymous with gbm’s relative.influence. Cover: the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose \\(feature_1\\) is used to decide the leaf node for 10, 5, and 2 observations in \\(tree_1\\), \\(tree_2\\) and \\(tree_3\\) respectively; then the metric will count cover for this feature as \\(10+5+2 = 17\\) observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features’ cover metrics. Frequency: the percentage representing the relative number of times a particular feature occurs in the trees of the model. In the above example, if \\(feature_1\\) occurred in 2 splits, 1 split and 3 splits in each of \\(tree_1\\), \\(tree_2\\) and \\(tree_3\\); then the weightage for \\(feature_1\\) will be \\(2+1+3 = 6\\). The frequency for \\(feature_1\\) is calculated as its percentage weight over weights of all features. The xgb.ggplot.importance plotting mechanism will also perform a cluster analysis on the features based on their importance scores. This becomes more useful when visualizing many features (i.e. 50) and you want to categorize them based on their importance. # create importance matrix importance_matrix &lt;- xgb.importance(model = xgb.fit.final) # variable importance plot p1 &lt;- xgb.ggplot.importance(importance_matrix, top_n = 10, measure = &quot;Gain&quot;) + ggtitle(&quot;Gain&quot;) + theme(legend.position=&quot;bottom&quot;) p2 &lt;- xgb.ggplot.importance(importance_matrix, top_n = 10, measure = &quot;Cover&quot;) + ggtitle(&quot;Cover&quot;) + theme(legend.position=&quot;bottom&quot;) p3 &lt;- xgb.ggplot.importance(importance_matrix, top_n = 10, measure = &quot;Frequency&quot;) + ggtitle(&quot;Frequency&quot;) + theme(legend.position=&quot;bottom&quot;) gridExtra::grid.arrange(p1, p2, p3, ncol = 1) Figure 5.12: Top 25 influential variables for our final xgboost model based on the three different variable importance metrics. 5.4.2.3.2 Feature effects PDP and ICE plots work similarly to how we implemented them with gbm. The only difference is you need to incorporate the training data within the partial function since these data cannot be extracted directly from the model object. We see a similar non-linear relationship between Gr_Liv_Area and predicted sale price as we did with gbm and in the random forest models; however, note the unique dip right after Gr_liv_Area reaches 3,000 square feet. We saw this dip in the gbm model; however, it is a pattern that was not picked up on by the random forests models. You do not need to supply the number of trees with n.trees = xgb.fit.final$niter; however, when supplying a cross-validated model where the optimal number of trees may be less than the total number of trees ran, then you will want to supply the optimal number of trees to the n.trees paramater. pdp &lt;- xgb.fit.final %&gt;% partial( pred.var = &quot;Gr_Liv_Area_clean&quot;, n.trees = xgb.fit.final$niter, grid.resolution = 50, train = features_train ) %&gt;% autoplot(rug = TRUE, train = features_train) + ggtitle(&quot;PDP&quot;) ice &lt;- xgb.fit.final %&gt;% partial( pred.var = &quot;Gr_Liv_Area_clean&quot;, n.trees = xgb.fit.final$niter, grid.resolution = 100, train = features_train, ice = TRUE ) %&gt;% autoplot(rug = TRUE, train = features_train, alpha = .05, center = TRUE) + ggtitle(&quot;ICE&quot;) gridExtra::grid.arrange(pdp, ice, nrow = 1) Figure 5.13: The mean predicted sale price as the above ground living area increases. 5.4.2.4 Predicting Lastly, we use predict to predict on new observations; however, unlike gbm we do not need to provide the number of trees. # predict values for test data pred &lt;- predict(xgb.fit.final, features_test) # test set results caret::RMSE(pred, response_test) ## [1] 23454.51 5.4.3 h2o Lets go ahead and start up h2o: h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpa5wE8r/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpa5wE8r/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 476 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 18 days ## H2O cluster name: H2O_started_from_R_bradboehmke_qdv114 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) 5.4.3.1 Basic implementation h2o.gbm allows us to perform a GBM with H2O. However, prior to running our initial model we need to convert our training data to an h2o object. By default, h2o.gbm applies a GBM model with the following parameters: number of trees (ntrees): 50 learning rate (learn_rate): 0.1 tree depth (max_depth): 5 minimum observations in a terminal node (min_rows): 10 no sampling of observations or columns Since we are performing a 5-fold cross-validation, the output reports results for both our training set (\\(RMSE = 12539.86\\)) and validation set (\\(RMSE = 24654.047\\)). # create feature names y &lt;- &quot;Sale_Price&quot; x &lt;- setdiff(names(ames_train), y) # turn training set into h2o object train.h2o &lt;- as.h2o(ames_train) # training basic GBM model with defaults h2o.fit1 &lt;- h2o.gbm( x = x, y = y, training_frame = train.h2o, nfolds = 5 # performs 5 fold cross validation ) # assess model results h2o.fit1 ## Model Details: ## ============== ## ## H2ORegressionModel: gbm ## Model ID: GBM_model_R_1533927247702_1 ## Model Summary: ## number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves ## 1 50 50 17591 5 5 5.00000 9 31 22.96000 ## ## ## H2ORegressionMetrics: gbm ## ** Reported on training data. ** ## ## MSE: 157248046 ## RMSE: 12539.86 ## MAE: 8988.278 ## RMSLE: 0.08190755 ## Mean Residual Deviance : 157248046 ## ## ## ## H2ORegressionMetrics: gbm ## ** Reported on cross-validation data. ** ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 612660925 ## RMSE: 24751.99 ## MAE: 15485.9 ## RMSLE: 0.1369165 ## Mean Residual Deviance : 612660925 ## ## ## Cross-Validation Metrics Summary: ## mean sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_valid ## mae 15465.6455 403.8833 15899.954 14460.245 16051.8125 15667.94 15248.277 ## mean_residual_deviance 6.1167661E8 6.4985276E7 6.851456E8 4.3496512E8 6.0995507E8 6.7103411E8 6.5728301E8 ## mse 6.1167661E8 6.4985276E7 6.851456E8 4.3496512E8 6.0995507E8 6.7103411E8 6.5728301E8 ## r2 0.9020621 0.008838167 0.8946199 0.9245291 0.89047533 0.8939862 0.90669996 ## residual_deviance 6.1167661E8 6.4985276E7 6.851456E8 4.3496512E8 6.0995507E8 6.7103411E8 6.5728301E8 ## rmse 24654.047 1388.2732 26175.287 20855.818 24697.27 25904.326 25637.531 ## rmsle 0.13682812 0.009084022 0.15625001 0.12442712 0.12835869 0.12703055 0.14807428 Similar to xgboost, we can incorporate automated stopping so that we can crank up the number of trees but terminate training once model improvement decreases or stops. There is also an option to terminate training after so much time has passed (see max_runtime_secs). In this example, I train a default model with 5,000 trees but stop training after 10 consecutive trees have no improvement on the cross-validated error. In this case, training stops after 2623 trees and has a cross-validated RMSE of $23,441.46. # training basic GBM model with defaults h2o.fit2 &lt;- h2o.gbm( x = x, y = y, training_frame = train.h2o, nfolds = 5, ntrees = 5000, stopping_rounds = 10, stopping_tolerance = 0, seed = 123 ) # model stopped after xx trees h2o.fit2@parameters$ntrees ## [1] 2623 # cross validated RMSE h2o.rmse(h2o.fit2, xval = TRUE) ## [1] 23441.46 5.4.3.2 Tuning H2O provides many parameters that can be adjusted. It is well worth your time to check out the available documentation at H2O.ai. For this chapter, we’ll focus on the more common hyperparameters that are adjusted. This includes: Tree complexity: ntrees: number of trees to train max_depth: depth of each tree min_rows: Fewest observations allowed in a terminal node Learning rate: learn_rate: rate to descend the loss function gradient learn_rate_annealing: allows you to have a high initial learn_rate, then gradually reduce as trees are added (speeds up training). Adding stochastic nature: sample_rate: row sample rate per tree col_sample_rate: column sample rate per tree (synonymous with xgboost’s colsample_bytree) Note that there are parameters that control how categorical and continuous variables are encoded, binned, and split. The defaults tend to perform quite well but we have been able to gain small improvements in certain circumstances by adjusting these. We will not cover them but they are worth reviewing. To perform grid search tuning with H2O we can perform either a full or random discrete grid search as discussed in Section 4.4.2.2. 5.4.3.2.1 Full grid search We’ll start with a full grid search. However, to speed up training with h2o I’ll use a validation set rather than perform k-fold cross validation. The following creates a hyperparameter grid consisting of 216 hyperparameter combinations. We apply h2o.grid to perform a grid search while also incorporating stopping parameters to reduce training time. We see that the worst model had an RMSE of $34,142.31 (\\(\\sqrt{476228672}\\)) and the best had an RMSE of $21,822.66 (\\(\\sqrt{1165697554}\\)). A few characteristics pop out when we assess the results - models that search across a sample of columns, include more interactions via deeper trees, and allow nodes with single observations tend to perform best. This full grid search took 18 minutes. # create training &amp; validation sets split &lt;- h2o.splitFrame(train.h2o, ratios = 0.75) train &lt;- split[[1]] valid &lt;- split[[2]] # create hyperparameter grid hyper_grid &lt;- list( max_depth = c(1, 3, 5), min_rows = c(1, 5, 10), learn_rate = c(0.05, 0.1, 0.15), learn_rate_annealing = c(.99, 1), sample_rate = c(.75, 1), col_sample_rate = c(.9, 1) ) # perform grid search grid &lt;- h2o.grid( algorithm = &quot;gbm&quot;, grid_id = &quot;gbm_grid1&quot;, x = x, y = y, training_frame = train, validation_frame = valid, hyper_params = hyper_grid, ntrees = 5000, stopping_rounds = 10, stopping_tolerance = 0, seed = 123 ) # collect the results and sort by our model performance metric of choice grid_perf &lt;- h2o.getGrid( grid_id = &quot;gbm_grid1&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE ) grid_perf ## H2O Grid Details ## ================ ## ## Grid ID: gbm_grid1 ## Used hyper parameters: ## - col_sample_rate ## - learn_rate ## - learn_rate_annealing ## - max_depth ## - min_rows ## - sample_rate ## Number of models: 216 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate model_ids mse ## 1 0.9 0.05 1.0 5 1.0 1.0 gbm_grid1_model_138 4.762286724006572E8 ## 2 0.9 0.05 1.0 5 1.0 0.75 gbm_grid1_model_30 4.9950455156501746E8 ## 3 0.9 0.1 0.99 5 1.0 1.0 gbm_grid1_model_134 5.04407597702603E8 ## 4 0.9 0.15 1.0 5 1.0 1.0 gbm_grid1_model_142 5.1283047945094573E8 ## 5 0.9 0.05 0.99 5 1.0 1.0 gbm_grid1_model_132 5.1307290689450026E8 ## ## --- ## col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate model_ids mse ## 211 1.0 0.05 0.99 1 5.0 0.75 gbm_grid1_model_37 1.1602102219725711E9 ## 212 1.0 0.05 0.99 1 10.0 0.75 gbm_grid1_model_73 1.1620871092432442E9 ## 213 0.9 0.05 0.99 1 5.0 1.0 gbm_grid1_model_144 1.1631655136872623E9 ## 214 1.0 0.05 0.99 1 5.0 1.0 gbm_grid1_model_145 1.164083893251335E9 ## 215 0.9 0.05 0.99 1 10.0 1.0 gbm_grid1_model_180 1.1653298042199028E9 ## 216 1.0 0.05 0.99 1 10.0 1.0 gbm_grid1_model_181 1.1656975535614166E9 We can check out more details of the best performing model. We can see that our best model used all 5000 trees, thus a future grid search may want to increase the number of trees. # Grab the model_id for the top model, chosen by validation error best_model_id &lt;- grid_perf@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) best_model@parameters$ntrees ## [1] 5000 # Now let’s get performance metrics on the best model h2o.performance(model = best_model, valid = TRUE) ## H2ORegressionMetrics: gbm ## ** Reported on validation data. ** ## ## MSE: 476228672 ## RMSE: 21822.66 ## MAE: 13972.88 ## RMSLE: 0.1341252 ## Mean Residual Deviance : 476228672 5.4.3.2.2 Random discrete grid search As discussed in Section 4.4.2.2.2, h2o also allows us to perform a random grid search that allows early stopping. We can build onto the previous results and perform a random discrete grid. This time I increase the max_depth, refine the min_rows, and allow for 80% col_sample_rate. I keep all hyperparameter search criteria the same. I also add a search criteria that stops the grid search if none of the last 10 models have managed to have a 0.5% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 1800 seconds (30 minutes). In this example, our search went for the entire 90 minutes and evaluated 57 of the 216 potential models. In the body of the grid search, notice that I increased the trees to 10,000 since our best model used all 5,000 but I also include a stopping mechanism so that the model quits adding trees once no improvement is made in the validation RMSE. This random grid search took 30 minutes. It only assessed a third of the number of models the full grid search did but keep in mind that this grid search was assessing models with very low learning rates, which can take a long time. # refined hyperparameter grid hyper_grid &lt;- list( max_depth = c(5, 7, 9), min_rows = c(1, 3, 5), learn_rate = c(0.05, 0.1, 0.15), learn_rate_annealing = c(.99, 1), sample_rate = c(.75, 1), col_sample_rate = c(.8, .9) ) # random grid search criteria search_criteria &lt;- list( strategy = &quot;RandomDiscrete&quot;, stopping_metric = &quot;mse&quot;, stopping_tolerance = 0.005, stopping_rounds = 10, max_runtime_secs = 60*30 ) # perform grid search grid &lt;- h2o.grid( algorithm = &quot;gbm&quot;, grid_id = &quot;gbm_grid2&quot;, x = x, y = y, training_frame = train, validation_frame = valid, hyper_params = hyper_grid, search_criteria = search_criteria, # add search criteria ntrees = 10000, stopping_rounds = 10, stopping_tolerance = 0, seed = 123 ) # collect the results and sort by our model performance metric of choice grid_perf &lt;- h2o.getGrid( grid_id = &quot;gbm_grid2&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE ) grid_perf ## H2O Grid Details ## ================ ## ## Grid ID: gbm_grid2 ## Used hyper parameters: ## - col_sample_rate ## - learn_rate ## - learn_rate_annealing ## - max_depth ## - min_rows ## - sample_rate ## Number of models: 68 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate model_ids mse ## 1 0.9 0.05 1.0 5 1.0 1.0 gbm_grid2_model_50 4.7624213765845805E8 ## 2 0.8 0.1 0.99 5 5.0 0.75 gbm_grid2_model_40 4.9473498408073145E8 ## 3 0.8 0.1 0.99 5 1.0 0.75 gbm_grid2_model_34 4.989151160193848E8 ## 4 0.9 0.05 1.0 5 1.0 0.75 gbm_grid2_model_2 4.99534198008744E8 ## 5 0.9 0.1 0.99 7 1.0 1.0 gbm_grid2_model_46 5.1068183500740755E8 ## ## --- ## col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate model_ids mse ## 63 0.9 0.1 0.99 9 1.0 1.0 gbm_grid2_model_19 6.888828473137908E8 ## 64 0.9 0.05 1.0 9 5.0 1.0 gbm_grid2_model_66 6.978688028588266E8 ## 65 0.9 0.05 0.99 9 3.0 1.0 gbm_grid2_model_53 7.120703549234108E8 ## 66 0.8 0.15 1.0 9 1.0 0.75 gbm_grid2_model_64 7.175292869435712E8 ## 67 0.9 0.1 0.99 9 3.0 1.0 gbm_grid2_model_3 7.793274661905156E8 ## 68 0.8 0.15 1.0 7 5.0 0.75 gbm_grid2_model_67 8.279874389570463E8 In this example, the best model obtained a cross-validated RMSE of $21,822.97. So although we assessed only 31% of the total models we were able to find a model that was better than our initial full grid search. # Grab the model_id for the top model, chosen by validation error best_model_id &lt;- grid_perf@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) # Now let’s get performance metrics on the best model h2o.performance(model = best_model, valid = TRUE) ## H2ORegressionMetrics: gbm ## ** Reported on validation data. ** ## ## MSE: 476242138 ## RMSE: 21822.97 ## MAE: 13972.9 ## RMSLE: 0.1341249 ## Mean Residual Deviance : 476242138 Once we’ve found our preferred model, we’ll go ahead and retrain a new model with the full training data. I’ll use the best model from the full grid search and perform a 5-fold CV to get a robust estimate of the expected error. I crank up the number of trees just to make sure we find a global minimum. # train final model h2o.final &lt;- h2o.gbm( x = x, y = y, training_frame = train.h2o, nfolds = 5, ntrees = 10000, learn_rate = 0.05, learn_rate_annealing = 1, max_depth = 5, min_rows = 5, sample_rate = 1, col_sample_rate = 0.9, stopping_rounds = 10, stopping_tolerance = 0, seed = 123 ) # model stopped after xx trees h2o.final@parameters$ntrees ## [1] 5526 # cross validated RMSE h2o.rmse(h2o.final, xval = TRUE) ## [1] 23553.76 5.4.3.3 Feature interpretation 5.4.3.3.1 Feature importance Looking at the top 25 most important features, we see many of the same predictors as we have with the other GBM implementations and also with the random forest approaches. vip::vip(h2o.final, num_features = 25, bar = FALSE) Figure 5.14: Top 25 most influential predictors based on impurity. 5.4.3.3.2 Feature effects PDP and ICE plots work similarly to how we implemented them with the h2o approaches for random forests. Figure 5.15 illustrates the same non-linear relationship between Gr_Liv_Area and predicted sale price that we have been seeing with other GBM and random forest implementations. # build custom prediction function pfun &lt;- function(object, newdata) { as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]] } # compute ICE curves partial( h2o.final, pred.var = &quot;Gr_Liv_Area&quot;, train = ames_train, pred.fun = pfun, grid.resolution = 20 ) %&gt;% autoplot(rug = TRUE, train = ames_train, alpha = 0.05, center = TRUE) + ggtitle(&quot;Centered ICE curves&quot;) Figure 5.15: ICE curves illustrating the non-linear relationship between above ground square footage (Gr_Liv_Area) and predicted sale price. 5.4.3.4 Predicting Lastly, we use h2o.predict or predict to predict on new observations and we can also evaluate the performance of our model on our test set easily with h2o.performance. Results are quite similar to both gmb and xgboost. # convert test set to h2o object test.h2o &lt;- as.h2o(ames_test) # evaluate performance on new data h2o.performance(model = h2o.final, newdata = test.h2o) ## H2ORegressionMetrics: gbm ## ## MSE: 501872966 ## RMSE: 22402.52 ## MAE: 13844.76 ## RMSLE: 0.1127162 ## Mean Residual Deviance : 501872966 # predict with h2o.predict h2o.predict(h2o.final, newdata = test.h2o) ## predict ## 1 122831.1 ## 2 179027.7 ## 3 228672.4 ## 4 261521.0 ## 5 407383.8 ## 6 384198.8 ## ## [876 rows x 1 column] # predict values with predict predict(h2o.final, test.h2o) ## predict ## 1 122831.1 ## 2 179027.7 ## 3 228672.4 ## 4 261521.0 ## 5 407383.8 ## 6 384198.8 ## ## [876 rows x 1 column] # shut down h2o h2o.shutdown(prompt = FALSE) ## [1] TRUE 5.5 Implementation: Binary Classification 5.5.1 gbm Coming soon! 5.5.2 xgboost Coming soon! 5.5.3 h2o Coming soon! 5.6 Implementation: Multinomial Classification 5.6.1 gbm Coming soon! 5.6.2 xgboost Coming soon! 5.6.3 h2o Coming soon! 5.7 Learning More GBMs are one of the most powerful ensemble algorithms that are often first-in-class with predictive accuracy. Although they are less intuitive and more computationally demanding than many other machine learning algorithms, they are essential to have in your toolbox. To learn more I would start with the following resources: Traditional book resources: An Introduction to Statistical Learning Applied Predictive Modeling Computer Age Statistical Inference The Elements of Statistical Learning Alternative online resources: Trevor Hastie - Gradient Boosting &amp; Random Forests at H2O World 2014 (YouTube) Trevor Hastie - Data Science of GBM (2013) (slides) Mark Landry - Gradient Boosting Method and Random Forest at H2O World 2015 (YouTube) Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn at PyData London 2014 (YouTube) Alexey Natekin1 and Alois Knoll - Gradient boosting machines, a tutorial (blog post) References "],
["references.html", "References", " References "]
]
