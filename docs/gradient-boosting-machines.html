<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Hands-on Machine Learning with R</title>
  <meta name="description" content="A Machine Learning Algorithmic Deep Dive Using R.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Hands-on Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A Machine Learning Algorithmic Deep Dive Using R." />
  <meta name="github-repo" content="bradleyboehmke/hands-on-machine-learning-with-r" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Hands-on Machine Learning with R" />
  
  <meta name="twitter:description" content="A Machine Learning Algorithmic Deep Dive Using R." />
  



<meta name="date" content="2018-08-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="random-forest.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Hands-on Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-should-read-this"><i class="fa fa-check"></i>Who should read this</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i>Why R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#additional-resources"><i class="fa fa-check"></i>Additional resources</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-information"><i class="fa fa-check"></i>Software information</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#supervised-learning"><i class="fa fa-check"></i><b>1.1</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#regression-problems"><i class="fa fa-check"></i><b>1.1.1</b> Regression problems</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#classification-problems"><i class="fa fa-check"></i><b>1.1.2</b> Classification problems</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#algorithm-comparison-guide"><i class="fa fa-check"></i><b>1.1.3</b> Algorithm Comparison Guide</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.2</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#algorithm-decision-guide"><i class="fa fa-check"></i><b>1.2.1</b> Algorithm Decision Guide</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#machine-learning-interpretability"><i class="fa fa-check"></i><b>1.3</b> Machine learning interpretability</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.4</b> The data sets</a></li>
</ul></li>
<li class="part"><span><b>I Supervised Learning</b></span></li>
<li class="chapter" data-level="2" data-path="regression-performance.html"><a href="regression-performance.html"><i class="fa fa-check"></i><b>2</b> Preparing for Supervised Machine Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-performance.html"><a href="regression-performance.html#reg_perf_prereq"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="regression-performance.html"><a href="regression-performance.html#reg_perf_split"><i class="fa fa-check"></i><b>2.2</b> Data splitting</a><ul>
<li class="chapter" data-level="2.2.1" data-path="regression-performance.html"><a href="regression-performance.html#spending-our-data-wisely"><i class="fa fa-check"></i><b>2.2.1</b> Spending our data wisely</a></li>
<li class="chapter" data-level="2.2.2" data-path="regression-performance.html"><a href="regression-performance.html#simple-random-sampling"><i class="fa fa-check"></i><b>2.2.2</b> Simple random sampling</a></li>
<li class="chapter" data-level="2.2.3" data-path="regression-performance.html"><a href="regression-performance.html#stratified-sampling"><i class="fa fa-check"></i><b>2.2.3</b> Stratified sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="regression-performance.html"><a href="regression-performance.html#reg_perf_feat"><i class="fa fa-check"></i><b>2.3</b> Feature engineering</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-performance.html"><a href="regression-performance.html#response-transformation"><i class="fa fa-check"></i><b>2.3.1</b> Response Transformation</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-performance.html"><a href="regression-performance.html#predictor-transformation"><i class="fa fa-check"></i><b>2.3.2</b> Predictor Transformation</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-performance.html"><a href="regression-performance.html#one-hot-encoding"><i class="fa fa-check"></i><b>2.3.3</b> One-hot encoding</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-performance.html"><a href="regression-performance.html#standardizing"><i class="fa fa-check"></i><b>2.3.4</b> Standardizing</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-performance.html"><a href="regression-performance.html#alternative-feature-transformation"><i class="fa fa-check"></i><b>2.3.5</b> Alternative Feature Transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-performance.html"><a href="regression-performance.html#reg_perf_model"><i class="fa fa-check"></i><b>2.4</b> Basic model formulation</a></li>
<li class="chapter" data-level="2.5" data-path="regression-performance.html"><a href="regression-performance.html#reg_perf_tune"><i class="fa fa-check"></i><b>2.5</b> Model tuning</a></li>
<li class="chapter" data-level="2.6" data-path="regression-performance.html"><a href="regression-performance.html#cv"><i class="fa fa-check"></i><b>2.6</b> Cross Validation for Generalization</a></li>
<li class="chapter" data-level="2.7" data-path="regression-performance.html"><a href="regression-performance.html#reg_perf_eval"><i class="fa fa-check"></i><b>2.7</b> Model evaluation</a><ul>
<li class="chapter" data-level="2.7.1" data-path="regression-performance.html"><a href="regression-performance.html#regression-models"><i class="fa fa-check"></i><b>2.7.1</b> Regression models</a></li>
<li class="chapter" data-level="2.7.2" data-path="regression-performance.html"><a href="regression-performance.html#classification-models"><i class="fa fa-check"></i><b>2.7.2</b> Classification models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regularized-regression.html"><a href="regularized-regression.html"><i class="fa fa-check"></i><b>3</b> Regularized Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-req"><i class="fa fa-check"></i><b>3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="3.2" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-pros-cons"><i class="fa fa-check"></i><b>3.2</b> Advantages &amp; Disadvantages</a></li>
<li class="chapter" data-level="3.3" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-why"><i class="fa fa-check"></i><b>3.3</b> The Idea</a><ul>
<li class="chapter" data-level="3.3.1" data-path="regularized-regression.html"><a href="regularized-regression.html#multicollinearity"><i class="fa fa-check"></i><b>3.3.1</b> 1. Multicollinearity</a></li>
<li class="chapter" data-level="3.3.2" data-path="regularized-regression.html"><a href="regularized-regression.html#insufficient-solution"><i class="fa fa-check"></i><b>3.3.2</b> 2. Insufficient solution</a></li>
<li class="chapter" data-level="3.3.3" data-path="regularized-regression.html"><a href="regularized-regression.html#interpretability"><i class="fa fa-check"></i><b>3.3.3</b> 3. Interpretability</a></li>
<li class="chapter" data-level="3.3.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regularized_regress"><i class="fa fa-check"></i><b>3.3.4</b> Regularized Models</a><ul>
<li class="chapter" data-level="3.3.4.1" data-path="regularized-regression.html"><a href="regularized-regression.html#ridge"><i class="fa fa-check"></i><b>3.3.4.1</b> Ridge penalty</a></li>
<li class="chapter" data-level="3.3.4.2" data-path="regularized-regression.html"><a href="regularized-regression.html#lasso"><i class="fa fa-check"></i><b>3.3.4.2</b> Lasso penalty</a></li>
<li class="chapter" data-level="3.3.4.3" data-path="regularized-regression.html"><a href="regularized-regression.html#elastic"><i class="fa fa-check"></i><b>3.3.4.3</b> Elastic nets</a></li>
</ul></li>
<li class="chapter" data-level="3.3.5" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-tuning"><i class="fa fa-check"></i><b>3.3.5</b> Tuning</a></li>
<li class="chapter" data-level="3.3.6" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-pkg-implementation"><i class="fa fa-check"></i><b>3.3.6</b> Package implementation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-regression"><i class="fa fa-check"></i><b>3.4</b> Implementation: Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glm-glmnet"><i class="fa fa-check"></i><b>3.4.1</b> <code>glmnet</code></a><ul>
<li class="chapter" data-level="3.4.1.1" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-basic"><i class="fa fa-check"></i><b>3.4.1.1</b> Basic implementation</a></li>
<li class="chapter" data-level="3.4.1.2" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-tune"><i class="fa fa-check"></i><b>3.4.1.2</b> Tuning</a></li>
<li class="chapter" data-level="3.4.1.3" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-visualizing"><i class="fa fa-check"></i><b>3.4.1.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="3.4.1.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-predict"><i class="fa fa-check"></i><b>3.4.1.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="3.4.2" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-regularize-h2o"><i class="fa fa-check"></i><b>3.4.2</b> <code>h2o</code></a><ul>
<li class="chapter" data-level="3.4.2.1" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-h2o-basic"><i class="fa fa-check"></i><b>3.4.2.1</b> Basic implementation</a></li>
<li class="chapter" data-level="3.4.2.2" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-h2o-tune"><i class="fa fa-check"></i><b>3.4.2.2</b> Tuning</a></li>
<li class="chapter" data-level="3.4.2.3" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-h2o-viz"><i class="fa fa-check"></i><b>3.4.2.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="3.4.2.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-h2o-predict"><i class="fa fa-check"></i><b>3.4.2.4</b> Predicting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-binary-classification"><i class="fa fa-check"></i><b>3.5</b> Implementation: Binary Classification</a><ul>
<li class="chapter" data-level="3.5.1" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-binary-glm-glmnet"><i class="fa fa-check"></i><b>3.5.1</b> <code>glmnet</code></a><ul>
<li class="chapter" data-level="3.5.1.1" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-binary-glmnet-basic"><i class="fa fa-check"></i><b>3.5.1.1</b> Basic implementation</a></li>
<li class="chapter" data-level="3.5.1.2" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-binaryglmnet-tune"><i class="fa fa-check"></i><b>3.5.1.2</b> Tuning</a></li>
<li class="chapter" data-level="3.5.1.3" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-binary-glmnet-visualizing"><i class="fa fa-check"></i><b>3.5.1.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="3.5.1.4" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-binary-glmnet-predict"><i class="fa fa-check"></i><b>3.5.1.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="3.5.2" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-binaryglm-h2o"><i class="fa fa-check"></i><b>3.5.2</b> <code>h2o</code></a><ul>
<li class="chapter" data-level="3.5.2.1" data-path="regularized-regression.html"><a href="regularized-regression.html#h2o-glm-classification-binary-basic"><i class="fa fa-check"></i><b>3.5.2.1</b> Basic implementation</a></li>
<li class="chapter" data-level="3.5.2.2" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-h2o-classification-binary-tune"><i class="fa fa-check"></i><b>3.5.2.2</b> Tuning</a></li>
<li class="chapter" data-level="3.5.2.3" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-h2o-classification-binary-viz"><i class="fa fa-check"></i><b>3.5.2.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="3.5.2.4" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-h2o-classification-binary-predict"><i class="fa fa-check"></i><b>3.5.2.4</b> Predicting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-multinomial-classification"><i class="fa fa-check"></i><b>3.6</b> Implementation: Multinomial Classification</a><ul>
<li class="chapter" data-level="3.6.1" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-multi-glm-glmnet"><i class="fa fa-check"></i><b>3.6.1</b> <code>glmnet</code></a><ul>
<li class="chapter" data-level="3.6.1.1" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-multi-glmnet-basic"><i class="fa fa-check"></i><b>3.6.1.1</b> Basic implementation</a></li>
<li class="chapter" data-level="3.6.1.2" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-multi-glmnet-tune"><i class="fa fa-check"></i><b>3.6.1.2</b> Tuning</a></li>
<li class="chapter" data-level="3.6.1.3" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-multi-glmnet-visualizing"><i class="fa fa-check"></i><b>3.6.1.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="3.6.1.4" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-multi-glmnet-predict"><i class="fa fa-check"></i><b>3.6.1.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="3.6.2" data-path="regularized-regression.html"><a href="regularized-regression.html#classification-multinomial-glm-h2o"><i class="fa fa-check"></i><b>3.6.2</b> <code>h2o</code></a><ul>
<li class="chapter" data-level="3.6.2.1" data-path="regularized-regression.html"><a href="regularized-regression.html#h2o-glm-classification-multinomial-basic"><i class="fa fa-check"></i><b>3.6.2.1</b> Basic implementation</a></li>
<li class="chapter" data-level="3.6.2.2" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-h2o-classification-multinomial-tune"><i class="fa fa-check"></i><b>3.6.2.2</b> Tuning</a></li>
<li class="chapter" data-level="3.6.2.3" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-h2o-classification-multinomial-viz"><i class="fa fa-check"></i><b>3.6.2.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="3.6.2.4" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-h2o-classification-multinomial-predict"><i class="fa fa-check"></i><b>3.6.2.4</b> Predicting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="regularized-regression.html"><a href="regularized-regression.html#glm-learning"><i class="fa fa-check"></i><b>3.7</b> Learning More</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>4</b> Random Forest</a><ul>
<li class="chapter" data-level="4.1" data-path="random-forest.html"><a href="random-forest.html#rf-requirements"><i class="fa fa-check"></i><b>4.1</b> Prerequisites</a></li>
<li class="chapter" data-level="4.2" data-path="random-forest.html"><a href="random-forest.html#rf-proscons"><i class="fa fa-check"></i><b>4.2</b> Advantages &amp; Disadvantages</a></li>
<li class="chapter" data-level="4.3" data-path="random-forest.html"><a href="random-forest.html#rf-idea"><i class="fa fa-check"></i><b>4.3</b> The Idea</a><ul>
<li class="chapter" data-level="4.3.1" data-path="random-forest.html"><a href="random-forest.html#rf-oob"><i class="fa fa-check"></i><b>4.3.1</b> OOB error vs. test set error</a></li>
<li class="chapter" data-level="4.3.2" data-path="random-forest.html"><a href="random-forest.html#rf-tune"><i class="fa fa-check"></i><b>4.3.2</b> Tuning</a></li>
<li class="chapter" data-level="4.3.3" data-path="random-forest.html"><a href="random-forest.html#rf-pkgs"><i class="fa fa-check"></i><b>4.3.3</b> Package implementation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="random-forest.html"><a href="random-forest.html#rf-regression"><i class="fa fa-check"></i><b>4.4</b> Implementation: Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="random-forest.html"><a href="random-forest.html#ranger-regression"><i class="fa fa-check"></i><b>4.4.1</b> <code>ranger</code></a><ul>
<li class="chapter" data-level="4.4.1.1" data-path="random-forest.html"><a href="random-forest.html#ranger-regression-basic"><i class="fa fa-check"></i><b>4.4.1.1</b> Basic implementation</a></li>
<li class="chapter" data-level="4.4.1.2" data-path="random-forest.html"><a href="random-forest.html#ranger-regression-tune"><i class="fa fa-check"></i><b>4.4.1.2</b> Tuning</a></li>
<li class="chapter" data-level="4.4.1.3" data-path="random-forest.html"><a href="random-forest.html#ranger-regression-viz"><i class="fa fa-check"></i><b>4.4.1.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="4.4.1.4" data-path="random-forest.html"><a href="random-forest.html#ranger-regression-predic"><i class="fa fa-check"></i><b>4.4.1.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="4.4.2" data-path="random-forest.html"><a href="random-forest.html#h2o-rf-regression"><i class="fa fa-check"></i><b>4.4.2</b> <code>h20</code></a><ul>
<li class="chapter" data-level="4.4.2.1" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-regression-basic"><i class="fa fa-check"></i><b>4.4.2.1</b> Basic implementation</a></li>
<li class="chapter" data-level="4.4.2.2" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-regression-tune"><i class="fa fa-check"></i><b>4.4.2.2</b> Tuning</a></li>
<li class="chapter" data-level="4.4.2.3" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-regression-viz"><i class="fa fa-check"></i><b>4.4.2.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="4.4.2.4" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-regression-predict"><i class="fa fa-check"></i><b>4.4.2.4</b> Predicting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="random-forest.html"><a href="random-forest.html#rf-binary-classification"><i class="fa fa-check"></i><b>4.5</b> Implementation: Binary Classification</a><ul>
<li class="chapter" data-level="4.5.1" data-path="random-forest.html"><a href="random-forest.html#ranger-rf-binary-classification"><i class="fa fa-check"></i><b>4.5.1</b> <code>ranger</code></a><ul>
<li class="chapter" data-level="4.5.1.1" data-path="random-forest.html"><a href="random-forest.html#ranger-binary-classification-basic"><i class="fa fa-check"></i><b>4.5.1.1</b> Basic implementation</a></li>
<li class="chapter" data-level="4.5.1.2" data-path="random-forest.html"><a href="random-forest.html#ranger-rf-binary-classification-tune"><i class="fa fa-check"></i><b>4.5.1.2</b> Tuning</a></li>
<li class="chapter" data-level="4.5.1.3" data-path="random-forest.html"><a href="random-forest.html#ranger-rf-binary-classification-viz"><i class="fa fa-check"></i><b>4.5.1.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="4.5.1.4" data-path="random-forest.html"><a href="random-forest.html#ranger-rf-binary-classification-predict"><i class="fa fa-check"></i><b>4.5.1.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="4.5.2" data-path="random-forest.html"><a href="random-forest.html#h2o-rf-binary-classification"><i class="fa fa-check"></i><b>4.5.2</b> <code>h20</code></a><ul>
<li class="chapter" data-level="4.5.2.1" data-path="random-forest.html"><a href="random-forest.html#h2o-rf-binary-classification-basic"><i class="fa fa-check"></i><b>4.5.2.1</b> Basic implementation</a></li>
<li class="chapter" data-level="4.5.2.2" data-path="random-forest.html"><a href="random-forest.html#h2o-rf-multi-classification-tune"><i class="fa fa-check"></i><b>4.5.2.2</b> Tuning</a></li>
<li class="chapter" data-level="4.5.2.3" data-path="random-forest.html"><a href="random-forest.html#h2o-rf-binary-classification-viz"><i class="fa fa-check"></i><b>4.5.2.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="4.5.2.4" data-path="random-forest.html"><a href="random-forest.html#h2o-rf-binary-classification-predict"><i class="fa fa-check"></i><b>4.5.2.4</b> Predicting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="random-forest.html"><a href="random-forest.html#rf-multi"><i class="fa fa-check"></i><b>4.6</b> Implementation: Multinomial Classification</a><ul>
<li class="chapter" data-level="4.6.1" data-path="random-forest.html"><a href="random-forest.html#rf-ranger-multi"><i class="fa fa-check"></i><b>4.6.1</b> <code>ranger</code></a><ul>
<li class="chapter" data-level="4.6.1.1" data-path="random-forest.html"><a href="random-forest.html#ranger-multi-basic"><i class="fa fa-check"></i><b>4.6.1.1</b> Basic implementation</a></li>
<li class="chapter" data-level="4.6.1.2" data-path="random-forest.html"><a href="random-forest.html#ranger-multi-tune"><i class="fa fa-check"></i><b>4.6.1.2</b> Tuning</a></li>
<li class="chapter" data-level="4.6.1.3" data-path="random-forest.html"><a href="random-forest.html#ranger-multi-viz"><i class="fa fa-check"></i><b>4.6.1.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="4.6.1.4" data-path="random-forest.html"><a href="random-forest.html#ranger-multi-predict"><i class="fa fa-check"></i><b>4.6.1.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="4.6.2" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-multi"><i class="fa fa-check"></i><b>4.6.2</b> <code>h2o</code></a><ul>
<li class="chapter" data-level="4.6.2.1" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-multi-basic"><i class="fa fa-check"></i><b>4.6.2.1</b> Basic implementation</a></li>
<li class="chapter" data-level="4.6.2.2" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-multi-tune"><i class="fa fa-check"></i><b>4.6.2.2</b> Tuning</a></li>
<li class="chapter" data-level="4.6.2.3" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-multi-viz"><i class="fa fa-check"></i><b>4.6.2.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="4.6.2.4" data-path="random-forest.html"><a href="random-forest.html#rf-h2o-multi-predict"><i class="fa fa-check"></i><b>4.6.2.4</b> Predicting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="random-forest.html"><a href="random-forest.html#rf-learn"><i class="fa fa-check"></i><b>4.7</b> Learning More</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html"><i class="fa fa-check"></i><b>5</b> Gradient Boosting Machines</a><ul>
<li class="chapter" data-level="5.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-prereq"><i class="fa fa-check"></i><b>5.1</b> Package Requirements</a></li>
<li class="chapter" data-level="5.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-proscons"><i class="fa fa-check"></i><b>5.2</b> Advantages &amp; Disadvantages</a></li>
<li class="chapter" data-level="5.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-idea"><i class="fa fa-check"></i><b>5.3</b> The Idea</a><ul>
<li class="chapter" data-level="5.3.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-gradient"><i class="fa fa-check"></i><b>5.3.1</b> Gradient descent</a></li>
<li class="chapter" data-level="5.3.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-tuning"><i class="fa fa-check"></i><b>5.3.2</b> Tuning</a></li>
<li class="chapter" data-level="5.3.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-pkg-options"><i class="fa fa-check"></i><b>5.3.3</b> Package implementation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression"><i class="fa fa-check"></i><b>5.4</b> Implementation: Regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-gbm"><i class="fa fa-check"></i><b>5.4.1</b> gbm</a><ul>
<li class="chapter" data-level="5.4.1.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-gbm-basic"><i class="fa fa-check"></i><b>5.4.1.1</b> Basic implementation</a></li>
<li class="chapter" data-level="5.4.1.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-gbm-tune"><i class="fa fa-check"></i><b>5.4.1.2</b> Tuning</a></li>
<li class="chapter" data-level="5.4.1.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-gbm-viz"><i class="fa fa-check"></i><b>5.4.1.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="5.4.1.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-gbm-predict"><i class="fa fa-check"></i><b>5.4.1.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="5.4.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#regression-xgboost"><i class="fa fa-check"></i><b>5.4.2</b> xgboost</a><ul>
<li class="chapter" data-level="5.4.2.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-xgb-basic"><i class="fa fa-check"></i><b>5.4.2.1</b> Basic implementation</a></li>
<li class="chapter" data-level="5.4.2.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-xgb-tune"><i class="fa fa-check"></i><b>5.4.2.2</b> Tuning</a></li>
<li class="chapter" data-level="5.4.2.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-xgb-viz"><i class="fa fa-check"></i><b>5.4.2.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="5.4.2.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-gbm-predict"><i class="fa fa-check"></i><b>5.4.2.4</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="5.4.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-h2o"><i class="fa fa-check"></i><b>5.4.3</b> h2o</a><ul>
<li class="chapter" data-level="5.4.3.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-h2o-basic"><i class="fa fa-check"></i><b>5.4.3.1</b> Basic implementation</a></li>
<li class="chapter" data-level="5.4.3.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-h2o-tune"><i class="fa fa-check"></i><b>5.4.3.2</b> Tuning</a></li>
<li class="chapter" data-level="5.4.3.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-h2o-viz"><i class="fa fa-check"></i><b>5.4.3.3</b> Feature interpretation</a></li>
<li class="chapter" data-level="5.4.3.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-regression-h2o-predict"><i class="fa fa-check"></i><b>5.4.3.4</b> Predicting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-binary-classification"><i class="fa fa-check"></i><b>5.5</b> Implementation: Binary Classification</a><ul>
<li class="chapter" data-level="5.5.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm"><i class="fa fa-check"></i><b>5.5.1</b> gbm</a></li>
<li class="chapter" data-level="5.5.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#xgboost"><i class="fa fa-check"></i><b>5.5.2</b> xgboost</a></li>
<li class="chapter" data-level="5.5.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#h2o"><i class="fa fa-check"></i><b>5.5.3</b> h2o</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-multi-classification"><i class="fa fa-check"></i><b>5.6</b> Implementation: Multinomial Classification</a><ul>
<li class="chapter" data-level="5.6.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-1"><i class="fa fa-check"></i><b>5.6.1</b> gbm</a></li>
<li class="chapter" data-level="5.6.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#xgboost-1"><i class="fa fa-check"></i><b>5.6.2</b> xgboost</a></li>
<li class="chapter" data-level="5.6.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#h2o-1"><i class="fa fa-check"></i><b>5.6.3</b> h2o</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#learning-more"><i class="fa fa-check"></i><b>5.7</b> Learning More</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Hands-on Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient-boosting-machines" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Gradient Boosting Machines</h1>
<p><img src="images/boosted_stumps.gif"  style="float:right; margin: -20px 0px 0px 0px; width: 30%; height: 30%;" /> Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions. Whereas random forests (Chapter @ref(random_forest)) build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms. This chapter will cover the fundamentals to understanding and implementing GBMs.</p>
<div id="gbm-prereq" class="section level2">
<h2><span class="header-section-number">5.1</span> Package Requirements</h2>
<p>This chapter leverages the following packages. Some of these packages play a supporting role; however, our focus is on demonstrating how to implement GBMs with the <strong>gbm</strong> <span class="citation">(others <a href="#ref-R-gbm">2017</a>)</span>, <strong>xgboost</strong> <span class="citation">(T. Chen et al. <a href="#ref-R-xgboost">2018</a>)</span>, and <strong>h2o</strong> packages and discuss the pros and cons to each.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rsample)  <span class="co"># data splitting </span>
<span class="kw">library</span>(gbm)      <span class="co"># original implementation of gbm</span>
<span class="kw">library</span>(xgboost)  <span class="co"># a faster implementation of gbm</span>
<span class="kw">library</span>(h2o)      <span class="co"># a java-based platform</span>
<span class="kw">library</span>(vip)      <span class="co"># visualize feature importance </span>
<span class="kw">library</span>(pdp)      <span class="co"># visualize feature effects</span>
<span class="kw">library</span>(ggplot2)  <span class="co"># model visualization</span></code></pre></div>
</div>
<div id="gbm-proscons" class="section level2">
<h2><span class="header-section-number">5.2</span> Advantages &amp; Disadvantages</h2>
<p><strong>Advantages:</strong></p>
<ul>
<li>Often provides predictive accuracy that cannot be beat.</li>
<li>Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.</li>
<li>No data pre-processing required - often works great with categorical and numerical values as is.</li>
<li>Handles missing data - imputation not required.</li>
</ul>
<p><strong>Disdvantages:</strong></p>
<ul>
<li>GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.</li>
<li>Computationally expensive - GBMs often require many trees (&gt;1000) which can be time and memory exhaustive.</li>
<li>The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.</li>
<li>Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, local variable importance, etc.).</li>
</ul>
</div>
<div id="gbm-idea" class="section level2">
<h2><span class="header-section-number">5.3</span> The Idea</h2>
<p>Several supervised machine learning models are founded on a single predictive model such as linear regression, penalized models, naive Bayes, support vector machines. Alternatively, other approaches such as bagging and random forests are built on the idea of building an ensemble of models where each individual model predicts the outcome and then the ensemble simply averages the predicted values. The family of boosting methods is based on a different, constructive strategy of ensemble formation.</p>
<p>The main idea of boosting is to add new models to the ensemble <strong><em>sequentially</em></strong>. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.</p>
<div class="figure" style="text-align: center"><span id="fig:sequential-fig"></span>
<img src="images/boosted-trees-process.png" alt="Sequential ensemble approach." width="75%" height="75%" />
<p class="caption">
Figure 5.1: Sequential ensemble approach.
</p>
</div>
<p>Let’s discuss each component of the previous sentence in closer detail because they are important.</p>
<p><strong>Base-learning models</strong>: Boosting is a framework that iteratively improves <em>any</em> weak learning model. Many gradient boosting applications allow you to “plug in” various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner. Consequently, this chapter will discuss boosting in the context of decision trees.</p>
<p><strong>Training weak models</strong>: A weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used. Combining many weak models (versus strong ones) has a few benefits:</p>
<ul>
<li>Speed: Constructing weak models is computationally cheap.</li>
<li>Accuracy improvement: Weak models allow the algorithm to <em>learn slowly</em>; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.</li>
<li>Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).</li>
</ul>
<p><strong>Sequential training with respect to errors</strong>: Boosted trees are grown sequentially; each tree is grown using information from previously grown trees. The basic algorithm for boosted regression trees can be generalized to the following where <em>x</em> represents our features and <em>y</em> represents our response:</p>
<ol style="list-style-type: decimal">
<li>Fit a decision tree to the data: <span class="math inline">\(F_1(x) = y\)</span>,</li>
<li>We then fit the next decision tree to the residuals of the previous: <span class="math inline">\(h_1(x) = y - F_1(x)\)</span>,</li>
<li>Add this new tree to our algorithm: <span class="math inline">\(F_2(x) = F_1(x) + h_1(x)\)</span>,</li>
<li>Fit the next decision tree to the residuals of <span class="math inline">\(F_2\)</span>: <span class="math inline">\(h_2(x) = y - F_2(x)\)</span>,</li>
<li>Add this new tree to our algorithm: <span class="math inline">\(F_3(x) = F_2(x) + h_1(x)\)</span>,</li>
<li>Continue this process until some mechanism (i.e. cross validation) tells us to stop.</li>
</ol>
<p>The basic algorithm for boosted decision trees can be generalized to the following where the final model is simply a stagewise additive model of <em>b</em> individual trees:</p>
<p><span class="math display">\[ f(x) =  \sum^B_{b=1}f^b(x) \tag{1} \]</span></p>
<p>To illustrate the behavior, assume the following <em>x</em> and <em>y</em> observations. The blue sine wave represents the true underlying function and the points represent observations that include some irriducible error (noise). The boosted prediction illustrates the adjusted predictions after each additional sequential tree is added to the algorithm. Initially, there are large errors which the boosted algorithm improves upon immediately but as the predictions get closer to the true underlying function you see each additional tree make small improvements in different areas across the feature space where errors remain. Towards the end of the gif, the predicted values nearly converge to the true underlying function.</p>
<div class="figure" style="text-align: center"><span id="fig:boosted-gif"></span>
<img src="images/boosted_stumps.gif" alt="Boosted regression tree predictions (courtesy of [Brandon Greenwell](https://github.com/bgreenwell))" width="50%" height="50%" />
<p class="caption">
Figure 5.2: Boosted regression tree predictions (courtesy of <a href="https://github.com/bgreenwell">Brandon Greenwell</a>)
</p>
</div>
<div id="gbm-gradient" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Gradient descent</h3>
<p>Many algorithms, including decision trees, focus on minimizing the residuals and, therefore, emphasize the MSE loss function. The algorithm discussed in the previous section outlines the approach of sequentially fitting regression trees to minimize the errors. This specific approach is how gradient boosting minimizes the mean squared error (MSE) loss function. However, often we wish to focus on other loss functions such as mean absolute error (MAE) or to be able to apply the method to a classification problem with a loss function such as deviance. The name <strong><em>gradient</em></strong> boosting machine comes from the fact that this procedure can be generalized to loss functions other than MSE.</p>
<p>Gradient boosting is considered a <strong><em>gradient descent</em></strong> algorithm. Gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are a downhill skier racing your friend. A good strategy to beat your friend to the bottom is to take the path with the steepest slope. This is exactly what gradient descent does - it measures the local gradient of the loss (cost) function for a given set of parameters (<span class="math inline">\(\Theta\)</span>) and takes steps in the direction of the descending gradient. Once the gradient is zero, we have reached the minimum.</p>
<div class="figure" style="text-align: center"><span id="fig:gradient-descent-fig"></span>
<img src="images/gradient_descent.png" alt="Gradient descent is the process of gradually decreasing the cost function (i.e. MSE) by tweaking parameters iteratively until you have reached a minimum. Image courtesy of @geron2017hands." width="50%" height="50%" />
<p class="caption">
Figure 5.3: Gradient descent is the process of gradually decreasing the cost function (i.e. MSE) by tweaking parameters iteratively until you have reached a minimum. Image courtesy of <span class="citation">Géron (<a href="#ref-geron2017hands">2017</a>)</span>.
</p>
</div>
<p>Gradient descent can be performed on any loss function that is differentiable. Consequently, this allows GBMs to optimize different loss functions as desired (see <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-esl">2001</a>)</span>, p. 360 for common loss functions). An important parameter in gradient descent is the size of the steps which is determined by the <em>learning rate</em>. If the learning rate is too small, then the algorithm will take many iterations to find the minimum. On the other hand, if the learning rate is too high, you might jump across the minimum and end up further away than when you started.</p>
<div class="figure" style="text-align: center"><span id="fig:learning-rate-fig"></span>
<img src="images/learning_rate_comparison.png" alt="A learning rate that is too small will require many iterations to find the minimum. A learning rate too big may jump over the minimum.  Image courtesy of @geron2017hands." width="70%" height="70%" />
<p class="caption">
Figure 5.4: A learning rate that is too small will require many iterations to find the minimum. A learning rate too big may jump over the minimum. Image courtesy of <span class="citation">Géron (<a href="#ref-geron2017hands">2017</a>)</span>.
</p>
</div>
<p>Moreover, not all cost functions are convex (bowl shaped). There may be local minimas, plateaus, and other irregular terrain of the loss function that makes finding the global minimum difficult. <strong><em>Stochastic gradient descent</em></strong> can help us address this problem by sampling a fraction of the training observations (typically without replacement) and growing the next tree using that subsample. This makes the algorithm faster but the stochastic nature of random sampling also adds some random nature in descending the loss function gradient. Although this randomness does not allow the algorithm to find the absolute global minimum, it can actually help the algorithm jump out of local minima and off plateaus and get near the global minimum.</p>
<div class="figure" style="text-align: center"><span id="fig:stochastic-gradient-descent-fig"></span>
<img src="images/stochastic_gradient_descent.png" alt="Stochastic gradient descent will often find a near-optimal solution by jumping out of local minimas and off plateaus. Image courtesy of @geron2017hands." width="40%" height="40%" />
<p class="caption">
Figure 5.5: Stochastic gradient descent will often find a near-optimal solution by jumping out of local minimas and off plateaus. Image courtesy of <span class="citation">Géron (<a href="#ref-geron2017hands">2017</a>)</span>.
</p>
</div>
<p>As we’ll see in the next section, there are several hyperparameter tuning options that allow us to address how we approach the gradient descent of our loss function.</p>
</div>
<div id="gbm-tuning" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Tuning</h3>
<p>Part of the beauty and challenges of GBMs is that they offer several tuning parameters. The beauty in this is GBMs are highly flexible. The challenge is that they can be time consuming to tune and find the optimal combination of hyperparamters. The most common hyperparameters that you will find in most GBM implementations include:</p>
<ul>
<li><strong>Number of trees:</strong> The total number of trees to fit. GBMs often require many trees; however, unlike random forests GBMs can overfit so the goal is to find the optimal number of trees that minimize the loss function of interest with cross validation.</li>
<li><strong>Depth of trees:</strong> The number <em>d</em> of splits in each tree, which controls the complexity of the boosted ensemble. Often <span class="math inline">\(d = 1\)</span> works well, in which case each tree is a <em>stump</em> consisting of a single split. More commonly, <em>d</em> is greater than 1 but it is unlikely <span class="math inline">\(d &gt; 10\)</span> will be required.</li>
<li><strong>Learning rate:</strong> Controls how quickly the algorithm proceeds down the gradient descent. Smaller values reduce the chance of overfitting but also increases the time to find the optimal fit. This is also called <em>shrinkage</em>.</li>
<li><strong>Subsampling:</strong> Controls whether or not you use a fraction of the available training observations. Using less than 100% of the training observations means you are implementing stochastic gradient descent. This can help to minimize overfitting and keep from getting stuck in a local minimum or plateau of the loss function gradient.</li>
</ul>
<p>Throughout this chapter you’ll be exposed to additional hyperparameters that are specific to certain packages and can improve performance and/or the efficiency of training and tuning models.</p>
</div>
<div id="gbm-pkg-options" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Package implementation</h3>
<p>There are many packages that implement GBMs and GBM variants. You can find a fairly comprehensive list <a href="https://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html#gbm-software-in-r">here</a> and at the <a href="https://cran.r-project.org/web/views/MachineLearning.html">CRAN Machine Learning Task View</a>. However, the most popular implementations which we will cover in this post include:</p>
<ul>
<li><a href="https://cran.r-project.org/web/packages/gbm/index.html">gbm</a>: The <a href="https://github.com/gbm-developers/gbm">gbm</a> R package is an implementation of extensions to Freund and Schapire’s AdaBoost algorithm and Friedman’s gradient boosting machine. This is the original R implementation of GBM. A presentation is available <a href="https://www.slideshare.net/mark_landry/gbm-package-in-r">here</a> by Mark Landry. Features include<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>:
<ul>
<li>Stochastic GBM.</li>
<li>Supports up to 1024 factor levels.</li>
<li>Supports Classification and regression trees.</li>
<li>Includes methods for:
<ul>
<li>least squares</li>
<li>absolute loss</li>
<li>t-distribution loss</li>
<li>quantile regression</li>
<li>logistic</li>
<li>multinomial logistic</li>
<li>Poisson</li>
<li>Cox proportional hazards partial likelihood</li>
<li>AdaBoost exponential loss</li>
<li>Huberized hinge loss</li>
<li>Learning to Rank measures (<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-109.pdf">LambdaMart</a>)</li>
</ul></li>
<li>Out-of-bag estimator for the optimal number of iterations is provided.</li>
<li>Easy to overfit since early stopping functionality is not automated in thispackage.</li>
<li>If internal cross-validation is used, this can be parallelized to all cores on the machine.</li>
<li>Currently undergoing a major refactoring &amp; rewrite (and has been for sometime).</li>
<li>GPL-2/3 License.</li>
</ul></li>
<li><a href="https://cran.r-project.org/web/packages/xgboost/index.html">xgboost</a>: A fast and efficient gradient boosting framework with a C++ backend). Many resource are available <a href="https://github.com/dmlc/xgboost/tree/master/demo">here</a>. The xgboost package is quite popular on <a href="http://blog.kaggle.com/tag/xgboost/">Kaggle</a> for data mining competitions. Features include:
<ul>
<li>Stochastic GBM with column and row sampling (per split and per tree) for better generalization.</li>
<li>Includes efficient linear model solver and tree learning algorithms.</li>
<li>Parallel computation on a single machine.</li>
<li>Supports various objective functions, including regression, classification and ranking.</li>
<li>The package is made to be extensible, so that users are also allowed to define their own objectives easily.</li>
<li>Apache 2.0 License.</li>
</ul></li>
<li><a href="https://cran.r-project.org/web/packages/gamboostLSS/index.html">h2o</a>: A powerful java-based interface that provides parallel distributed algorithms and efficient productionalization. Resources regarding <strong>h2o</strong>’s GBM implementation include a Tuning guide by Arno Candel](<a href="https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd" class="uri">https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd</a>) and a <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/GBMBooklet.pdf">Vignette</a>. Features include:
<ul>
<li>Distributed and parallelized computation on either a single node or a multi-node cluster.</li>
<li>Automatic early stopping based on convergence of user-specified metrics to user-specied relative tolerance.</li>
<li>Stochastic GBM with column and row sampling (per split and per tree) for better generalization.</li>
<li>Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace).</li>
<li>Grid search for hyperparameter optimization and model selection.</li>
<li>Data-distributed, which means the entire dataset does not need to fit into memory on a single node, hence scales to any size training set.</li>
<li>Uses histogram approximations of continuous variables for speedup.</li>
<li>Uses dynamic binning - bin limits are reset at each tree level based on the split bins’ min and max values discovered during the last pass.</li>
<li>Uses squared error to determine optimal splits.</li>
<li>Distributed implementation details outlined in a <a href="http://blog.h2o.ai/2013/10/building-distributed-gbm-h2o/">blog post</a> by Cliff Click.</li>
<li>Unlimited factor levels.</li>
<li>Multiclass trees (one for each class) built in parallel with each other.</li>
<li>Apache 2.0 Licensed.</li>
<li>Model export in plain Java code for deployment in production environments.</li>
</ul></li>
</ul>
</div>
</div>
<div id="gbm-regression" class="section level2">
<h2><span class="header-section-number">5.4</span> Implementation: Regression</h2>
<p>To illustrate various GBM concepts for a regression problem we will continue with the Ames, IA housing data, where our intent is to predict <code>Sale_Price</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.</span>
<span class="co"># Use set.seed for reproducibility</span>

<span class="kw">set.seed</span>(<span class="dv">123</span>)
ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(AmesHousing<span class="op">::</span><span class="kw">make_ames</span>(), <span class="dt">prop =</span> .<span class="dv">7</span>, <span class="dt">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)
ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)
ames_test  &lt;-<span class="st"> </span><span class="kw">testing</span>(ames_split)</code></pre></div>
<div class="rmdcomment">
<p>
Tree-based methods tend to perform well on unprocessed data (i.e. without normalizing, centering, scaling features). In this chapter I focus on how to implement GBMs with various packages. Although I do not pre-process the data, realize that you <strong><em>can</em></strong> improve model performance by spending time processing variable attributes.
</p>
</div>
<div id="gbm-regression-gbm" class="section level3">
<h3><span class="header-section-number">5.4.1</span> gbm</h3>
<div id="gbm-regression-gbm-basic" class="section level4">
<h4><span class="header-section-number">5.4.1.1</span> Basic implementation</h4>
<p><strong>gbm</strong> has two primary training functions - <code>gbm::gbm</code> and <code>gbm::gbm.fit</code>. The primary difference is that <code>gbm::gbm</code> uses the formula interface to specify your model whereas <code>gbm::gbm.fit</code> requires the separated <code>x</code> and <code>y</code> matrices. When working with <em>many</em> variables it is more efficient to use the matrix rather than formula interface.</p>
<p>The default settings in <code>gbm</code> includes a learning rate (<code>shrinkage</code>) of 0.001. This is a very small learning rate and typically requires a large number of trees to find the minimum MSE. However, <code>gbm</code> uses a default number of trees of 100, which is rarely sufficient. Consequently, I crank it up to 5,000 trees. The default depth of each tree (<code>interaction.depth</code>) is 1, which means we are ensembling a bunch of stumps. Lastly, I also include <code>cv.folds</code> to perform a 5 fold cross validation.</p>
<div class="rmdnote">
<p>
The model took 48 seconds to run and the results show that our MSE loss function is minimized with 5,000 trees.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># train GBM model</span>
gbm.fit &lt;-<span class="st"> </span><span class="kw">gbm</span>(
  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
  <span class="dt">distribution =</span> <span class="st">&quot;gaussian&quot;</span>,
  <span class="dt">data =</span> ames_train,
  <span class="dt">n.trees =</span> <span class="dv">5000</span>,
  <span class="dt">interaction.depth =</span> <span class="dv">1</span>,
  <span class="dt">shrinkage =</span> <span class="fl">0.001</span>,
  <span class="dt">cv.folds =</span> <span class="dv">5</span>,
  <span class="dt">n.cores =</span> <span class="ot">NULL</span>, <span class="co"># will use all cores by default</span>
  <span class="dt">verbose =</span> <span class="ot">FALSE</span>
  )  

<span class="co"># print results</span>
<span class="kw">print</span>(gbm.fit)
## gbm(formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = ames_train, 
##     n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 5000 iterations were performed.
## The best cross-validation iteration was 5000.
## There were 80 predictors of which 27 had non-zero influence.</code></pre></div>
<p>The output object is a list containing several modelling and results information. We can access this information with regular indexing; I recommend you take some time to dig around in the object to get comfortable with its components. Here, we see that the minimum CV RMSE is $33,079.61 but the plot also illustrates that the CV error (MSE) is still decreasing at 5,000 trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get MSE and compute RMSE</span>
<span class="kw">sqrt</span>(<span class="kw">min</span>(gbm.fit<span class="op">$</span>cv.error))
## [1] 33079.61

<span class="co"># plot loss function as a result of n trees added to the ensemble</span>
<span class="kw">gbm.perf</span>(gbm.fit, <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gbm1-gradient-descent"></span>
<img src="images/gbm1_gradient_descent.png" alt="Training and cross-validated MSE as *n* trees are added to the GBM algorithm." width="100%" height="100%" />
<p class="caption">
Figure 5.6: Training and cross-validated MSE as <em>n</em> trees are added to the GBM algorithm.
</p>
</div>
<p>In this case, the small learning rate is resulting in very small incremental improvements which means <strong><em>many</em></strong> trees are required. In fact, with the default learning rate and tree depth settings, the CV error is still reducing after 10,000 trees!</p>
</div>
<div id="gbm-regression-gbm-tune" class="section level4">
<h4><span class="header-section-number">5.4.1.2</span> Tuning</h4>
<p>However, rarely do the default settings suffice. We could tune parameters one at a time to see how the results change. For example, here, I increase the learning rate to take larger steps down the gradient descent, reduce the number of trees (since we are reducing the learning rate), and increase the depth of each tree from using a single split to 3 splits. Our RMSE ($23,813.34) is lower than our initial model and the optimal number of trees required was 964.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># train GBM model</span>
gbm.fit2 &lt;-<span class="st"> </span><span class="kw">gbm</span>(
  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
  <span class="dt">distribution =</span> <span class="st">&quot;gaussian&quot;</span>,
  <span class="dt">data =</span> ames_train,
  <span class="dt">n.trees =</span> <span class="dv">5000</span>,
  <span class="dt">interaction.depth =</span> <span class="dv">3</span>,
  <span class="dt">shrinkage =</span> <span class="fl">0.1</span>,
  <span class="dt">cv.folds =</span> <span class="dv">5</span>,
  <span class="dt">n.cores =</span> <span class="ot">NULL</span>, <span class="co"># will use all cores by default</span>
  <span class="dt">verbose =</span> <span class="ot">FALSE</span>
  )  

<span class="co"># find index for n trees with minimum CV error</span>
min_MSE &lt;-<span class="st"> </span><span class="kw">which.min</span>(gbm.fit2<span class="op">$</span>cv.error)

<span class="co"># get MSE and compute RMSE</span>
<span class="kw">sqrt</span>(gbm.fit2<span class="op">$</span>cv.error[min_MSE])
## [1] 23813.34

<span class="co"># plot loss function as a result of n trees added to the ensemble</span>
<span class="kw">gbm.perf</span>(gbm.fit2, <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gbm2-gradient-descent"></span>
<img src="images/gbm2_gradient_descent.png" alt="Training and cross-validated MSE as *n* trees are added to the GBM algorithm. We can see that are new hyperparameter settings result in a much quicker progression down the gradient descent than our initial model." width="100%" height="100%" />
<p class="caption">
Figure 5.7: Training and cross-validated MSE as <em>n</em> trees are added to the GBM algorithm. We can see that are new hyperparameter settings result in a much quicker progression down the gradient descent than our initial model.
</p>
</div>
<p>However, a better option than manually tweaking hyperparameters one at a time is to perform a grid search which iterates over every combination of hyperparameter values and allows us to assess which combination tends to perform well. To perform a manual grid search, first we want to construct our grid of hyperparameter combinations. We’re going to search across 81 models with varying learning rates and tree depth. I also vary the minimum number of observations allowed in the trees terminal nodes (<code>n.minobsinnode</code>) and introduce stochastic gradient descent by allowing <code>bag.fraction</code> &lt; 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create hyperparameter grid</span>
hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
  <span class="dt">shrinkage =</span> <span class="kw">c</span>(.<span class="dv">01</span>, .<span class="dv">1</span>, .<span class="dv">3</span>),
  <span class="dt">interaction.depth =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>),
  <span class="dt">n.minobsinnode =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>),
  <span class="dt">bag.fraction =</span> <span class="kw">c</span>(.<span class="dv">65</span>, .<span class="dv">8</span>, <span class="dv">1</span>), 
  <span class="dt">optimal_trees =</span> <span class="dv">0</span>,               <span class="co"># a place to dump results</span>
  <span class="dt">min_RMSE =</span> <span class="dv">0</span>                     <span class="co"># a place to dump results</span>
)

<span class="co"># total number of combinations</span>
<span class="kw">nrow</span>(hyper_grid)
## [1] 81</code></pre></div>
<p>We loop through each hyperparameter combination and apply 5,000 trees. However, to speed up the tuning process, instead of performing 5-fold CV I train on 75% of the training observations and evaluate performance on the remaining 25%.</p>
<div class="rmdwarning">
<p>
When using <code>train.fraction</code> it will take the first XX% of the data so its important to randomize your rows in case their is any logic behind the ordering of the data (i.e. ordered by neighbhorhood).
</p>
</div>
<p>Our grid search revealed a few important attributes. First, our top model has better performance than our previously fitted model above and any of the other models covered in Chapters <a href="regularized-regression.html#regularized-regression">3</a> and <a href="random-forest.html#random-forest">4</a>, with an RMSE of $20,390.55. Second, looking at the top 10 models we see that:</p>
<ul>
<li>all the top models used a learning rate of 0.1 or smaller; small incremental steps down the gradient descent appears to work best,</li>
<li>all the top models used deeper trees (<code>interaction.depth = 5</code>); there are likely stome important interactions that the deeper trees are able to capture,</li>
<li>most of the models with a learning rate of 0.01 used nearly all the trees meaning they had just enough trees to converge to their global minimum.</li>
</ul>
<div class="rmdtip">
<p>
Searching this entire grid took 36 minutes.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># randomize data</span>
random_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(ames_train), <span class="kw">nrow</span>(ames_train))
random_train &lt;-<span class="st"> </span>ames_train[random_index, ]

<span class="co"># grid search </span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(hyper_grid)) {
  
  <span class="co"># reproducibility</span>
  <span class="kw">set.seed</span>(<span class="dv">123</span>)
  
  <span class="co"># train model</span>
  gbm.tune &lt;-<span class="st"> </span><span class="kw">gbm</span>(
    <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
    <span class="dt">distribution =</span> <span class="st">&quot;gaussian&quot;</span>,
    <span class="dt">data =</span> random_train,
    <span class="dt">n.trees =</span> <span class="dv">5000</span>,
    <span class="dt">interaction.depth =</span> hyper_grid<span class="op">$</span>interaction.depth[i],
    <span class="dt">shrinkage =</span> hyper_grid<span class="op">$</span>shrinkage[i],
    <span class="dt">n.minobsinnode =</span> hyper_grid<span class="op">$</span>n.minobsinnode[i],
    <span class="dt">bag.fraction =</span> hyper_grid<span class="op">$</span>bag.fraction[i],
    <span class="dt">train.fraction =</span> .<span class="dv">75</span>,
    <span class="dt">n.cores =</span> <span class="ot">NULL</span>, <span class="co"># will use all cores by default</span>
    <span class="dt">verbose =</span> <span class="ot">FALSE</span>
  )
  
  <span class="co"># add min training error and trees to grid</span>
  hyper_grid<span class="op">$</span>optimal_trees[i] &lt;-<span class="st"> </span><span class="kw">which.min</span>(gbm.tune<span class="op">$</span>valid.error)
  hyper_grid<span class="op">$</span>min_RMSE[i] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">min</span>(gbm.tune<span class="op">$</span>valid.error))
}

hyper_grid <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">arrange</span>(min_RMSE) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>(<span class="dv">10</span>)
##    shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees min_RMSE
## 1       0.01                 5              5         0.80          4911 20390.55
## 2       0.01                 5              5         0.65          4726 20588.02
## 3       0.10                 5             10         0.80           500 20758.72
## 4       0.01                 5             10         0.80          4897 20761.53
## 5       0.01                 5              5         1.00          5000 20997.68
## 6       0.10                 5              5         1.00           665 21277.84
## 7       0.10                 5              5         0.80           514 21277.90
## 8       0.01                 5             10         0.65          4987 21310.94
## 9       0.01                 5             15         0.80          4990 21456.17
## 10      0.01                 5             10         1.00          4960 21481.65</code></pre></div>
<p>These results help us to zoom into areas where we can refine our search. In practice, tuning is an iterative process so you would likely refine this search grid to analyze a search space around the top models. For example, I would likely search the following values in my next grid search:</p>
<ul>
<li>learning rate: 0.1, 0.05, 0.01, 0.005</li>
<li>interaction depth: 3, 5, 7</li>
</ul>
<p>along with the previously assessed values for <code>n.minobsinnode</code> and <code>bag.fraction</code>. Also, since we used nearly all 5000 trees when the learning rate was 0.01, I would increase this to ensure there are enough trees for learning rate <span class="math inline">\(=0.005\)</span>.</p>
<p>Once we have found our top model we train a model with those specific parameters. I’ll use the top model in our grid search and since the model converged at 4911 trees I train a model with that many trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># train GBM model</span>
gbm.fit.final &lt;-<span class="st"> </span><span class="kw">gbm</span>(
  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
  <span class="dt">distribution =</span> <span class="st">&quot;gaussian&quot;</span>,
  <span class="dt">data =</span> ames_train,
  <span class="dt">n.trees =</span> <span class="dv">4342</span>,
  <span class="dt">interaction.depth =</span> <span class="dv">5</span>,
  <span class="dt">shrinkage =</span> <span class="fl">0.01</span>,
  <span class="dt">n.minobsinnode =</span> <span class="dv">5</span>,
  <span class="dt">bag.fraction =</span> .<span class="dv">80</span>, 
  <span class="dt">train.fraction =</span> <span class="dv">1</span>,
  <span class="dt">n.cores =</span> <span class="ot">NULL</span>, <span class="co"># will use all cores by default</span>
  <span class="dt">verbose =</span> <span class="ot">FALSE</span>
  )  </code></pre></div>
</div>
<div id="gbm-regression-gbm-viz" class="section level4">
<h4><span class="header-section-number">5.4.1.3</span> Feature interpretation</h4>
<p>Similar to random forests, GBMs make no assumption regarding the linearity and monoticity of the predictor-response relationship. So as we did in the random forest chapter (Chapter <a href="random-forest.html#random-forest">4</a>) we can understand the relationship between the features and the response using variable importance plots and partial dependence plots.</p>
<div class="rmdtip">
<p>
Additional model interpretability approaches will be discussed in the <strong><em>Model Interpretability</em></strong> chapter.
</p>
</div>
<div id="gbm-regression-gbm-vip" class="section level5">
<h5><span class="header-section-number">5.4.1.3.1</span> Feature importance</h5>
<p>After re-running our final model we likely want to understand the variables that have the largest influence on our response variable. The <code>summary</code> method for <strong>gbm</strong> will output a data frame and a plot that shows the most influential variables. <code>cBars</code> allows you to adjust the number of variables to show (in order of influence). The default method for computing variable importance is with relative influence but your options include:</p>
<ol style="list-style-type: decimal">
<li><code>method = relative.influence</code>: At each split in each tree, <code>gbm</code> computes the improvement in the split-criterion (MSE for regression). <code>gbm</code> then averages the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important.</li>
<li><code>method = permutation.test.gbm</code>: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly “shaking up” of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">1</span>))

<span class="co"># relative influence approach</span>
<span class="kw">summary</span>(gbm.fit.final, <span class="dt">cBars =</span> <span class="dv">10</span>, <span class="dt">method =</span> relative.influence, <span class="dt">las =</span> <span class="dv">2</span>)

<span class="co"># permutation approach</span>
<span class="kw">summary</span>(gbm.fit.final, <span class="dt">cBars =</span> <span class="dv">10</span>, <span class="dt">method =</span> permutation.test.gbm, <span class="dt">las =</span> <span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gbm-gbm-vip-plot"></span>
<img src="images/gbm-gbm-vip.png" alt="Top 10 influential variables using the relative influence (left) and permutation (right) approach. We can see common themes among the top variables although in differing order." width="100%" height="100%" />
<p class="caption">
Figure 5.8: Top 10 influential variables using the relative influence (left) and permutation (right) approach. We can see common themes among the top variables although in differing order.
</p>
</div>
</div>
<div id="gbm-regression-gbm-pdp" class="section level5">
<h5><span class="header-section-number">5.4.1.3.2</span> Feature effects</h5>
<p>After the most relevant variables have been identified, we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves to better understand the relationship between the predictors and response. Here we plot two of the most influential variables (<code>Gr_Liv_Area</code> and <code>Overall_Qual</code>). We see that both predictor non-linear relationships with the sale price.</p>
<div class="rmdtip">
<p>
As in Chapter <span class="citation">(<span class="citeproc-not-found" data-reference-id="ref"><strong>???</strong></span>)</span>(random-forest), you can produce ICE curves by incorporating <code>ice = TRUE</code> and <code>center = TRUE</code> (for centered ICE curves).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p1 &lt;-<span class="st"> </span>gbm.fit.final <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">partial</span>(
    <span class="dt">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, 
    <span class="dt">n.trees =</span> gbm.fit.final<span class="op">$</span>n.trees, 
    <span class="dt">grid.resolution =</span> <span class="dv">50</span>
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_train) 

p2 &lt;-<span class="st"> </span>gbm.fit.final <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">partial</span>(
    <span class="dt">pred.var =</span> <span class="st">&quot;Overall_Qual&quot;</span>, 
    <span class="dt">n.trees =</span> gbm.fit.final<span class="op">$</span>n.trees, 
    <span class="dt">train =</span> <span class="kw">data.frame</span>(ames_train)
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">autoplot</span>() 

gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:pdp1"></span>
<img src="05-gradient-boosting-machines_files/figure-html/pdp1-1.png" alt="The mean predicted sale price as `Gr_Liv_Area` and `Overall_Qual` change in value." width="864" />
<p class="caption">
Figure 5.9: The mean predicted sale price as <code>Gr_Liv_Area</code> and <code>Overall_Qual</code> change in value.
</p>
</div>
</div>
</div>
<div id="gbm-regression-gbm-predict" class="section level4">
<h4><span class="header-section-number">5.4.1.4</span> Predicting</h4>
<p>Once you’ve found your optimal model, predicting new observations with the <strong>gbm</strong> model follows the same procedure as most R models. We simply use the <code>predict</code> function; however, we also need to supply the number of trees to use (see <code>?predict.gbm</code> for details). We see that our RMSE for our test set is right in line with the optimal RMSE obtained during our grid search and far better than any model to-date.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict values for test data</span>
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(gbm.fit.final, <span class="dt">n.trees =</span> gbm.fit.final<span class="op">$</span>n.trees, ames_test)

<span class="co"># results</span>
caret<span class="op">::</span><span class="kw">RMSE</span>(pred, ames_test<span class="op">$</span>Sale_Price)
## [1] 20859.01</code></pre></div>
</div>
</div>
<div id="regression-xgboost" class="section level3">
<h3><span class="header-section-number">5.4.2</span> xgboost</h3>
<p>The <strong>xgboost</strong> package only works with matrices that contain all numeric variables; consequently, we need to one hot encode our data. Throughout this book we’ve illustrated different ways to do this in R (i.e. <code>Matrix::sparse.model.matrix</code>, <code>caret::dummyVars</code>) but here we will use the <strong>vtreat</strong> package. <strong>vtreat</strong> is a robust package for data prep and helps to eliminate problems caused by missing values, novel categorical levels that appear in future data sets that were not in the training data, etc. However, <strong>vtreat</strong> is not very intuitive. I will not explain the functionalities but you can find more information <a href="https://arxiv.org/abs/1611.09477">here</a>, <a href="https://www.r-bloggers.com/a-demonstration-of-vtreat-data-preparation/">here</a>, and <a href="https://github.com/WinVector/vtreat">here</a>.</p>
<p>The following applies <strong>vtreat</strong> to one-hot encode the training and testing data sets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># variable names</span>
features &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">names</span>(ames_train), <span class="st">&quot;Sale_Price&quot;</span>)

<span class="co"># Create the treatment plan from the training data</span>
treatplan &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">designTreatmentsZ</span>(ames_train, features, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)

<span class="co"># Get the &quot;clean&quot; variable names from the scoreFrame</span>
new_vars &lt;-<span class="st"> </span>treatplan <span class="op">%&gt;%</span>
<span class="st">  </span>magrittr<span class="op">::</span><span class="kw">use_series</span>(scoreFrame) <span class="op">%&gt;%</span><span class="st">        </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">filter</span>(code <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;clean&quot;</span>, <span class="st">&quot;lev&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>magrittr<span class="op">::</span><span class="kw">use_series</span>(varName)     

<span class="co"># Prepare the training data</span>
features_train &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatplan, ames_train, <span class="dt">varRestriction =</span> new_vars) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()
response_train &lt;-<span class="st"> </span>ames_train<span class="op">$</span>Sale_Price

<span class="co"># Prepare the test data</span>
features_test &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatplan, ames_test, <span class="dt">varRestriction =</span> new_vars) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()
response_test &lt;-<span class="st"> </span>ames_test<span class="op">$</span>Sale_Price

<span class="co"># dimensions of one-hot encoded data</span>
<span class="kw">dim</span>(features_train)
## [1] 2054  211
<span class="kw">dim</span>(features_test)
## [1] 876 211</code></pre></div>
<div id="gbm-regression-xgb-basic" class="section level4">
<h4><span class="header-section-number">5.4.2.1</span> Basic implementation</h4>
<p><strong>xgboost</strong> provides different training functions (i.e. <code>xgb.train</code> which is just a wrapper for <code>xgboost</code>). However, to train an <strong>xgboost</strong> model we typically want to use <code>xgb.cv</code>, which incorporates cross-validation. The following trains a basic 5-fold cross validated XGBoost model with 1,000 trees. There are many parameters available in <code>xgb.cv</code> but the ones you have become more familiar with in this chapter include the following default values:</p>
<ul>
<li>learning rate (<code>eta</code>): 0.3</li>
<li>tree depth (<code>max_depth</code>): 6</li>
<li>minimum node size (<code>min_child_weight</code>): 1</li>
<li>percent of training data to sample for each tree (<code>subsample</code> –&gt; equivalent to <code>gbm</code>’s <code>bag.fraction</code>): 100%</li>
</ul>
<div class="rmdnote">
<p>
This model took nearly 2 minutes to run. The reason <strong>xgboost</strong> seems slower than <strong>gbm</strong> is since we one-hot encoded our data, <strong>xgboost</strong> is searching across 211 features where <strong>gbm</strong> uses non-one-hot encoded which means it was only searching across 80 features.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

xgb.fit1 &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(
  <span class="dt">data =</span> features_train,
  <span class="dt">label =</span> response_train,
  <span class="dt">nrounds =</span> <span class="dv">1000</span>,
  <span class="dt">nfold =</span> <span class="dv">5</span>,
  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,  <span class="co"># for regression models</span>
  <span class="dt">verbose =</span> <span class="dv">0</span>                <span class="co"># silent,</span>
)</code></pre></div>
<p>The <code>xgb.fit1</code> object contains lots of good information. In particular we can assess the <code>xgb.fit1$evaluation_log</code> to identify the minimum RMSE and the optimal number of trees for both the training data and the cross-validated error. We can see that the training error continues to decreasing through 980 trees where the RMSE nearly reaches 0; however, the cross validated error reaches a minimum RMSE of $26,758.30 with only 98 trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get number of trees that minimize error</span>
xgb.fit1<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(
    <span class="dt">ntrees.train =</span> <span class="kw">which.min</span>(train_rmse_mean),
    <span class="dt">rmse.train =</span> <span class="kw">min</span>(train_rmse_mean),
    <span class="dt">ntrees.test =</span> <span class="kw">which.min</span>(test_rmse_mean),
    <span class="dt">rmse.test =</span> <span class="kw">min</span>(test_rmse_mean),
  )
##   ntrees.train rmse.train ntrees.test rmse.test
## 1          980    0.05009          98   26758.3

<span class="co"># plot error vs number trees</span>
xgb.fit1<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidyr<span class="op">::</span><span class="kw">gather</span>(error, RMSE, train_rmse_mean, test_rmse_mean) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(iter, RMSE, <span class="dt">color =</span> error)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:xgb-gradient-descent-plot"></span>
<img src="images/xgb1_gradient_descent.png" alt="Training (blue) and cross-validation (red) error for each additional tree added to the GBM algorithm. The CV error is quickly minimized with 98 trees while the training error reduces to near zero over 980 trees." width="85%" height="85%" />
<p class="caption">
Figure 5.10: Training (blue) and cross-validation (red) error for each additional tree added to the GBM algorithm. The CV error is quickly minimized with 98 trees while the training error reduces to near zero over 980 trees.
</p>
</div>
<p>A nice feature provided by <code>xgb.cv</code> is early stopping. This allows us to tell the function to stop running if the cross validated error does not improve for <em>n</em> continuous trees. For example, the above model could be re-run with the following where we tell it stop if we see no improvement for 10 consecutive trees. This feature will help us speed up the tuning process in the next section.</p>
<div class="rmdtip">
<p>
This reduced our training time from 2 minutes to 8 seconds!
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

xgb.fit2 &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(
  <span class="dt">data =</span> features_train,
  <span class="dt">label =</span> response_train,
  <span class="dt">nrounds =</span> <span class="dv">1000</span>,
  <span class="dt">nfold =</span> <span class="dv">5</span>,
  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,  <span class="co"># for regression models</span>
  <span class="dt">verbose =</span> <span class="dv">0</span>,               <span class="co"># silent,</span>
  <span class="dt">early_stopping_rounds =</span> <span class="dv">10</span> <span class="co"># stop if no improvement for 10 consecutive trees</span>
)

<span class="co"># plot error vs number trees</span>
xgb.fit2<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidyr<span class="op">::</span><span class="kw">gather</span>(error, RMSE, train_rmse_mean, test_rmse_mean) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(iter, RMSE, <span class="dt">color =</span> error)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:xgb2-gradient-descent2-plot"></span>
<img src="images/xgb2_gradient_descent.png" alt="Early stopping allows us to stop training once we experience no additional improvement on our cross-validated error." width="80%" height="80%" />
<p class="caption">
Figure 5.11: Early stopping allows us to stop training once we experience no additional improvement on our cross-validated error.
</p>
</div>
</div>
<div id="gbm-regression-xgb-tune" class="section level4">
<h4><span class="header-section-number">5.4.2.2</span> Tuning</h4>
<p>To tune the XGBoost model we pass parameters as a list object to the <code>params</code> argument. The most common parameters include:</p>
<ul>
<li><code>eta</code>:controls the learning rate</li>
<li><code>max_depth</code>: tree depth</li>
<li><code>min_child_weight</code>: minimum number of observations required in each terminal node</li>
<li><code>subsample</code>: percent of training data to sample for each tree</li>
<li><code>colsample_bytrees</code>: percent of columns to sample from for each tree</li>
</ul>
<p>For example, if we wanted to specify specific values for these parameters we would extend the above model with the following parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create parameter list</span>
params &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">eta =</span> .<span class="dv">1</span>,
  <span class="dt">max_depth =</span> <span class="dv">5</span>,
  <span class="dt">min_child_weight =</span> <span class="dv">2</span>,
  <span class="dt">subsample =</span> .<span class="dv">8</span>,
  <span class="dt">colsample_bytree =</span> .<span class="dv">9</span>
  )

<span class="co"># reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># train model</span>
xgb.fit3 &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(
  <span class="dt">params =</span> params,
  <span class="dt">data =</span> features_train,
  <span class="dt">label =</span> response_train,
  <span class="dt">nrounds =</span> <span class="dv">1000</span>,
  <span class="dt">nfold =</span> <span class="dv">5</span>,
  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,  <span class="co"># for regression models</span>
  <span class="dt">verbose =</span> <span class="dv">0</span>,               <span class="co"># silent,</span>
  <span class="dt">early_stopping_rounds =</span> <span class="dv">10</span> <span class="co"># stop if no improvement for 10 consecutive trees</span>
)

<span class="co"># assess results</span>
xgb.fit3<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(
    <span class="dt">ntrees.train =</span> <span class="kw">which.min</span>(train_rmse_mean),
    <span class="dt">rmse.train =</span> <span class="kw">min</span>(train_rmse_mean),
    <span class="dt">ntrees.test =</span> <span class="kw">which.min</span>(test_rmse_mean),
    <span class="dt">rmse.test =</span> <span class="kw">min</span>(test_rmse_mean),
  )
##   ntrees.train rmse.train ntrees.test rmse.test
## 1          122   7954.668         112  24547.28</code></pre></div>
<p>To perform a large search grid, we can follow the same procedure we did with <strong>gbm</strong>. We create our hyperparameter search grid along with columns to dump our results in. Here, I create a pretty large search grid consisting of 108 different hyperparameter combinations to model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create hyperparameter grid</span>
hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
  <span class="dt">eta =</span> <span class="kw">c</span>(.<span class="dv">05</span>, .<span class="dv">1</span>, .<span class="dv">15</span>),
  <span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>),
  <span class="dt">min_child_weight =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>),
  <span class="dt">subsample =</span> <span class="kw">c</span>(.<span class="dv">65</span>, .<span class="dv">8</span>), 
  <span class="dt">colsample_bytree =</span> <span class="kw">c</span>(.<span class="dv">9</span>, <span class="dv">1</span>),
  <span class="dt">optimal_trees =</span> <span class="dv">0</span>,               <span class="co"># a place to dump results</span>
  <span class="dt">min_RMSE =</span> <span class="dv">0</span>                     <span class="co"># a place to dump results</span>
)

<span class="kw">nrow</span>(hyper_grid)
## [1] 108</code></pre></div>
<p>Now I apply the same <code>for</code> loop procedure to loop through and apply an <strong>xgboost</strong> model for each hyperparameter combination and dump the results in the <code>hyper_grid</code> data frame. Our minimum RMSE ($23,316.40) is a little higher than the <strong>gbm</strong> model, likely a result of one-hot encoding our data and how the models treat these dummy coded variables differently.</p>
<div class="rmdwarning">
<p>
This full search grid took <strong>34 minutes</strong> to run!
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># grid search </span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(hyper_grid)) {
  
  <span class="co"># create parameter list</span>
  params &lt;-<span class="st"> </span><span class="kw">list</span>(
    <span class="dt">eta =</span> hyper_grid<span class="op">$</span>eta[i],
    <span class="dt">max_depth =</span> hyper_grid<span class="op">$</span>max_depth[i],
    <span class="dt">min_child_weight =</span> hyper_grid<span class="op">$</span>min_child_weight[i],
    <span class="dt">subsample =</span> hyper_grid<span class="op">$</span>subsample[i],
    <span class="dt">colsample_bytree =</span> hyper_grid<span class="op">$</span>colsample_bytree[i]
  )
  
  <span class="co"># reproducibility</span>
  <span class="kw">set.seed</span>(<span class="dv">123</span>)
  
  <span class="co"># train model</span>
  xgb.tune &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(
    <span class="dt">params =</span> params,
    <span class="dt">data =</span> features_train,
    <span class="dt">label =</span> response_train,
    <span class="dt">nrounds =</span> <span class="dv">5000</span>,
    <span class="dt">nfold =</span> <span class="dv">5</span>,
    <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,  <span class="co"># for regression models</span>
    <span class="dt">verbose =</span> <span class="dv">0</span>,               <span class="co"># silent,</span>
    <span class="dt">early_stopping_rounds =</span> <span class="dv">10</span> <span class="co"># stop if no improvement for 10 consecutive trees</span>
  )
  
  <span class="co"># add min training error and trees to grid</span>
  hyper_grid<span class="op">$</span>optimal_trees[i] &lt;-<span class="st"> </span><span class="kw">which.min</span>(xgb.tune<span class="op">$</span>evaluation_log<span class="op">$</span>test_rmse_mean)
  hyper_grid<span class="op">$</span>min_RMSE[i] &lt;-<span class="st"> </span><span class="kw">min</span>(xgb.tune<span class="op">$</span>evaluation_log<span class="op">$</span>test_rmse_mean)
}

hyper_grid <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">arrange</span>(min_RMSE) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>(<span class="dv">10</span>)
##     eta max_depth min_child_weight subsample colsample_bytree optimal_trees min_RMSE
## 1  0.05         7                5      0.65              1.0           481 23316.40
## 2  0.05         5                5      0.65              1.0           355 23515.42
## 3  0.05         5                5      0.80              1.0           469 23856.31
## 4  0.05         5                5      0.65              0.9           312 23888.34
## 5  0.05         7                5      0.80              1.0           660 23904.30
## 6  0.15         3                5      0.65              1.0           227 23909.78
## 7  0.05         3                5      0.80              1.0           567 23967.56
## 8  0.05         3               10      0.80              1.0           665 23998.05
## 9  0.15         3                5      0.65              0.9           230 24001.86
## 10 0.05         5               10      0.80              1.0           624 24033.78</code></pre></div>
<p>After assessing the results you would likely perform a few more grid searches to hone in on the parameters that appear to influence the model the most. In fact, <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">here is a link</a> to a great blog post that discusses a strategic approach to tuning with <strong>xgboost</strong>. However, for brevity, we’ll just assume the top model in the above search is the globally optimal model. Once you’ve found the optimal model, we can fit our final model with <code>xgb.train</code> or <code>xgboost</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># parameter list</span>
params &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">eta =</span> <span class="fl">0.05</span>,
  <span class="dt">max_depth =</span> <span class="dv">7</span>,
  <span class="dt">min_child_weight =</span> <span class="dv">5</span>,
  <span class="dt">subsample =</span> <span class="fl">0.65</span>,
  <span class="dt">colsample_bytree =</span> <span class="dv">1</span>
)

<span class="co"># train final model</span>
xgb.fit.final &lt;-<span class="st"> </span><span class="kw">xgboost</span>(
  <span class="dt">params =</span> params,
  <span class="dt">data =</span> features_train,
  <span class="dt">label =</span> response_train,
  <span class="dt">nrounds =</span> <span class="dv">481</span>,
  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,
  <span class="dt">verbose =</span> <span class="dv">0</span>
)</code></pre></div>
</div>
<div id="gbm-regression-xgb-viz" class="section level4">
<h4><span class="header-section-number">5.4.2.3</span> Feature interpretation</h4>
<div id="gbm-regression-xgb-vip" class="section level5">
<h5><span class="header-section-number">5.4.2.3.1</span> Feature importance</h5>
<p><strong>xgboost</strong> provides built-in variable importance plotting. First, you need to create the importance matrix with <code>xgb.importance</code> and then feed this matrix into <code>xgb.plot.importance</code> (or <code>xgb.ggplot.importance</code> for a ggplot output). <strong>xgboost</strong> provides 3 variable importance measures:</p>
<ul>
<li>Gain: the relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model. This is synonymous with <strong>gbm</strong>’s <code>relative.influence</code>.</li>
<li>Cover: the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose <span class="math inline">\(feature_1\)</span> is used to decide the leaf node for 10, 5, and 2 observations in <span class="math inline">\(tree_1\)</span>, <span class="math inline">\(tree_2\)</span> and <span class="math inline">\(tree_3\)</span> respectively; then the metric will count cover for this feature as <span class="math inline">\(10+5+2 = 17\)</span> observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features’ cover metrics.</li>
<li>Frequency: the percentage representing the relative number of times a particular feature occurs in the trees of the model. In the above example, if <span class="math inline">\(feature_1\)</span> occurred in 2 splits, 1 split and 3 splits in each of <span class="math inline">\(tree_1\)</span>, <span class="math inline">\(tree_2\)</span> and <span class="math inline">\(tree_3\)</span>; then the weightage for <span class="math inline">\(feature_1\)</span> will be <span class="math inline">\(2+1+3 = 6\)</span>. The frequency for <span class="math inline">\(feature_1\)</span> is calculated as its percentage weight over weights of all features.</li>
</ul>
<div class="rmdnote">
<p>
The <code>xgb.ggplot.importance</code> plotting mechanism will also perform a cluster analysis on the features based on their importance scores. This becomes more useful when visualizing many features (i.e. 50) and you want to categorize them based on their importance.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create importance matrix</span>
importance_matrix &lt;-<span class="st"> </span><span class="kw">xgb.importance</span>(<span class="dt">model =</span> xgb.fit.final)

<span class="co"># variable importance plot</span>
p1 &lt;-<span class="st"> </span><span class="kw">xgb.ggplot.importance</span>(importance_matrix, <span class="dt">top_n =</span> <span class="dv">10</span>, <span class="dt">measure =</span> <span class="st">&quot;Gain&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Gain&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;bottom&quot;</span>)
p2 &lt;-<span class="st"> </span><span class="kw">xgb.ggplot.importance</span>(importance_matrix, <span class="dt">top_n =</span> <span class="dv">10</span>, <span class="dt">measure =</span> <span class="st">&quot;Cover&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Cover&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;bottom&quot;</span>)
p3 &lt;-<span class="st"> </span><span class="kw">xgb.ggplot.importance</span>(importance_matrix, <span class="dt">top_n =</span> <span class="dv">10</span>, <span class="dt">measure =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Frequency&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;bottom&quot;</span>)

gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, p3, <span class="dt">ncol =</span> <span class="dv">1</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:xgb-vip"></span>
<img src="05-gradient-boosting-machines_files/figure-html/xgb-vip-1.png" alt="Top 25 influential variables for our final __xgboost__ model based on the three different variable importance metrics." width="2400" />
<p class="caption">
Figure 5.12: Top 25 influential variables for our final <strong>xgboost</strong> model based on the three different variable importance metrics.
</p>
</div>
</div>
<div id="gbm-regression-xgb-pdp" class="section level5">
<h5><span class="header-section-number">5.4.2.3.2</span> Feature effects</h5>
<p>PDP and ICE plots work similarly to how we implemented them with <strong>gbm</strong>. The only difference is you need to incorporate the training data within the <code>partial</code> function since these data cannot be extracted directly from the model object. We see a similar non-linear relationship between <code>Gr_Liv_Area</code> and predicted sale price as we did with <strong>gbm</strong> and in the random forest models; however, note the unique dip right after <code>Gr_liv_Area</code> reaches 3,000 square feet. We saw this dip in the <strong>gbm</strong> model; however, it is a pattern that was not picked up on by the random forests models.</p>
<div class="rmdtip">
<p>
You do not need to supply the number of trees with <code>n.trees = xgb.fit.final$niter</code>; however, when supplying a cross-validated model where the optimal number of trees may be less than the total number of trees ran, then you will want to supply the optimal number of trees to the <code>n.trees</code> paramater.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pdp &lt;-<span class="st"> </span>xgb.fit.final <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">partial</span>(
    <span class="dt">pred.var =</span> <span class="st">&quot;Gr_Liv_Area_clean&quot;</span>, 
    <span class="dt">n.trees =</span> xgb.fit.final<span class="op">$</span>niter, 
    <span class="dt">grid.resolution =</span> <span class="dv">50</span>, 
    <span class="dt">train =</span> features_train
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> features_train) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;PDP&quot;</span>)

ice &lt;-<span class="st"> </span>xgb.fit.final <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">partial</span>(
    <span class="dt">pred.var =</span> <span class="st">&quot;Gr_Liv_Area_clean&quot;</span>, 
    <span class="dt">n.trees =</span> xgb.fit.final<span class="op">$</span>niter, 
    <span class="dt">grid.resolution =</span> <span class="dv">100</span>, 
    <span class="dt">train =</span> features_train, 
    <span class="dt">ice =</span> <span class="ot">TRUE</span>
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> features_train, <span class="dt">alpha =</span> .<span class="dv">05</span>, <span class="dt">center =</span> <span class="ot">TRUE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;ICE&quot;</span>)

gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(pdp, ice, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:xgb-pdp-ice"></span>
<img src="05-gradient-boosting-machines_files/figure-html/xgb-pdp-ice-1.png" alt="The mean predicted sale price as the above ground living area increases." width="864" />
<p class="caption">
Figure 5.13: The mean predicted sale price as the above ground living area increases.
</p>
</div>
</div>
</div>
<div id="gbm-regression-gbm-predict" class="section level4">
<h4><span class="header-section-number">5.4.2.4</span> Predicting</h4>
<p>Lastly, we use <code>predict</code> to predict on new observations; however, unlike <strong>gbm</strong> we do not need to provide the number of trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict values for test data</span>
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(xgb.fit.final, features_test)

<span class="co"># test set results</span>
caret<span class="op">::</span><span class="kw">RMSE</span>(pred, response_test)
## [1] 23454.51</code></pre></div>
</div>
</div>
<div id="gbm-regression-h2o" class="section level3">
<h3><span class="header-section-number">5.4.3</span> h2o</h3>
<p>Lets go ahead and start up h2o:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">h2o.no_progress</span>()
<span class="kw">h2o.init</span>(<span class="dt">max_mem_size =</span> <span class="st">&quot;5g&quot;</span>)
## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpa5wE8r/h2o_bradboehmke_started_from_r.out
##     /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpa5wE8r/h2o_bradboehmke_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: .. Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         2 seconds 476 milliseconds 
##     H2O cluster timezone:       America/New_York 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.18.0.11 
##     H2O cluster version age:    2 months and 18 days  
##     H2O cluster name:           H2O_started_from_R_bradboehmke_qdv114 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   4.44 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.1 (2018-07-02)</code></pre></div>
<div id="gbm-regression-h2o-basic" class="section level4">
<h4><span class="header-section-number">5.4.3.1</span> Basic implementation</h4>
<p><code>h2o.gbm</code> allows us to perform a GBM with H2O. However, prior to running our initial model we need to convert our training data to an h2o object. By default, <code>h2o.gbm</code> applies a GBM model with the following parameters:</p>
<ul>
<li>number of trees (<code>ntrees</code>): 50</li>
<li>learning rate (<code>learn_rate</code>): 0.1</li>
<li>tree depth (<code>max_depth</code>): 5</li>
<li>minimum observations in a terminal node (<code>min_rows</code>): 10</li>
<li>no sampling of observations or columns</li>
</ul>
<p>Since we are performing a 5-fold cross-validation, the output reports results for both our training set (<span class="math inline">\(RMSE = 12539.86\)</span>) and validation set (<span class="math inline">\(RMSE = 24654.047\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create feature names</span>
y &lt;-<span class="st"> &quot;Sale_Price&quot;</span>
x &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">names</span>(ames_train), y)

<span class="co"># turn training set into h2o object</span>
train.h2o &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(ames_train)

<span class="co"># training basic GBM model with defaults</span>
h2o.fit1 &lt;-<span class="st"> </span><span class="kw">h2o.gbm</span>(
  <span class="dt">x =</span> x,
  <span class="dt">y =</span> y,
  <span class="dt">training_frame =</span> train.h2o,
  <span class="dt">nfolds =</span> <span class="dv">5</span>   <span class="co"># performs 5 fold cross validation</span>
)

<span class="co"># assess model results</span>
h2o.fit1
## Model Details:
## ==============
## 
## H2ORegressionModel: gbm
## Model ID:  GBM_model_R_1533927247702_1 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
## 1              50                       50               17591         5         5    5.00000          9         31    22.96000
## 
## 
## H2ORegressionMetrics: gbm
## ** Reported on training data. **
## 
## MSE:  157248046
## RMSE:  12539.86
## MAE:  8988.278
## RMSLE:  0.08190755
## Mean Residual Deviance :  157248046
## 
## 
## 
## H2ORegressionMetrics: gbm
## ** Reported on cross-validation data. **
## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  612660925
## RMSE:  24751.99
## MAE:  15485.9
## RMSLE:  0.1369165
## Mean Residual Deviance :  612660925
## 
## 
## Cross-Validation Metrics Summary: 
##                               mean          sd cv_1_valid  cv_2_valid  cv_3_valid  cv_4_valid  cv_5_valid
## mae                     15465.6455    403.8833  15899.954   14460.245  16051.8125    15667.94   15248.277
## mean_residual_deviance 6.1167661E8 6.4985276E7 6.851456E8 4.3496512E8 6.0995507E8 6.7103411E8 6.5728301E8
## mse                    6.1167661E8 6.4985276E7 6.851456E8 4.3496512E8 6.0995507E8 6.7103411E8 6.5728301E8
## r2                       0.9020621 0.008838167  0.8946199   0.9245291  0.89047533   0.8939862  0.90669996
## residual_deviance      6.1167661E8 6.4985276E7 6.851456E8 4.3496512E8 6.0995507E8 6.7103411E8 6.5728301E8
## rmse                     24654.047   1388.2732  26175.287   20855.818    24697.27   25904.326   25637.531
## rmsle                   0.13682812 0.009084022 0.15625001  0.12442712  0.12835869  0.12703055  0.14807428</code></pre></div>
<p>Similar to <strong>xgboost</strong>, we can incorporate automated stopping so that we can crank up the number of trees but terminate training once model improvement decreases or stops. There is also an option to terminate training after so much time has passed (see <code>max_runtime_secs</code>). In this example, I train a default model with 5,000 trees but stop training after 10 consecutive trees have no improvement on the cross-validated error. In this case, training stops after 2623 trees and has a cross-validated RMSE of $23,441.46.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># training basic GBM model with defaults</span>
h2o.fit2 &lt;-<span class="st"> </span><span class="kw">h2o.gbm</span>(
  <span class="dt">x =</span> x,
  <span class="dt">y =</span> y,
  <span class="dt">training_frame =</span> train.h2o,
  <span class="dt">nfolds =</span> <span class="dv">5</span>,
  <span class="dt">ntrees =</span> <span class="dv">5000</span>,
  <span class="dt">stopping_rounds =</span> <span class="dv">10</span>,
  <span class="dt">stopping_tolerance =</span> <span class="dv">0</span>,
  <span class="dt">seed =</span> <span class="dv">123</span>
)

<span class="co"># model stopped after xx trees</span>
h2o.fit2<span class="op">@</span>parameters<span class="op">$</span>ntrees
## [1] 2623

<span class="co"># cross validated RMSE</span>
<span class="kw">h2o.rmse</span>(h2o.fit2, <span class="dt">xval =</span> <span class="ot">TRUE</span>)
## [1] 23441.46</code></pre></div>
</div>
<div id="gbm-regression-h2o-tune" class="section level4">
<h4><span class="header-section-number">5.4.3.2</span> Tuning</h4>
<p>H2O provides <strong><em>many</em></strong> parameters that can be adjusted. It is well worth your time to check out the available documentation at <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#gbm-tuning-guide">H2O.ai</a>. For this chapter, we’ll focus on the more common hyperparameters that are adjusted. This includes:</p>
<ul>
<li>Tree complexity:
<ul>
<li><code>ntrees</code>: number of trees to train</li>
<li><code>max_depth</code>: depth of each tree</li>
<li><code>min_rows</code>: Fewest observations allowed in a terminal node</li>
</ul></li>
<li>Learning rate:
<ul>
<li><code>learn_rate</code>: rate to descend the loss function gradient</li>
<li><code>learn_rate_annealing</code>: allows you to have a high initial <code>learn_rate</code>, then gradually reduce as trees are added (speeds up training).</li>
</ul></li>
<li>Adding stochastic nature:
<ul>
<li><code>sample_rate</code>: row sample rate per tree</li>
<li><code>col_sample_rate</code>: column sample rate per tree (synonymous with <code>xgboost</code>’s <code>colsample_bytree</code>)</li>
</ul></li>
</ul>
<p>Note that there are parameters that control how categorical and continuous variables are encoded, binned, and split. The defaults tend to perform quite well but we have been able to gain small improvements in certain circumstances by adjusting these. We will not cover them but they are worth reviewing.</p>
<p>To perform grid search tuning with H2O we can perform either a full or random discrete grid search as discussed in Section <a href="random-forest.html#rf-h2o-regression-tune">4.4.2.2</a>.</p>
<div id="gbm-regression-h2o-tune-full" class="section level5">
<h5><span class="header-section-number">5.4.3.2.1</span> Full grid search</h5>
<p>We’ll start with a full grid search. However, to speed up training with <strong>h2o</strong> I’ll use a validation set rather than perform k-fold cross validation. The following creates a hyperparameter grid consisting of 216 hyperparameter combinations. We apply <code>h2o.grid</code> to perform a grid search while also incorporating stopping parameters to reduce training time.</p>
<p>We see that the worst model had an RMSE of $34,142.31 (<span class="math inline">\(\sqrt{476228672}\)</span>) and the best had an RMSE of $21,822.66 (<span class="math inline">\(\sqrt{1165697554}\)</span>). A few characteristics pop out when we assess the results - models that search across a sample of columns, include more interactions via deeper trees, and allow nodes with single observations tend to perform best.</p>
<div class="rmdtip">
<p>
This full grid search took <strong>18 minutes</strong>.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create training &amp; validation sets</span>
split &lt;-<span class="st"> </span><span class="kw">h2o.splitFrame</span>(train.h2o, <span class="dt">ratios =</span> <span class="fl">0.75</span>)
train &lt;-<span class="st"> </span>split[[<span class="dv">1</span>]]
valid &lt;-<span class="st"> </span>split[[<span class="dv">2</span>]]

<span class="co"># create hyperparameter grid</span>
hyper_grid &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>),
  <span class="dt">min_rows =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>),
  <span class="dt">learn_rate =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.15</span>),
  <span class="dt">learn_rate_annealing =</span> <span class="kw">c</span>(.<span class="dv">99</span>, <span class="dv">1</span>),
  <span class="dt">sample_rate =</span> <span class="kw">c</span>(.<span class="dv">75</span>, <span class="dv">1</span>),
  <span class="dt">col_sample_rate =</span> <span class="kw">c</span>(.<span class="dv">9</span>, <span class="dv">1</span>)
)

<span class="co"># perform grid search </span>
grid &lt;-<span class="st"> </span><span class="kw">h2o.grid</span>(
  <span class="dt">algorithm =</span> <span class="st">&quot;gbm&quot;</span>,
  <span class="dt">grid_id =</span> <span class="st">&quot;gbm_grid1&quot;</span>,
  <span class="dt">x =</span> x, 
  <span class="dt">y =</span> y, 
  <span class="dt">training_frame =</span> train,
  <span class="dt">validation_frame =</span> valid,
  <span class="dt">hyper_params =</span> hyper_grid,
  <span class="dt">ntrees =</span> <span class="dv">5000</span>,
  <span class="dt">stopping_rounds =</span> <span class="dv">10</span>,
  <span class="dt">stopping_tolerance =</span> <span class="dv">0</span>,
  <span class="dt">seed =</span> <span class="dv">123</span>
  )

<span class="co"># collect the results and sort by our model performance metric of choice</span>
grid_perf &lt;-<span class="st"> </span><span class="kw">h2o.getGrid</span>(
  <span class="dt">grid_id =</span> <span class="st">&quot;gbm_grid1&quot;</span>, 
  <span class="dt">sort_by =</span> <span class="st">&quot;mse&quot;</span>, 
  <span class="dt">decreasing =</span> <span class="ot">FALSE</span>
  )
grid_perf
## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid1 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 216 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 1             0.9       0.05                  1.0         5      1.0         1.0 gbm_grid1_model_138  4.762286724006572E8
## 2             0.9       0.05                  1.0         5      1.0        0.75  gbm_grid1_model_30 4.9950455156501746E8
## 3             0.9        0.1                 0.99         5      1.0         1.0 gbm_grid1_model_134   5.04407597702603E8
## 4             0.9       0.15                  1.0         5      1.0         1.0 gbm_grid1_model_142 5.1283047945094573E8
## 5             0.9       0.05                 0.99         5      1.0         1.0 gbm_grid1_model_132 5.1307290689450026E8
## 
## ---
##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 211             1.0       0.05                 0.99         1      5.0        0.75  gbm_grid1_model_37 1.1602102219725711E9
## 212             1.0       0.05                 0.99         1     10.0        0.75  gbm_grid1_model_73 1.1620871092432442E9
## 213             0.9       0.05                 0.99         1      5.0         1.0 gbm_grid1_model_144 1.1631655136872623E9
## 214             1.0       0.05                 0.99         1      5.0         1.0 gbm_grid1_model_145  1.164083893251335E9
## 215             0.9       0.05                 0.99         1     10.0         1.0 gbm_grid1_model_180 1.1653298042199028E9
## 216             1.0       0.05                 0.99         1     10.0         1.0 gbm_grid1_model_181 1.1656975535614166E9</code></pre></div>
<p>We can check out more details of the best performing model. We can see that our best model used all 5000 trees, thus a future grid search may want to increase the number of trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Grab the model_id for the top model, chosen by validation error</span>
best_model_id &lt;-<span class="st"> </span>grid_perf<span class="op">@</span>model_ids[[<span class="dv">1</span>]]
best_model &lt;-<span class="st"> </span><span class="kw">h2o.getModel</span>(best_model_id)

best_model<span class="op">@</span>parameters<span class="op">$</span>ntrees
## [1] 5000

<span class="co"># Now let’s get performance metrics on the best model</span>
<span class="kw">h2o.performance</span>(<span class="dt">model =</span> best_model, <span class="dt">valid =</span> <span class="ot">TRUE</span>)
## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  476228672
## RMSE:  21822.66
## MAE:  13972.88
## RMSLE:  0.1341252
## Mean Residual Deviance :  476228672</code></pre></div>
</div>
<div id="gbm-regression-h2o-tune-random" class="section level5">
<h5><span class="header-section-number">5.4.3.2.2</span> Random discrete grid search</h5>
<p>As discussed in Section <a href="random-forest.html#rf-h2o-regression-tune-random">4.4.2.2.2</a>, <strong>h2o</strong> also allows us to perform a random grid search that allows early stopping. We can build onto the previous results and perform a random discrete grid. This time I increase the <code>max_depth</code>, refine the <code>min_rows</code>, and allow for 80% <code>col_sample_rate</code>. I keep all hyperparameter search criteria the same. I also add a search criteria that stops the grid search if none of the last 10 models have managed to have a 0.5% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 1800 seconds (30 minutes). In this example, our search went for the entire 90 minutes and evaluated 57 of the 216 potential models.</p>
<p>In the body of the grid search, notice that I increased the trees to 10,000 since our best model used all 5,000 but I also include a stopping mechanism so that the model quits adding trees once no improvement is made in the validation RMSE.</p>
<div class="rmdtip">
<p>
This random grid search took <strong>30 minutes</strong>. It only assessed a third of the number of models the full grid search did but keep in mind that this grid search was assessing models with very low learning rates, which can take a long time.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># refined hyperparameter grid</span>
hyper_grid &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>),
  <span class="dt">min_rows =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>),
  <span class="dt">learn_rate =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.15</span>),
  <span class="dt">learn_rate_annealing =</span> <span class="kw">c</span>(.<span class="dv">99</span>, <span class="dv">1</span>),
  <span class="dt">sample_rate =</span> <span class="kw">c</span>(.<span class="dv">75</span>, <span class="dv">1</span>),
  <span class="dt">col_sample_rate =</span> <span class="kw">c</span>(.<span class="dv">8</span>, .<span class="dv">9</span>)
)

<span class="co"># random grid search criteria</span>
search_criteria &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">strategy =</span> <span class="st">&quot;RandomDiscrete&quot;</span>,
  <span class="dt">stopping_metric =</span> <span class="st">&quot;mse&quot;</span>,
  <span class="dt">stopping_tolerance =</span> <span class="fl">0.005</span>,
  <span class="dt">stopping_rounds =</span> <span class="dv">10</span>,
  <span class="dt">max_runtime_secs =</span> <span class="dv">60</span><span class="op">*</span><span class="dv">30</span>
  )

<span class="co"># perform grid search </span>
grid &lt;-<span class="st"> </span><span class="kw">h2o.grid</span>(
  <span class="dt">algorithm =</span> <span class="st">&quot;gbm&quot;</span>,
  <span class="dt">grid_id =</span> <span class="st">&quot;gbm_grid2&quot;</span>,
  <span class="dt">x =</span> x, 
  <span class="dt">y =</span> y, 
  <span class="dt">training_frame =</span> train,
  <span class="dt">validation_frame =</span> valid,
  <span class="dt">hyper_params =</span> hyper_grid,
  <span class="dt">search_criteria =</span> search_criteria, <span class="co"># add search criteria</span>
  <span class="dt">ntrees =</span> <span class="dv">10000</span>,
  <span class="dt">stopping_rounds =</span> <span class="dv">10</span>,
  <span class="dt">stopping_tolerance =</span> <span class="dv">0</span>,
  <span class="dt">seed =</span> <span class="dv">123</span>
  )

<span class="co"># collect the results and sort by our model performance metric of choice</span>
grid_perf &lt;-<span class="st"> </span><span class="kw">h2o.getGrid</span>(
  <span class="dt">grid_id =</span> <span class="st">&quot;gbm_grid2&quot;</span>, 
  <span class="dt">sort_by =</span> <span class="st">&quot;mse&quot;</span>, 
  <span class="dt">decreasing =</span> <span class="ot">FALSE</span>
  )
grid_perf
## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid2 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 68 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate          model_ids                  mse
## 1             0.9       0.05                  1.0         5      1.0         1.0 gbm_grid2_model_50 4.7624213765845805E8
## 2             0.8        0.1                 0.99         5      5.0        0.75 gbm_grid2_model_40 4.9473498408073145E8
## 3             0.8        0.1                 0.99         5      1.0        0.75 gbm_grid2_model_34  4.989151160193848E8
## 4             0.9       0.05                  1.0         5      1.0        0.75  gbm_grid2_model_2   4.99534198008744E8
## 5             0.9        0.1                 0.99         7      1.0         1.0 gbm_grid2_model_46 5.1068183500740755E8
## 
## ---
##    col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate          model_ids                 mse
## 63             0.9        0.1                 0.99         9      1.0         1.0 gbm_grid2_model_19 6.888828473137908E8
## 64             0.9       0.05                  1.0         9      5.0         1.0 gbm_grid2_model_66 6.978688028588266E8
## 65             0.9       0.05                 0.99         9      3.0         1.0 gbm_grid2_model_53 7.120703549234108E8
## 66             0.8       0.15                  1.0         9      1.0        0.75 gbm_grid2_model_64 7.175292869435712E8
## 67             0.9        0.1                 0.99         9      3.0         1.0  gbm_grid2_model_3 7.793274661905156E8
## 68             0.8       0.15                  1.0         7      5.0        0.75 gbm_grid2_model_67 8.279874389570463E8</code></pre></div>
<p>In this example, the best model obtained a cross-validated RMSE of $21,822.97. So although we assessed only 31% of the total models we were able to find a model that was better than our initial full grid search.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Grab the model_id for the top model, chosen by validation error</span>
best_model_id &lt;-<span class="st"> </span>grid_perf<span class="op">@</span>model_ids[[<span class="dv">1</span>]]
best_model &lt;-<span class="st"> </span><span class="kw">h2o.getModel</span>(best_model_id)

<span class="co"># Now let’s get performance metrics on the best model</span>
<span class="kw">h2o.performance</span>(<span class="dt">model =</span> best_model, <span class="dt">valid =</span> <span class="ot">TRUE</span>)
## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  476242138
## RMSE:  21822.97
## MAE:  13972.9
## RMSLE:  0.1341249
## Mean Residual Deviance :  476242138</code></pre></div>
<p>Once we’ve found our preferred model, we’ll go ahead and retrain a new model with the full training data. I’ll use the best model from the full grid search and perform a 5-fold CV to get a robust estimate of the expected error. I crank up the number of trees just to make sure we find a global minimum.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># train final model</span>
h2o.final &lt;-<span class="st"> </span><span class="kw">h2o.gbm</span>(
  <span class="dt">x =</span> x,
  <span class="dt">y =</span> y,
  <span class="dt">training_frame =</span> train.h2o,
  <span class="dt">nfolds =</span> <span class="dv">5</span>,
  <span class="dt">ntrees =</span> <span class="dv">10000</span>,
  <span class="dt">learn_rate =</span> <span class="fl">0.05</span>,
  <span class="dt">learn_rate_annealing =</span> <span class="dv">1</span>,
  <span class="dt">max_depth =</span> <span class="dv">5</span>,
  <span class="dt">min_rows =</span> <span class="dv">5</span>,
  <span class="dt">sample_rate =</span> <span class="dv">1</span>,
  <span class="dt">col_sample_rate =</span> <span class="fl">0.9</span>,
  <span class="dt">stopping_rounds =</span> <span class="dv">10</span>,
  <span class="dt">stopping_tolerance =</span> <span class="dv">0</span>,
  <span class="dt">seed =</span> <span class="dv">123</span>
)

<span class="co"># model stopped after xx trees</span>
h2o.final<span class="op">@</span>parameters<span class="op">$</span>ntrees
## [1] 5526

<span class="co"># cross validated RMSE</span>
<span class="kw">h2o.rmse</span>(h2o.final, <span class="dt">xval =</span> <span class="ot">TRUE</span>)
## [1] 23553.76</code></pre></div>
</div>
</div>
<div id="gbm-regression-h2o-viz" class="section level4">
<h4><span class="header-section-number">5.4.3.3</span> Feature interpretation</h4>
<div id="gbm-regression-h2o-vip" class="section level5">
<h5><span class="header-section-number">5.4.3.3.1</span> Feature importance</h5>
<p>Looking at the top 25 most important features, we see many of the same predictors as we have with the other GBM implementations and also with the random forest approaches.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vip<span class="op">::</span><span class="kw">vip</span>(h2o.final, <span class="dt">num_features =</span> <span class="dv">25</span>, <span class="dt">bar =</span> <span class="ot">FALSE</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:h2o-vip2"></span>
<img src="05-gradient-boosting-machines_files/figure-html/h2o-vip2-1.png" alt="Top 25 most influential predictors based on impurity." width="672" />
<p class="caption">
Figure 5.14: Top 25 most influential predictors based on impurity.
</p>
</div>
</div>
<div id="gbm-regression-h2o-pdp" class="section level5">
<h5><span class="header-section-number">5.4.3.3.2</span> Feature effects</h5>
<p>PDP and ICE plots work similarly to how we implemented them with the <strong>h2o</strong> approaches for random forests. Figure <a href="gradient-boosting-machines.html#fig:gbm-regression-h2o-pdp-ice">5.15</a> illustrates the same non-linear relationship between <code>Gr_Liv_Area</code> and predicted sale price that we have been seeing with other GBM and random forest implementations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># build custom prediction function</span>
pfun &lt;-<span class="st"> </span><span class="cf">function</span>(object, newdata) {
  <span class="kw">as.data.frame</span>(<span class="kw">predict</span>(object, <span class="dt">newdata =</span> <span class="kw">as.h2o</span>(newdata)))[[1L]]
}

<span class="co"># compute ICE curves </span>
<span class="kw">partial</span>(
  h2o.final, 
  <span class="dt">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, 
  <span class="dt">train =</span> ames_train,
  <span class="dt">pred.fun =</span> pfun,
  <span class="dt">grid.resolution =</span> <span class="dv">20</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_train, <span class="dt">alpha =</span> <span class="fl">0.05</span>, <span class="dt">center =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Centered ICE curves&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gbm-regression-h2o-pdp-ice"></span>
<img src="05-gradient-boosting-machines_files/figure-html/gbm-regression-h2o-pdp-ice-1.png" alt="ICE curves illustrating the non-linear relationship between above ground square footage (`Gr_Liv_Area`) and predicted sale price." width="480" />
<p class="caption">
Figure 5.15: ICE curves illustrating the non-linear relationship between above ground square footage (<code>Gr_Liv_Area</code>) and predicted sale price.
</p>
</div>
</div>
</div>
<div id="gbm-regression-h2o-predict" class="section level4">
<h4><span class="header-section-number">5.4.3.4</span> Predicting</h4>
<p>Lastly, we use <code>h2o.predict</code> or <code>predict</code> to predict on new observations and we can also evaluate the performance of our model on our test set easily with <code>h2o.performance</code>. Results are quite similar to both <strong>gmb</strong> and <strong>xgboost</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert test set to h2o object</span>
test.h2o &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(ames_test)

<span class="co"># evaluate performance on new data</span>
<span class="kw">h2o.performance</span>(<span class="dt">model =</span> h2o.final, <span class="dt">newdata =</span> test.h2o)
## H2ORegressionMetrics: gbm
## 
## MSE:  501872966
## RMSE:  22402.52
## MAE:  13844.76
## RMSLE:  0.1127162
## Mean Residual Deviance :  501872966

<span class="co"># predict with h2o.predict</span>
<span class="kw">h2o.predict</span>(h2o.final, <span class="dt">newdata =</span> test.h2o)
##    predict
## 1 122831.1
## 2 179027.7
## 3 228672.4
## 4 261521.0
## 5 407383.8
## 6 384198.8
## 
## [876 rows x 1 column]

<span class="co"># predict values with predict</span>
<span class="kw">predict</span>(h2o.final, test.h2o)
##    predict
## 1 122831.1
## 2 179027.7
## 3 228672.4
## 4 261521.0
## 5 407383.8
## 6 384198.8
## 
## [876 rows x 1 column]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># shut down h2o</span>
<span class="kw">h2o.shutdown</span>(<span class="dt">prompt =</span> <span class="ot">FALSE</span>)
## [1] TRUE</code></pre></div>
</div>
</div>
</div>
<div id="gbm-binary-classification" class="section level2">
<h2><span class="header-section-number">5.5</span> Implementation: Binary Classification</h2>
<div id="gbm" class="section level3">
<h3><span class="header-section-number">5.5.1</span> gbm</h3>
<p>Coming soon!</p>
</div>
<div id="xgboost" class="section level3">
<h3><span class="header-section-number">5.5.2</span> xgboost</h3>
<p>Coming soon!</p>
</div>
<div id="h2o" class="section level3">
<h3><span class="header-section-number">5.5.3</span> h2o</h3>
<p>Coming soon!</p>
</div>
</div>
<div id="gbm-multi-classification" class="section level2">
<h2><span class="header-section-number">5.6</span> Implementation: Multinomial Classification</h2>
<div id="gbm-1" class="section level3">
<h3><span class="header-section-number">5.6.1</span> gbm</h3>
<p>Coming soon!</p>
</div>
<div id="xgboost-1" class="section level3">
<h3><span class="header-section-number">5.6.2</span> xgboost</h3>
<p>Coming soon!</p>
</div>
<div id="h2o-1" class="section level3">
<h3><span class="header-section-number">5.6.3</span> h2o</h3>
<p>Coming soon!</p>
</div>
</div>
<div id="learning-more" class="section level2">
<h2><span class="header-section-number">5.7</span> Learning More</h2>
<p>GBMs are one of the most powerful ensemble algorithms that are often first-in-class with predictive accuracy. Although they are less intuitive and more computationally demanding than many other machine learning algorithms, they are essential to have in your toolbox. To learn more I would start with the following resources:</p>
<p><strong>Traditional book resources:</strong></p>
<ul>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a></li>
<li><a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a></li>
<li><a href="https://www.amazon.com/Computer-Age-Statistical-Inference-Mathematical/dp/1107149894">Computer Age Statistical Inference</a></li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a></li>
</ul>
<p><strong>Alternative online resources:</strong></p>
<ul>
<li><a href="https://koalaverse.github.io/machine-learning-in-R/%20//www.youtube.com/watch?v=wPqtzj5VZus&amp;index=16&amp;list=PLNtMya54qvOFQhSZ4IKKXRbMkyL%20Mn0caa">Trevor Hastie - Gradient Boosting &amp; Random Forests at H2O World 2014</a> (YouTube)</li>
<li><a href="http://www.slideshare.net/0xdata/gbm-27891077">Trevor Hastie - Data Science of GBM (2013)</a> (slides)</li>
<li><a href="https://www.youtube.com/watch?v=9wn1f-30_ZY">Mark Landry - Gradient Boosting Method and Random Forest at H2O World 2015</a> (YouTube)</li>
<li><a href="https://www.youtube.com/watch?v=IXZKgIsZRm0">Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn at PyData London 2014</a> (YouTube)</li>
<li><a href="http://journal.frontiersin.org/article/10.3389/fnbot.2013.00021/full">Alexey Natekin1 and Alois Knoll - Gradient boosting machines, a tutorial</a> (blog post)</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-gbm">
<p>others, Greg Ridgeway with contributions from. 2017. <em>Gbm: Generalized Boosted Regression Models</em>. <a href="https://CRAN.R-project.org/package=gbm" class="uri">https://CRAN.R-project.org/package=gbm</a>.</p>
</div>
<div id="ref-R-xgboost">
<p>Chen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2018. <em>Xgboost: Extreme Gradient Boosting</em>. <a href="https://CRAN.R-project.org/package=xgboost" class="uri">https://CRAN.R-project.org/package=xgboost</a>.</p>
</div>
<div id="ref-geron2017hands">
<p>Géron, Aurélien. 2017. <em>Hands-on Machine Learning with Scikit-Learn and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems</em>. “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-esl">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA:</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>The features highlighted for each package were originally identified by Erin LeDell in her <a href="https://github.com/ledell/useR-machine-learning-tutorial">useR! 2016 tutorial</a>.<a href="gradient-boosting-machines.html#fnref6">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-forest.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-gradient-boosting-machines.rmd",
"text": "Edit"
},
"download": ["hands-on-machine-learning-with-R.pdf", "hands-on-machine-learning-with-R.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
