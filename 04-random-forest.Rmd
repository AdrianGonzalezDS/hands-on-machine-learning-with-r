
# Random Forest {#random-forest}

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```

<img src="images/RF_icon.jpg"  style="float:right; margin: 0px 0px 0px 10px; width: 22%; height: 22%;" />
___Random forests___ are a modification of decision trees and bagging that builds a large collection of *de-correlated* trees to reduce overfitting (aka variance). They have become a very popular "out-of-the-box" learning algorithm that enjoys good predictive performance and easy hyperparameter tuning. Many modern implementations of random forests algorithms exist; however, Leo Breiman's algorithm [@breiman2001random] has largely become the authoritative procedure. This chapter will cover the fundamentals of random forests.

## Prerequisites {#rf-requirements}

```{block, type = "rmdwarning"}
Any tutorial on random forests (RF) should also include a review of decision trees, as these are models that are ensembled together to create the random forest model – or put another way, the “trees that comprise the forest.” Much of the complexity and detail of the random forest algorithm occurs within the individual decision trees and therefore it’s important to understand decision trees to understand the RF algorithm as a whole. Therefore, before proceeding, it is recommended that you read through [http://uc-r.github.io/regression_trees](http://uc-r.github.io/regression_trees) prior to continuing.
```

This chapter leverages the following packages. Some of these packages play a supporting role; however, the emphasis is on how to implement random forests with the `ranger` [@R-ranger] and `h2o` packages.

```{r pkg-req, cache=FALSE}
library(rsample)  # data splitting 
library(ranger)   # a fast c++ implementation of the random forest algorithm
library(h2o)      # a java-based platform
library(vip)      # visualize feature importance 
library(pdp)      # visualize feature effects
library(ggplot2)  # supports visualization
library(dplyr)    # basic data transformation
```


## Advantages & Disadvantages {#rf-proscons}

__Advantages:__

- Typically have very good performance.
- Remarkably good "out-of-the box" - very little tuning required.
- Built-in validation set - don't need to sacrifice data for extra validation.
- Does not overfit.
- No data pre-processing required - often works great with categorical and numerical values as is.
- Robust to outliers.
- Handles missing data - imputation not required.
- Provide automatic feature selection.

__Disadvantages:__

- Can become slow on large data sets.
- Although accurate, often cannot compete with the accuracy of advanced boosting algorithms.
- Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).


## The Idea {#rf-idea}

Random forests are built on the same fundamental principles as decision trees and bagging (check out this [tutorial](http://uc-r.github.io/regression_trees) if you need a refresher on these techniques).  Bagging trees introduces a random component in to the tree building process that reduces the variance of a single tree's prediction and improves predictive performance.  However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree.  Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships.

For example, if we create six decision trees with different bootstrapped samples of the [Boston housing data]((http://lib.stat.cmu.edu/datasets/boston)) [@harrison1978hedonic], we see that the top of the trees all have a very similar structure.  Although there are 15 predictor variables to split on, all six trees have both `lstat` and `rm` variables driving the first few splits.  

```{r tree-correlation, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Six decision trees based on different bootstrap samples.", echo=FALSE, dev='png', eval=FALSE}

library(caret)
library(randomForest)

iter = 6
par(mfrow = c(3, 3))

for(i in 1:iter){
  set.seed(i+30)
  # create train/test sets
  train_index <- caret::createDataPartition(pdp::boston$cmedv, p = .6333,
                                     list = FALSE,
                                     times = 1)
  
  train_DF <- pdp::boston[train_index,]
  validate_DF <- pdp::boston[-train_index,]
  
  train_y <- train_DF$cmedv
  train_x <- train_DF[, setdiff(names(train_DF), "cmedv")]
  
  validate_y <- validate_DF$cmedv
  validate_x <- validate_DF[, setdiff(names(validate_DF), "cmedv")]
  
  d_tree <- rpart::rpart(cmedv ~ ., train_DF)
  
  # graphs
  
  rpart.plot::rpart.plot(d_tree, main = paste0("Decision Tree ", i), type = 0, extra = 0) 
  
}

```

```{r boston-trees, echo=FALSE, fig.cap="Six decision trees based on different bootstrap samples.", out.height="100%", out.width="100%"}
knitr::include_graphics("images/Boston-6-trees.png")
```

This characteristic is known as *tree correlation* and prevents bagging from optimally reducing variance of the predictive values.  In order to reduce variance further, we need to minimize the amount of correlation between the trees. This can be achieved by injecting more randomness into the tree-growing process.  Random forests achieve this in two ways:

1. __Bootstrap__: similar to bagging, each tree is grown to a bootstrap resampled data set, which makes them different and _somewhat_ decorrelates them.
2. __Split-variable randomization__: each time a split is to be performed, the search for the split variable is limited to a random subset of *m* of the *p* variables.  Typical default values are $m = \frac{p}{3}$ (regression trees) and $m = \sqrt{p}$ (classification trees) but this should be considered a tuning parameter.  When $m = p$, the randomization amounts to using only step 1 and is the same as *bagging*.

The basic algorithm for a regression or classification random forest can be generalized to the following:

```r
1.  Given training data set
2.  Select number of trees to build (ntrees)
3.  for i = 1 to ntrees do
4.  |  Generate a bootstrap sample of the original data
5.  |  Grow a regression or classification tree to the bootstrapped data
6.  |  for each split do
7.  |  | Select m variables at random from all p variables
8.  |  | Pick the best variable/split-point among the m
9.  |  | Split the node into two child nodes
10. |  end
11. | Use typical tree model stopping criteria to determine when a tree is complete (but do not prune)
12. end
```

Since the algorithm randomly selects a bootstrap sample to train on ___and___ predictors to use at each split, tree correlation will be lessened beyond bagged trees.

### OOB error vs. test set error {#rf-oob}

Similar to bagging, a natural benefit of the bootstrap resampling process is that random forests have an out-of-bag (OOB) sample that provides an efficient and reasonable approximation of the test error.  This provides a built-in validation set without any extra work on your part, and you do not need to sacrifice any of your training data to use for validation. This makes identifying the number of trees required to stablize the error rate during tuning more efficient; however, as illustrated below some difference between the OOB error and test error are expected.

```{r, echo=FALSE, fig.cap="Random forest out-of-bag error versus validation error.", out.height="100%", out.width="100%"}
knitr::include_graphics("images/OOB_error.png")
```


Furthermore, many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not.  If you are comparing multiple models to one-another, you'd want to score each on the same validation set to compare performance. Also, although technically it is possible to compute certain metrics such as root mean squared logarithmic error (RMSLE) on the OOB sample, it is not built in to all packages.  So if you are looking to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation.


### Tuning {#rf-tune}

Random forests are fairly easy to tune since there are only a handful of tuning parameters.  Typically, the primary concern when starting out is tuning the number of candidate variables to select from at each split.  However, there are a few additional hyperparameters that we should be aware of. Although the argument names may differ across packages, these hyperparameters should be present: 

- __Number of trees___:  We want enough trees to stabalize the error but using too many trees is unncessarily inefficient, especially when using large data sets.
- __Number of variables to randomly sample as candidates at each split__ (often referred to as `mtry`): When `mtry` $=p$ the model equates to bagging.  When `mtry` $=1$ the split variable is completely random, so all variables get a chance but can lead to overly biased results. A common suggestion is to start with 5 values evenly spaced across the range from 2 to *p*.
- __Sample size__: the number of samples to train on. The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample.  Lower sample sizes can reduce the training time but may introduce more bias than necessary.  Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range.
- __Node size__: minimum number of samples within the terminal nodes. Controls the complexity of the trees.  Smaller node size allows for deeper, more complex trees and a larger node size results in shallower trees.  This is another bias-variance tradeoff where deeper trees introduce more variance (risk of overfitting) and shallower trees introduce more bias (risk of not fully capturing unique patters and relatonships in the data).
- __Number of terminal nodes__: Another way to control the complexity of the trees. More nodes equates to deeper, more complex trees and less nodes result in shallower trees.


### Package implementation {#rf-pkgs}

There are over 20 random forest packages in R.[^task]  The oldest and most well known implementation of the Random Forest algorithm in R is the `randomForest` package.  

```{block, type = "rmdwarning"}
`randomForest` is not a recommended package because as your data sets grow in size `randomForest` does not scale well (although you can parallelize with `foreach`).  Instead, we recommend you use the `ranger` and `h2o` packages.  
```

Since `randomForest` does not scale well to many of the data set sizes that organizations analyze, we will demonstrate how to implement the random forest algorithm with two fast, efficient, and highly recommended packages:

* [`ranger`](https://github.com/imbs-hl/ranger): a C++ implementation of Brieman's random forest algorithm and particularly well suited for high dimensional data. The original paper describing `ranger` and providing benchmarking to other packages can be found [here](http://arxiv.org/pdf/1508.04409v1.pdf). Features include[^ledell]:
    - Classification, regression, probability estimation and survival forests are supported.
    - Multi-threaded capabilities for optimal speed.
    - Excellent speed and support for high-dimensional or wide data.
    - Not as fast for "tall & skinny" data (many rows, few columns).
    - GPL-3 licensed.
* [`h2o`](https://cran.r-project.org/web/packages/gamboostLSS/index.html): The `h2o` R package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with a fairly comprehensive [online resource](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html) that includes methodology and code documentation along with tutorials. Features include:
    - Automated feature pre-processing (one-hot encode & standardization).
    - Built-in cross validation.
    - Built-in grid search capabilities.
    - Provides automatic early stopping for faster grid searches.
    - Supports the following distributions: "guassian", "binomial", "multinomial", "poisson", "gamma", "tweedie".
    - Uses histogram approximations of continuous variables for speedup on "long data" (many rows).
    - Distributed and parallelized computation on either a single node or a multi-node cluster.
    - Model export in plain Java code for deployment in production environments.



## Implementation: Regression {#rf-regression}

To illustrate various regularization concepts for a regression problem we will use the Ames, IA housing data, where our intent is to predict `Sale_Price`. 

```{r rf-regression-data-import}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

```{block, type = "rmdtip"}
Tree-based algorithms typically perform very well without preprocessing the data (i.e. one-hot encoding, normalizing, standardizing).
```

### `ranger` {#ranger-regression}

#### Basic implementation {#ranger-regression-basic}

`ranger::ranger` uses the formula method for specifying our model.  Below we apply the default `ranger` model specifying to model `Sale_Price` as a function of all features in our data set.  The key arguments to the `ranger` call are:

* `formula`: formula specification
* `data`: training data
* `num.trees`: number of trees in the forest
* `mtry`: randomly selected predictor variables at each split. Default is $\texttt{floor}(\sqrt{\texttt{number of features}})$; however, for regression problems the preferred `mtry` to start with is $\texttt{floor}(\frac{\texttt{number of features}}{3}) = \texttt{floor}(\frac{92}{3}) = 30$
* `respect.unordered.factors`: specifies how to treat unordered factor variables. We recommend setting this to "order" for regression. See @esl, chapter 9.2.4 for details.
* `seed`: because this is a random algorithm, you will set the seed to get reproducible results

```{block, type = "rmdnote"}
 By default, `ranger` will provide the computation status and estimated remaining time; however, to reduce output in this tutorial this is turned off with `verbose = FALSE`.
```

As the model results show, averaging across all 500 trees provides an OOB $MSE = 615848303$ ($RMSE \approx 24816$).

```{r rf-m1-ranger}
# number of features
features <- setdiff(names(ames_train), "Sale_Price")

# perform basic random forest model
m1_ranger <- ranger(
  formula    = Sale_Price ~ ., 
  data       = ames_train, 
  num.trees  = 500,
  mtry       = floor(length(features) / 3),
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123
  )

# look at results
m1_ranger

# compute RMSE (RMSE = square root of MSE)
sqrt(m1_ranger$prediction.error)
```


One of the benefits of tree-based methods is they do not require preprocessing steps such as normalization and standardization of the response and/or predictor variables.  However, because these methods do not require these steps does not mean you should not assess their impact.  Sometimes normalizing and standardizing the data can improve performance.  In the following code we compare a basic random forest model on unprocessed data to one on processed data (normalized, standardized, and zero variance features removded).  


```{r m1-ranger-preprocessed}
# create validation set
set.seed(123)
split2 <- initial_split(ames_train, prop = .8, strata = "Sale_Price")
train_tran <- training(split2)
validation <- testing(split2)


#-------------------------Unprocessed variables-------------------------#

# number of features in unprocessed data
m <- length(setdiff(names(train_tran), "Sale_Price"))

# perform basic random forest model on unprocessed data
m1_ranger_unprocessed <- ranger(
  formula    = Sale_Price ~ ., 
  data       = train_tran, 
  num.trees  = 500,
  mtry       = m,
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123
  )


#--------------------------Processed variables--------------------------#

# preprocess features
feature_process <- caret::preProcess(
  train_tran[, features],
  method = c("YeoJohnson", "center", "scale", "zv")
)

train_tran <- predict(feature_process, train_tran)

# preprocess response
train_tran$Sale_Price <- log(train_tran$Sale_Price) 

# number of features in processed data
m <- length(setdiff(names(train_tran), "Sale_Price"))

# perform basic random forest model on processed data
m1_ranger_processed <- ranger(
  formula    = Sale_Price ~ ., 
  data       = train_tran, 
  num.trees  = 500,
  mtry       = m,
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123
  )
```

We can now apply each model to the validation set.  For the second (preprocessed) model, we re-transform our predicted values back to the normal units and we compute the RMSE for both.  Now we see that our original model on unpreprocessed data is performing just as well as, if not better than, the second model on the processed data.  

```{r m1-preprocessed-results}
# apply unpreprocessed model
m1_pred <- predict(m1_ranger_unprocessed, validation)
caret::RMSE(m1_pred$predictions, validation$Sale_Price)

# preprocess features
valid_tran <- predict(feature_process, validation)

# apply preprocessed model
m1_tran_pred <- predict(m1_ranger_processed, valid_tran)
m1_processed_pred <- expm1(m1_tran_pred$predictions)
caret::RMSE(m1_processed_pred, validation$Sale_Price)
```




#### Tuning {#ranger-regression-tune}

With the `ranger` function we can tune various hyperparameters mentioned in the general [tuning](#rf-tune) section.  For example, the following model adjusts:

* `num.trees`: increase number of trees to 750
* `mtry`: reduce number of predictor variables to randomly select at each split to 20
* `min.node.size`: reduce minimum node size to 3 (default is 5 for regression)
* `sample.fraction`: increase training set to 70%



```{r m2-ranger}
m2_ranger <- ranger(
  formula         = Sale_Price ~ ., 
  data            = ames_train, 
  num.trees       = 750,
  mtry            = 20,
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  min.node.size   = 3,
  sample.fraction = .70
  )

# RMSE
sqrt(m2_ranger$prediction.error)

# model results
m2_ranger
```


We can continue to adjust these settings individually to identify the optimal combination; however, this becomes tedious when you want to explore a larger grid search.  To perform a larger grid search across several hyperparameters we’ll need to create a grid and loop through each hyperparameter combination and evaluate the model.  First we want to construct our grid of hyperparameters. We’re going to search across 80 different models with varying number of trees, `mtry`, minimum node size, and sample size.

```{r grid-ranger}
# hyperparameter grid search
hyper_grid <- expand.grid(
  num.trees  = seq(250, 500, 750),
  mtry       = seq(20, 40, by = 5),
  node_size  = seq(1, 10, by = 3),
  sample_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

# total number of combinations
nrow(hyper_grid)

# hyperparameter grid
head(hyper_grid)
```

We can now loop through each hyperparameter combination. Note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. 

```{block, type = "rmdtip"}
This full grid search ran for about __2.5__ minutes before completing.  Larger grid searches like these can become time consuming as your data set increases in dimensions.  The `h2o` package provides alternative approaches to search through larger grid spaces.
```

Our OOB RMSE ranges between ~25021-26089. Our top 10 performing models all have RMSE values in the low 25000 range and the results show that we can use a smaller number of trees than the default and models with slighly larger sample size appear to perform best. At first glance, no definitive evidence suggests that altering `mtry` or `node_size` have a sizable impact.  

```{r search-ranger}
for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train,
    respect.unordered.factors = 'order',
    seed            = 123,
    verbose         = FALSE,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i]
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)
```

The above grid search helps to focus where we can further refine our model tuning.  As a next step, we would perform additional grid searches that focus in on a refined grid space for sample size and also try a few additional settings of `mtry` and `min.node.size` to rule out their effects on performance.  However, for brevity we will leave this as an exercise for the reader.

#### Visual interpretation {#ranger-regression-viz}

Whereas regularized regression assumes a monotonic linear relationship between features and the response, random forests make no such assumption.  Moreover, random forests do not have coefficients to base these relationships on.  Consequently, with random forests we can understand the relationship between the features and the response using variable importance plots and partial dependence plots.

```{block, type = "rmdtip"}
Additional model interpretability approaches will be discussed in the ___Model Interpretability___ chapter.
```

##### Variable importance {#ranger-rf-regression-vip}

Whereas regularized models used the standardized coefficients to signal importance, random forests have, historically, applied two different approaches to measure variable importance.   

1. __Impurity__: At each split in each tree, compute the improvement in the split-criterion (MSE for regression). Then average the improvement made by each variable across all the trees that the variable is used.  The variables with the largest average decrease in MSE are considered most important.
2. __Permutation__: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly "shaking up" of variable values is averaged over all the trees for each variable.  The variables with the largest average decrease in accuracy are considered most important.

To compute these variable importance measures with __ranger__, you must include the `importance` argument.

```{block, type="rmdtip"}
Once you've identified the optimal parameter values from the grid search, you will want to re-run your model with these hyperparameter values.
```


```{r ranger-vip}
# re-run model with impurity-based variable importance
m3_ranger_impurity <- ranger(
  formula         = Sale_Price ~ ., 
  data            = ames_train, 
  num.trees       = 250,
  mtry            = 35,
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  min.node.size   = 1,
  sample.fraction = .80,
  importance = 'impurity'
  )

# re-run model with permutation-based variable importance
m3_ranger_permutation <- ranger(
  formula         = Sale_Price ~ ., 
  data            = ames_train, 
  num.trees       = 250,
  mtry            = 35,
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  min.node.size   = 1,
  sample.fraction = .80,
  importance = 'permutation'
  )
```

For both options, you can directly access the variable importance values with `model_name$variable.importance`.  However, here we will plot the variable importance using the `vip` package.  Typically, you will not see the same variable importance order between the two options; however, you will often see similar variables at the top of the plots.  Consquently, in this example, we can comfortably state that there appears to be enough evidence to suggest that three variables stand out as most influential:

* `Overall_Qual`
* `Gr_Liv_Area`
* `Neighborhood`

Looking at the next ~10 variables in both plots, you will also see some commonality in influential variables (i.e. `Garage_Cars`, `Bsmt_Qual`, `Year_Built`).

```{r vip-plots, fig.width=9, fig.cap="Top 25 most important variables based on impurity (left) and permutation (right)."}
p1 <- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle("Impurity-based variable importance")
p2 <- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle("Permutation-based variable importance")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```


##### Partial dependence plots {#ranger-rf-regression-pdp}

After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. Unlike linear approaches, random forests do not assume a linear relationship.  Consequently, we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves.

PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. For example, consider the `Gr_Liv_Area` variable.  In the __h20__ regularized regression section (\@ref(regression-h2o-viz)), we saw that the linear model assumed a continously increasing relationship between `Gr_Liv_Area` and `Sale_Price`.  However, the PDP plot below displays a non-linear relationship where `Sale_Price` appears to not be influenced by `Gr_Liv_Area` values below 750 sqft or above 3500 sqft.

```{r pdp-GrLiv-Area, fig.height=3, fig.width=4, fig.cap="The mean predicted sale price as the above ground living area increases."}
# partial dependence of Sale_Price on Gr_Liv_Area
m3_ranger_impurity %>%
  partial(pred.var = "Gr_Liv_Area", grid.resolution = 50) %>%
  autoplot(rug = TRUE, train = ames_train)
```

Additionally, if we assess the relationship between the `Overall_Qual` predictor and `Sale_Price`, we see a continual increase as the overall quality increases.  This is more intutive than the results we saw in the regularized regression section (\@ref(regression-h2o-viz)).  This may be an indication that the coefficients were biased in the regularized regression models.

```{r pdp-Overall-Qual, fig.height=3, fig.width=8, fig.cap="The mean predicted sale price for each level of the overall quality variable."}
# partial dependence of Sale_Price on Overall_Qual
m3_ranger_impurity %>%
  partial(pred.var = "Overall_Qual", train = as.data.frame(ames_train)) %>%
  autoplot()
```


Individual conditional expectation (ICE) curves [@goldstein2015peeking] are an extension of PDP plots but, rather than plot the _average_ marginal effect on the response variable, we plot the change in the predicted response variable ___for each observation___ as we vary each predictor variable.  Below shows the regular ICE curve plot (left) and the centered ICE curves (right).  When the curves have a wide range of intercepts and are consequently “stacked” on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results.

The plots below show that marginal changes in `Gr_Liv_Area` have a fairly homogenous effect on our response variable. As `Gr_Liv_Area` increases, the vast majority of observations show a similar increasing effect on the predicted `Sale_Price` value.  The primary differences is in the magnitude of the increasing effect. However, in the centered ICE plot you see evidence of a few observations that display a different pattern. These observations would be worth looking at more closely.

```{r ice-Gr-Liv-Area, fig.height=3, fig.width=9, fig.cap="Non-centered (left) and centered (right) individual conditional expectation curve plots illustrate how changes in above ground square footage influences predicted sale price for all observations."}
# ice curves of Sale_Price on Gr_Liv_Area
ice1 <- m3_ranger_impurity %>%
  partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = 0.2) +
  ggtitle("Non-centered ICE plot")

ice2 <- m3_ranger_impurity %>%
  partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = 0.2, center = TRUE) +
  ggtitle("Centered ICE plot")

gridExtra::grid.arrange(ice1, ice2, nrow = 1)
```

Both PDPs and ICE curves should be assessed for the most influential variables as they help to explain the underlying patterns in the data that the random forest model is picking up.

```{block, type = "rmdnote"}
Check out the ___Model Interpretation___ chapter to learn more about visualizing your machine learning models.
```

#### Predicting {#ranger-regression-predic}

Once you've found your optimal model, predicting new observations with the `ranger` model follows the same procedure as most R models.  We can apply the `predict` function and supply it the optimal model and the new data set we'd like to predict on.  The result is a list object that includes several attributes about the model used to predict (i.e. number of trees & predictor variables, sample size, tree type).  The predicted values we are most concerned with are contained in the `predict_object$predictions` list item.  

```{r ranger-predict}
# predict on test set
predict_ranger <- predict(m3_ranger_impurity, ames_test)

# predict object
str(predict_ranger)

# predicted values
head(predict_ranger$predictions)
```

We can use these predicted values to assess the final generalization error, which is slightly lower than our models OOB sample RMSE:

```{r ranger-generalization-error}
# final model OOB RMSE
sqrt(m3_ranger_impurity$prediction.error)

# generalization error
caret::RMSE(predict_ranger$predictions, ames_test$Sale_Price)
```



### `h20` {#h2o-rf-regression}

To perform a random forest model with __h2o__, we first need to initiate our __h2o__ session. 

```{r rf-regress-h2o-init-silenteval, echo=TRUE, cache=FALSE}
h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

```{r, include=FALSE}
h2o.removeAll()
```


Next, we need to convert our training and test data to __h2o__ objects.

```{r rf-regress-h2o-data-prep}
# convert training data to h2o object
train_h2o <- as.h2o(ames_train)

# convert test data to h2o object
test_h2o <- as.h2o(ames_test)

# set the response column to Sale_Price
response <- "Sale_Price"

# set the predictor names
predictors <- setdiff(colnames(ames_train), response)
```

#### Basic implementation {#rf-h2o-regression-basic}

To perform a random forest model with `h2o` we use `h2o::h2o.randomForest`.  Keep in mind that `h2o` uses the name method for specifying our model.  Below we apply the default `h2o.randomForest` model specifying to model `Sale_Price` as a function of all features in our data set.  `h2o.randomForest` has many arguments that can be adjusted; however, often the default settings perform very well.  To start with, a few key arguments in `h2o.randomForest` to understand include:

* `x`: names of the predictor variables
* `y`: name of the response variable
* `training_frame`: training data
* `ntrees`: number of trees in the forest (default is 50)
* `mtries`: randomly selected predictor variables at each split. Default is $\texttt{floor}(\frac{\texttt{number of features}}{3}) = \texttt{floor}(\frac{92}{3}) = 30$ for regression.
* `categorical_encoding`: Decides the encoding scheme for categorical variables.  Typically choose one of "Enum" or "SortByResponse" (`categorical_encoding = 'SortByResponse'` performs similar procedure as `respect.unordered.factors = 'order'`). For these data we do not see any difference in performance between the two.
* `seed`: because this is a random algorithm, you will set the seed to get reproducible results

```{block, type = "rmdnote"}
 `h2o` can provide the computation status; however, this feature is turned off by default but can be turned on with `verbose = FALSE`.
```

As the model results show, averaging across all 250 trees provides an OOB $RMSE \approx 24541$).  These are pretty similar to what we found with the default `ranger` model.

```{r rf-regress-h2o-m1, eval=FALSE}
# perform basic random forest model
m1_h2o <- h2o.randomForest(
  x = predictors, 
  y = response,
  training_frame = train_h2o, 
  ntrees = 250,
  seed = 123
  )

# look at results
## m1_h2o
## Model Details:
## ==============
## 
## H2ORegressionModel: drf
## Model ID:  DRF_model_R_1532981766487_1 
## Model Summary: 
## 
## 
## H2ORegressionMetrics: drf
## ** Reported on training data. **
## ** Metrics reported on Out-Of-Bag training samples **
## 
## MSE:  602273377
## RMSE:  24541.26
## MAE:  15060.85
## RMSLE:  0.1406867
## Mean Residual Deviance :  602273377
```

One of the benefits of `h2o` is it allows us to include `stopping_` arguments, which will stop the modeling automatically once the RMSE metric on the OOB samples stops improving by a certain value (say 1%) for a specified number of consecutive trees.  This helps us to identify the number of trees required to stabilize our error metric.  Below we see that 49 trees are sufficient.

```{r rf-regress-m2-h2o-stop, eval=FALSE}
# perform basic random forest model
m2_h2o <- h2o.randomForest(
  x = predictors, 
  y = response,
  training_frame = train_h2o, 
  ntrees = 500,
  seed = 123,
  stopping_metric = "RMSE",     # stopping mechanism
  stopping_rounds = 10,         # number of rounds
  stopping_tolerance = 0.005    # looking for 0.5% improvement
  )

# look at results
m2_h2o
## Model Details:
## ==============
## 
## H2ORegressionModel: drf
## Model ID:  DRF_model_R_1532981766487_2 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
## 1              49                       49              758326        20        20   20.00000       1175       1273  1226.34690
## 
## 
## H2ORegressionMetrics: drf
## ** Reported on training data. **
## ** Metrics reported on Out-Of-Bag training samples **
## 
## MSE:  670676969
## RMSE:  25897.43
## MAE:  15741.89
## RMSLE:  0.1434247
## Mean Residual Deviance :  670676969
```


#### Tuning {#rf-h2o-regression-tune}

`h2o.randomForest` provides ___many___ tuning options.  The more common tuning options can be categorized into three purposes:

1. Controlling how big your random forest will be:
    * `ntrees`: how many trees in the forest
    * `max_depth`: maximum depth to which each tree will be built (default is 20)
   
2. Controlling the random components of the model:   
    * `mtries`: number of predictor variables to randomly select at each split
    * `sample_rate`: Row sample rate per tree (default is 63.2%)
   
3. Controlling how the splitting is done   
    * `min_rows`: minimum number of observations for a leaf in order to split (default is 1)

```{block, type = "rmdtip"}
There are additional tuning parameters in each of the above categories but their defaults are typically sufficient.  You can read about the options [here](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html).
```

We can tune these hyperparameters individually; however, a major benefit of `h2o` is it provides two different approaches for hyperparameter grid searches:

* __Full cartesian grid search__: examine every combination of hyperparameter settings that we specify,
* __Random grid search__: jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met).

##### Full cartesian grid search {#rf-h2o-regression-tune-full}

First, we can try a comprehensive (full cartesian) grid search, which means we will examine every combination of hyperparameter settings that we specify in `hyper_grid.h2o`. Here, we search across 320 hyperparameter combinations.  You can include `ntrees` as a hyperparameter; however, its more efficient to set `ntrees` to a high value and then use early stopping to stop each model once improvement is no longer obtained.

```{block, type="rmdtip"}
This comprehensive grid search took __56__ minutes.
```

The results show a minimum RMSE of \$23,792 (slightly less than our optimal __ranger__ model), when `max_depth = 25`, `min_rows = 1`,
`mtries = 25`, and `sample_rate = 80%`.  Looking at the top 5 models it appears that the primary driving parameters for minimizing MSE are `min_rows` (smaller is better), `mtries` (smaller is better), and `sample_rate` (larger is better).

```{r regression-rf-h2o-full-catesian-grid-search, cache=FALSE, eval=FALSE}
# hyperparameter grid
hyper_grid.h2o <- list(
  mtries      = seq(20, 40, by = 5),
  max_depth   = seq(15, 30, by = 5),
  min_rows    = seq(1, 10, by = 3),
  sample_rate = c(.55, .632, .70, .80)
)

# build grid search 
grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_full_grid",
  x = predictors, 
  y = response, 
  training_frame = train_h2o,
  hyper_params = hyper_grid.h2o,
  ntrees = 500,
  seed = 123,
  stopping_metric = "RMSE",   
  stopping_rounds = 10,         
  stopping_tolerance = 0.005, 
  search_criteria = list(strategy = "Cartesian")
  )

# collect the results and sort by our model performance metric of choice
full_grid_perf <- h2o.getGrid(
  grid_id = "rf_full_grid", 
  sort_by = "mse", 
  decreasing = FALSE
  )
print(full_grid_perf)
## H2O Grid Details
## ================
## 
## Grid ID: rf_full_grid 
## Used hyper parameters: 
##   -  max_depth 
##   -  min_rows 
##   -  mtries 
##   -  sample_rate 
## Number of models: 291 
## Number of failed models: 29 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   max_depth min_rows mtries sample_rate              model_ids                 mse
## 1        25      1.0     25         0.8 rf_full_grid_model_258 5.660696269742714E8
## 2        30      1.0     25         0.8 rf_full_grid_model_259  5.66075155855145E8
## 3        20      1.0     25         0.8 rf_full_grid_model_257  5.66146259929908E8
## 4        30      1.0     20         0.8 rf_full_grid_model_243 5.665494562778755E8
## 5        25      1.0     20         0.8 rf_full_grid_model_242 5.665555943619757E8
## 
## ---
##     max_depth min_rows mtries sample_rate              model_ids                 mse
## 286        25     10.0     35        0.55  rf_full_grid_model_62 8.195309220382509E8
## 287        20     10.0     35        0.55  rf_full_grid_model_61 8.195309220382509E8
## 288        30     10.0     30       0.632 rf_full_grid_model_127 8.266536241339123E8
## 289        25     10.0     30       0.632 rf_full_grid_model_126 8.266536241339123E8
## 290        20     10.0     30       0.632 rf_full_grid_model_125 8.266536241339123E8
## 291        15     10.0     30       0.632 rf_full_grid_model_124 8.266536241339123E8
```


##### Random discrete grid search {#rf-h2o-regression-tune-random}

Because of the combinatorial explosion, each additional hyperparameter that gets added to our grid search has a huge effect on the time to complete. Consequently, `h2o` provides an additional grid search path called ___"RandomDiscrete"___, which will jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). Although using a random discrete search path will likely not find the optimal model, it typically does a good job of finding a very good model.

```{block, type="rmdtip"}
This comprehensive grid search took __30__ minutes.
```

For example, the following code searches the same grid search performed above. We create a random grid search that will stop if none of the last 10 models have managed to have a 0.1% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 1800 seconds (30 minutes). Our grid search assessed 191 models before stopping due to time. The best model (`max_depth = 25`, `min_rows = 1`, `mtries = 40`, and `sample_rate = 0.8`) achived an RMSE $\approx \$23,751$. So although our random search on assessed about half the number of models as the full grid search, the more efficient random search found a near-optimal model relatively speaking.

```{r h2o-random-grid-search, eval=FALSE}
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,
  stopping_rounds = 10,
  max_runtime_secs = 60*30
  )

# build grid search 
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response, 
  training_frame = train_h2o,
  hyper_params = hyper_grid.h2o,
  ntrees = 500,
  seed = 123,
  stopping_metric = "RMSE",   
  stopping_rounds = 10,         
  stopping_tolerance = 0.005, 
  search_criteria = search_criteria
  )

# collect the results and sort by our model performance metric of choice
random_grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "mse", 
  decreasing = FALSE
  )
print(random_grid_perf)
## H2O Grid Details
## ================
## 
## Grid ID: rf_random_grid 
## Used hyper parameters: 
##   -  max_depth 
##   -  min_rows 
##   -  mtries 
##   -  sample_rate 
## Number of models: 191 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   max_depth min_rows mtries sample_rate                model_ids                 mse
## 1        25      1.0     40         0.8 rf_random_grid_model_131 5.624310085353142E8
## 2        15      1.0     40         0.8 rf_random_grid_model_180  5.63276905670922E8
## 3        25      1.0     25         0.8 rf_random_grid_model_174 5.660696269742714E8
## 4        20      1.0     25         0.8  rf_random_grid_model_17  5.66146259929908E8
## 5        20      1.0     20         0.8 rf_random_grid_model_144  5.66631854910229E8
## 
## ---
##     max_depth min_rows mtries sample_rate                model_ids                  mse
## 186        20     10.0     40        0.55  rf_random_grid_model_52  8.163555317753098E8
## 187        30     10.0     30         0.8  rf_random_grid_model_50  8.182002205674793E8
## 188        20     10.0     35        0.55  rf_random_grid_model_23  8.195309220382509E8
## 189        30     10.0     35        0.55  rf_random_grid_model_97  8.195309220382509E8
## 190        20     10.0     30       0.632  rf_random_grid_model_44  8.266536241339123E8
## 191        30      7.0     30         0.8 rf_random_grid_model_190 1.3296523023487797E9
```

Once we’ve identifed the best model we can extract it with:

```{r h2o-best-model, cache=TRUE, eval=FALSE}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
```

```{r h2o-best-model-import, echo=FALSE, cache=FALSE}
# save the model
#h2o.saveModel(object = best_model, path = "outputs/h2o_best_rf_model", force = TRUE)

# import model
best_model <- h2o.loadModel(path = "outputs/h2o_best_rf_model/rf_random_grid_model_131")
```



#### Visualizing results {#rf-h2o-regression-viz}

##### Variable importance {#rf-h2o-regression-vip}

Once you’ve identified and selected the optimally tuned model, you can visualize variable importance with `h2o.varimp_plot`.  `h2o.varimp_plot` computes variable importance ___"by calculating the relative influence of each variable: whether that variable was selected during splitting in the tree building process and how much the squared error (over all trees) improved as a result."___ [^var]  This is equivalent to the impurity approach used by `ranger`.  The most important variables are relatively similar to those found with the ranger model.


```{r h2o-varimp-plot, cache=TRUE, fig.height=6, fig.cap="Variable importance plot provided by the __h2o__ package."}
h2o.varimp_plot(best_model, num_of_features = 25)
```

If you prefer the plotting provided by the `vip` package, you can also use `vip::vip` on any `h2o` model as well.

```{r h2o-rf-vip, fig.cap="Variable importance plot provided by the __vip__ package."}
vip(best_model, num_features = 25, bar = FALSE)
```


##### Partial dependence plots {#rh-h2o-regression-pdp}

As with __ranger__, we can also assess PDP plots. `h2o` provides the `h2o.partialPlot` function to plot PDPs.  Although it does not allow you to plot individual ICE curves, it does plot the standard error of the mean response across all observations along with automatically showing you the values of the predictor variable (by default it selects 20 values but can be adjusted with `nbins`), mean response, and standard error of the response. The partial dependence plot for `Gr_Liv_Area` follows a similar non-linear trend as we saw with the __ranger__ model.

```{r h2o-pdp-rf-silent, echo=FALSE}
train_h2o <- as.h2o(ames_train)
```

```{r h2o-pdp-rf, fig.cap="__h2o__'s partial dependence plot of the `Gr_Liv_Area` predictor variable based on the optimal __h2o__ model."}
h2o.partialPlot(best_model, data = train_h2o, cols = "Gr_Liv_Area")
```

If you prefer getting actual ICE curves, we can use the `pdp` package.  However, since `pdp` does not have an explicit method for `h2o` objects we need to create a prediction function and use the `pred.fun` argument:

```{r h2o-ice, fig.height=3.5, fig.width=8, fig.cap="__pdp__'s ICE curves of the `Gr_Liv_Area` predictor variable based on the optimal __h2o__ model."}
# build custom prediction function
pfun <- function(object, newdata) {
  as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]]
}

# compute ICE curves 
prod.ice <- partial(
  best_model, 
  pred.var = "Gr_Liv_Area", 
  train = ames_train,
  pred.fun = pfun,
  grid.resolution = 20
)

p1 <- autoplot(prod.ice, alpha = 0.2) + ggtitle("Non-centered ICE curves")
p2 <- autoplot(prod.ice, alpha = 0.2, center = TRUE) + ggtitle("Centered ICE curves")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```


#### Predicting {#rf-h2o-regression-predict}

Finally, if you are satisfied with your final model we can predict values for an unseen data set a couple different ways.  We can also quickly assess the model's performance on our test set with `h2o.performance`.  We see a similar generalizable error as we saw with the __ranger__ model.

```{r rf-regression-h20-final-predictions-silent, echo=FALSE}
test_h2o <- as.h2o(ames_test)
```


```{r rf-regression-h20-final-predictions}
# predict new values with base R predict()
predict(best_model, test_h2o)

# predict new values with h2o.predict()
h2o.predict(best_model, newdata = test_h2o)

# assess performance on test data
h2o.performance(best_model, newdata = test_h2o)
```


```{r rf-regression-h2o-shutdown}
# shut down h2o
h2o.shutdown(prompt = FALSE)
```



## Implementation: Binary Classification {#rf-binary-classification}

To illustrate random forests concepts for a binary classification problem we will continue with the employee attrition data. 

```{r import-rf-binary-classification}
attrition <- rsample::attrition %>% 
  mutate_if(is.ordered, factor, ordered = FALSE) %>%
  mutate(Attrition = relevel(Attrition, ref = "Yes"))

# Create training and testing sets
set.seed(123)
split <- initial_split(attrition, prop = .8, strata = "Attrition")
attrit_train <- training(split)
attrit_test  <- testing(split)
```

```{block, type = "rmdtip"}
Tree-based algorithms typically perform very well without preprocessing the data (i.e. one-hot encoding, normalizing, standardizing).
```

### `ranger` {#ranger-rf-binary-classification}


#### Basic implementation {#ranger-binary-classification-basic}

We apply `ranger::ranger` just as we did in the regression setting.  However, note that the default `mtry` is $\texttt{floor}(\sqrt{\texttt{number of features}})$, which is a good starting point for classification problems (we changed it to $mtry = \texttt{floor}(\frac{\texttt{number of features}}{3})$ in the regression setting).

```{block, type = "rmdtip"}
As long as your response variable is encoded as a character or factor, `ranger` will automatically perform a classification random forest model.
```

As the model results show, majority voting across all 500 trees provides an OOB error rate of 13.59%.

```{r m1-ranger-binary-classification}
# perform basic random forest model
m1_ranger <- ranger(
  formula    = Attrition ~ ., 
  data       = attrit_train, 
  num.trees  = 500,
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123
  )

# look at results
m1_ranger

# look at confusion matrix
m1_ranger$confusion.matrix
```

```{block, type = "rmdtip"}
The default `ranger` classification model does not provide probability estimates.  If you want to predict the probabilities then use `probability = TRUE`. When using this option, the OOB prediction error changes from misclassification rate to MSE.
```


One of the benefits of tree-based methods is they do not require preprocessing steps such as normalization and standardization of the response and/or predictor variables.  However, because these methods do not require these steps does not mean you should not assess their impact.  Sometimes normalizing and standardizing the data can improve performance.  In the following code we compare a basic random forest probability model with unprocessed features to one with processed features (normalized, standardized, and zero variance features removed).  


```{r m1-ranger-binary-classification-preprocessed}
# create validation set
set.seed(123)
split2 <- initial_split(attrit_train, prop = .8, strata = "Attrition")
train_tran <- training(split2)
validation <- testing(split2)


#-------------------------Unprocessed variables-------------------------#

# perform basic random forest model on unprocessed data
m1_ranger_unprocessed <- ranger(
  formula    = Attrition ~ ., 
  data       = train_tran, 
  num.trees  = 500,
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123,
  probability = TRUE
  )


#--------------------------Processed variables--------------------------#

features <- setdiff(names(attrit_train), "Attrition")

# preprocess features
feature_process <- caret::preProcess(
  train_tran[, features],
  method = c("BoxCox", "center", "scale", "zv")
)

train_tran <- predict(feature_process, train_tran)

# perform basic random forest model on processed data
m1_ranger_processed <- ranger(
  formula    = Attrition ~ ., 
  data       = train_tran, 
  num.trees  = 500,
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123,
  probability = TRUE
  )
```

We can now apply each model to the validation set and we see that the feature processing has no impact on our area under the curve.  

```{r ranger-binary-class-m1-preprocessed-results}
# apply unpreprocessed model
m1_pred <- predict(m1_ranger_unprocessed, validation)
roc <- pROC::roc(validation$Attrition, m1_pred$predictions[, 1])
pROC::auc(roc)

# preprocess features
valid_tran <- predict(feature_process, validation)

# apply preprocessed model
m1_tran_pred <- predict(m1_ranger_processed, valid_tran)
roc <- pROC::roc(validation$Attrition, m1_tran_pred$predictions[, 1])
pROC::auc(roc)
```


#### Tuning {#ranger-rf-binary-classification-tune}

With the `ranger` function we can tune various hyperparameters mentioned in the general [tuning](#rf-tune) section.  For example, the following model adjusts:

* `num.trees`: increase number of trees to 750
* `mtry`: increase the number of predictor variables to randomly select at each split to 20
* `min.node.size`: increase minimum node size to 3 (default is 1 for classification)
* `sample.fraction`: increase training set to 70%



```{r m2-ranger-binary-classification}
m2_ranger <- ranger(
  formula         = Attrition ~ ., 
  data            = attrit_train, 
  num.trees       = 750,
  mtry            = 20,
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  min.node.size   = 3,
  sample.fraction = .70
  )

# misclassification rate
m2_ranger$prediction.error

# model results
m2_ranger
```


We can continue to adjust these settings individually to identify the optimal combination; however, this become tedious when you want to explore a larger grid search.  Similar to the regression setting, to perform a larger grid search across several hyperparameters we need to create a grid and loop through each hyperparameter combination and evaluate the model.  First we want to construct our grid of hyperparameters. We’re going to search across 200 different models with varying number of trees, `mtry`, minimum node size, and sample size. I also vary the split rule, which determines when and how to split into branches.

```{r grid-ranger-binary-classification}
# hyperparameter grid search
hyper_grid <- expand.grid(
  num.trees  = seq(250, 500, 750),
  mtry       = seq(5, 25, by = 5),
  node_size  = seq(1, 10, by = 3),
  sample_size = c(.55, .632, .70, .80, 1),
  splitrule  = c("gini", "extratrees"),
  OOB_error  = 0
)

# total number of combinations
nrow(hyper_grid)

# hyperparameter grid
head(hyper_grid)
```

We can now loop through each hyperparameter combination. Note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. 

```{block, type = "rmdtip"}
This full grid search took about __90 seconds__ to compute.
```

Our OOB classification error ranges between ~0.1308-0.1487. Our top 10 performing models all have classification error rates in the lower 0.13 range. The results show that all the top 10 models use less trees, larger `mtry` than the default ($\text{floor}\big(\sqrt{\text{number of features}}\big) = 5$, and a sample size less than 100.  However, no definitive patterns are observed with the other hyperparameters. 

```{r search-ranger-binary-classification, eval=FALSE}
for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger(
    formula         = Attrition ~ ., 
    data            = attrit_train,
    respect.unordered.factors = 'order',
    seed            = 123,
    verbose         = FALSE,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    splitrule       = hyper_grid$splitrule[i]
  )
  
  # add OOB error to grid
  hyper_grid$OOB_error[i] <- model$prediction.error
}

hyper_grid %>% 
  dplyr::arrange(OOB_error) %>%
  head(10)
##    num.trees mtry node_size sample_size  splitrule OOB_error
## 1        250   25         1       0.800       gini 0.1308411
## 2        250   20         7       0.632       gini 0.1325404
## 3        250   25        10       0.800 extratrees 0.1325404
## 4        250   25         4       0.700       gini 0.1333900
## 5        250   25         7       0.700       gini 0.1333900
## 6        250   20         7       0.550 extratrees 0.1333900
## 7        250   25        10       0.700 extratrees 0.1333900
## 8        250   25         4       0.800 extratrees 0.1333900
## 9        250   25         1       0.700       gini 0.1342396
## 10       250   10         7       1.000       gini 0.1342396
```

The above grid search helps to focus where we can further refine our model tuning.  As a next step, we would perform additional grid searches; however, for brevity we will leave this as an exercise for the reader.


#### Visualizing results {#ranger-rf-binary-classification-viz}

##### Variable importance {#ranger-rf-binary-classification-vip}

As in the regression setting, once we've found our optimal hyperparameter settings we can re-run our model and set the `importance` argument to "impurity" and/or "permutation".  The following applies both settings so that we can compare and contrast the influential variables each method identifies.

```{r ranger-binary-classification-vip}
m3_ranger_impurity <- ranger(
  formula         = Attrition ~ ., 
  data            = attrit_train, 
  num.trees       = 250,
  mtry            = 25,
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  min.node.size   = 1,
  sample.fraction = .80,
  importance = 'impurity'
  )

m3_ranger_permutation <- ranger(
  formula         = Attrition ~ ., 
  data            = attrit_train, 
  num.trees       = 250,
  mtry            = 25,
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  min.node.size   = 1,
  sample.fraction = .80,
  importance = 'permutation'
  )
```

Plotting the top 25 influential variables using both variable importance methods results in a common theme among the top 3 variables - `MonthlyIncome`, `Age`, and `OverTime` appear to have strong influence on our results.  We also saw `OverTime` as an influential variable using [regularized regression](glm-binary-classification). Looking at the next dozen important variables, we see similar results across variable importance approaches but just in different order (i.e. `TotalWorkingYears`, `JobRole`, `NumCompaniesWorked`).  Some of these were influential variables in the regularized regression models and some were not; suggesting our random forest model is picking up different patterns and logic in our data.

```{r ranger-binary-classification-vip-plots, fig.width=10, fig.height=5, fig.cap="Top 25 most important variables based on impurity (left) and permutation (right)."}
p1 <- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle("Impurity-based variable importance")
p2 <- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle("Permutation-based variable importance")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```


##### Partial dependence plots {#ranger-rf-binary-classification-pdp}

After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables.  This is important considering random forests allow us to pick up non-linear, non-monotonic relationships. For this we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves.  However, to generate PDPs and ICE curves we need to run a probability model (`probability = TRUE`) so that we can extract the class probabilities.  

```{block, type="rmdnote"}
To produce a PDP with binary classification problems, we need to create a custom prediction function that will return a vector of the ___mean predicted probability___ for the response class of interest (in this example we want the probabilities for `Attrition = "Yes"`). We supply this custom prediction function within the `pdp::partial` function call.
```

Our PDPs illustrate a strong increase in the probability of attrition for employees that work overtime.  Also, note that non-linear relationship between the probability of attrition and monthly income and age.  The `MonthlyIncome` plot shows an increase in probability as monthly income reaches \$10,000 but then flatlines until employees make about \$20,000 per month.  Similiarly with age, as employees get older they tend to become more stable; however, this changes after the age of 45 where an increase of age tends to increase the probablity of attrition (recall in Section \@ref(glm-h2o-classification-binary-viz) that we saw how the regularized models assumed a constantly decreasing relationships between `Age` and the probability of attrition).

```{r rf-ranger-binary-classification-pdp, fig.height=3, fig.width=10, fig.cap="Partial dependence plots of our top 3 influential variables. Note the non-linear, non-monotonic relationship our random forest model is picking up for `MonthlyIncome` and `Age`."}
# probability model
m3_ranger_prob <- ranger(
  formula         = Attrition ~ ., 
  data            = attrit_train, 
  num.trees       = 250,
  mtry            = 25,
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  min.node.size   = 1,
  sample.fraction = .80,
  probability     = TRUE,
  importance      = 'impurity'
  )

# custom prediction function
custom_pred <- function(object, newdata) {
  pred <- predict(object, newdata)
  avg <- mean(pred$predictions[, 1])
  return(avg)
}

# partial dependence of OverTime
p1 <- m3_ranger_prob %>%
  partial(pred.var = "OverTime", pred.fun = custom_pred, train = attrit_train) %>%
  autoplot(rug = TRUE, train = attrit_train)

# partial dependence of MonthlyIncome
p2 <- m3_ranger_prob %>%
  partial(pred.var = "MonthlyIncome", pred.fun = custom_pred, train = attrit_train) %>%
  autoplot(rug = TRUE, train = attrit_train)

# partial dependence of Age
p3 <- m3_ranger_prob %>%
  partial(pred.var = "Age", pred.fun = custom_pred, train = attrit_train) %>%
  autoplot(rug = TRUE, train = attrit_train)

gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```


We can extract more insights with centered ICE curves. Although the PDP illustrates an increase in the average probability of attrition for employees who work overtime, the ICE curves illustrate that this is not the case for all employees.  A fair amount of the observations actually experience a decrease in probability when they work overtime.  This likely suggests an intereaction effect with other variables (we will discuss how to tease out interactions in the ___Model Interpretability___ chapter).

```{block, type="rmdnote"}
To produce ICE curves with binary classification problems, we need to create a custom prediction function that will return a vector of the ___predicted probabilities___ for the response class of interest. 
```

```{r ranger-binary-classification-ice, fig.height=3.5, fig.width=10, fig.cap="Centered ICE curves for our top 3 influential variables."}
# custom prediction function
custom_pred <- function(object, newdata) {
  pred <- predict(object, newdata)
  avg <- pred$predictions[, 1]
  return(avg)
}

# ICE curves for top 3 influential variables
p1 <- m3_ranger_prob %>%
  partial(pred.var = "OverTime", ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %>%
  autoplot(rug = TRUE, train = attrit_train, alpha = 0.2)

p2 <- m3_ranger_prob %>%
  partial(pred.var = "MonthlyIncome", ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %>%
  autoplot(rug = TRUE, train = attrit_train, alpha = 0.2)

p3 <- m3_ranger_prob %>%
  partial(pred.var = "Age", ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %>%
  autoplot(rug = TRUE, train = attrit_train, alpha = 0.2)

gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```



##### ROC curve {#ranger-rf-binary-classification-roc}

As in the regularize regression chapter, we can visualize the ROC curve with the `ROCR` and `pROC` packages.  Both packages compare the predicted probability output to the actual observed class so we need to use our probability ranger model `m3_ranger_prob`. The predicted probabilities for our model are accessible at `ranger_model$predictions` and we want to index for the class of interest.  

```{r ranger-classification-roc, fig.height=4, fig.width=9, fig.cap="ROC curve for our __ranger__ random forest model based on the training data."}
library(ROCR)
library(pROC)

# plot structure
par(mfrow = c(1, 2))

# ROCR plot
prediction(m3_ranger_prob$predictions[, 1], attrit_train$Attrition) %>%
  performance("tpr", "fpr") %>%
  plot(main = "ROCR ROC curve")

#pROC plot
roc(attrit_train$Attrition, m3_ranger_prob$predictions[, 1]) %>% 
  plot(main = "pROC ROC curve", legacy.axes = TRUE)
```


#### Predicting {#ranger-rf-binary-classification-predict}

Once you have identified your preferred model, you can simply use `predict` to predict the same model on a new data set.  If you use a probability model, the predicted values will be probabilities for each class.  If you use a non-probability model, the predicted values will be the class.

```{r ranger-binary-classification-predictions}
# predict a probability model
pred_probs <- predict(m3_ranger_prob, attrit_test)
head(pred_probs$predictions)

# predict a non-probability model
pred_class <- predict(m3_ranger_impurity, attrit_test)
head(pred_class$predictions)
```

Lastly, to assess various performance metrics on our test data we use `caret::confusionMatrix`, which provides the majority of the performance measures we are typically concerned with in classification models.  If you compare the results to the regularized regression model you will notice that our random forest model does not provide additional predictive performance.

```{block, type="rmdwarning"}
You need to supply `caret::confusionMatrix` with the predicted class.  Consequently, if you use the probability model which predicts probabilities then you will need perform an extract step that creates a predicted class based on the probabilities (i.e. `ifelse(probability >= .5, "Yes", "No")`).
```


```{r ranger-binary-class-confusion-matrix}
caret::confusionMatrix(factor(pred_class$predictions), attrit_test$Attrition, positive = "Yes")
```


### `h20` {#h2o-rf-binary-classification}

To perform a binary classification random forest with __h2o__, we first need to initiate our __h2o__ session.

```{r rf-h2o-binary-classification, cache=FALSE}
# launch h2o
h2o::h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

Next, we need to convert our training and test data to __h2o__ objects.

```{r rf-h2o-binary-classification-hidden-setup, cache=FALSE}
# convert training data to h2o object
attrit_train_h2o <- as.h2o(attrit_train)

# convert test data to h2o object
attrit_test_h2o <- as.h2o(attrit_test)

# set the response column to Attrition
response <- "Attrition"

# set the predictor names
predictors <- setdiff(colnames(attrit_train), "Attrition")
```

#### Basic implementation {#h2o-rf-binary-classification-basic}

Similar to our regression problem, we use `h2o::h2o.randomForest` to perform a random forest model with __h2o__.  Most of the default parameter settings in `h2o.randomForest` do not change between a regression and classification problem.  However, `mtries` (how many predictor variables are randomly selected at each split) defaults to $\texttt{floor}(\sqrt{p})$.

```{block, type = "rmdtip"}
As long as your response variable is encoded as a character or factor, `h2o` will apply a binomial or multinomial classification model.  Alternatively, you can specify the response distribution with the `distribution` argument.
```

The following performs a default `h2o.randomForest` model with 250 trees and a 10 fold cross validation.  As the model results show, averaging across all 250 trees provides an OOB $AUC = 0.8$.

```{r rf-h2o-binary-classification-m1}
# perform basic random forest model
m1_h2o <- h2o.randomForest(
  x = predictors, 
  y = response,
  training_frame = attrit_train_h2o, 
  ntrees = 250,
  seed = 123, 
  nfolds = 10,
  keep_cross_validation_predictions = TRUE
  )

# look at results
h2o.performance(m1_h2o, xval = TRUE)
```

The results above used all 250 trees but to make sure we are providing enough trees to stabilize the OOB error we can include automatic stopping. Also, when dealing with classification problems, if your response variable is significantly imbalanced, you can achieve additional predictive accuracy by over/under sampling.  We can over/under sample our classes to achieved balanced class counts by incorporating the `balance_classes` argument.  However, we do not achieve any performance improvement by balancing our attrition classes.

```{block, type="rmdnote"}
You will, typically, achieve performance improvements by over/under sampling when you binary response variable has a 90/10 or worse class imbalance.
```


```{r rf-h2o-binary-classification-stop}
# perform basic random forest model
m2_h2o <- h2o.randomForest(
  x = predictors, 
  y = response,
  training_frame = attrit_train_h2o, 
  ntrees = 500,
  seed = 123,
  nfolds = 10,
  keep_cross_validation_predictions = TRUE,
  balance_classes = TRUE,
  stopping_metric = "AUC",  # stopping mechanism
  stopping_rounds = 10,     # number of rounds
  stopping_tolerance = 0    # stops after trees add no improvement
  )

# look at results
h2o.performance(m2_h2o, xval = TRUE)
```


#### Tuning {#h2o-rf-multi-classification-tune}

As discussed in the regression section of this chapter, `h2o.randomForest` provides several tunable hyperparameters, for which we can perform a full (aka full cartesian) or stochastic (aka random discrete) grid search across.

##### Full cartesian grid search {#rf-h2o-classification-tune-full}

First, we can try a comprehensive (full cartesian) grid search, which means we will examine every combination of hyperparameter settings that we specify in `hyper_grid.h2o`. Here, we search across 360 models. 

```{block, type="rmdnote"}
To speed up the grid search I dropped it down to 5-fold cross validation; however, this grid search still took __30 minutes__. As an alternative, you could create a single validation frame (see `validation_frame` in `?h2o.grid`) to score against rather than perform _k_-fold cross validation.
```


The results show a maximum AUC of 1 but don't get too excited about this.  When you over/under sample with `balance_classes = TRUE`, we are essentially bootstrapping extra samples of the observations with `Attrition = Yes`.  This means our up-sampled observations will have many of the same values across the features.  This makes it easier to over exaggerate the predictive performance during our training.  A few characteristics we notice from our results suggest that predictive accuracy is maximized when `balance_classes = TRUE`, `max_depth` is larger, `min_rows` is smaller, and `sample_rate` $< 1$.  


```{r rf-h2o-binary-classification-full-catesian-grid-search, eval=FALSE}
# hyperparameter grid
hyper_grid.h2o <- list(
  mtries      = c(2, 5, 10, 15),
  max_depth   = seq(10, 30, by = 5),
  min_rows    = c(1, 3, 5),
  sample_rate = c(.632, .8, .95),
  balance_classes = c(TRUE, FALSE)
)

# build grid search 
grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_full_grid",
  x = predictors, 
  y = response, 
  training_frame = attrit_train_h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = list(strategy = "Cartesian"),
  ntrees = 500,
  seed = 123,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE,
  stopping_metric = "AUC",     
  stopping_rounds = 10,        
  stopping_tolerance = 0 
  )

# collect the results and sort by our model performance metric of choice
full_grid_perf <- h2o.getGrid(
  grid_id = "rf_full_grid", 
  sort_by = "auc", 
  decreasing = TRUE
  )
print(full_grid_perf)
## H2O Grid Details
## ================
## 
## Grid ID: rf_full_grid 
## Used hyper parameters: 
##   -  balance_classes 
##   -  max_depth 
##   -  min_rows 
##   -  mtries 
##   -  sample_rate 
## Number of models: 720 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by decreasing auc
##   balance_classes max_depth min_rows mtries sample_rate              model_ids auc
## 1            true        25      1.0      2       0.632   rf_full_grid_model_6 1.0
## 2            true        30      1.0     10        0.95 rf_full_grid_model_308 1.0
## 3            true        30      1.0      2       0.632   rf_full_grid_model_8 1.0
## 4            true        30      1.0      5         0.8 rf_full_grid_model_158 1.0
## 5            true        15      1.0     10         0.8 rf_full_grid_model_182 1.0
## 
## ---
##     balance_classes max_depth min_rows mtries sample_rate              model_ids                auc
## 715           false        10      1.0     10        0.95 rf_full_grid_model_301 0.6200193262072915
## 716           false        25      3.0      5        0.95 rf_full_grid_model_287 0.6037350054525626
## 717           false        10      1.0     15        0.95 rf_full_grid_model_331 0.6013023735068234
## 718           false        10      1.0      2        0.95 rf_full_grid_model_241 0.5981035997865863
## 719           false        25      1.0     10        0.95 rf_full_grid_model_307 0.5896310022558814
## 720           false        15      1.0      2        0.95 rf_full_grid_model_243 0.5875399042298484
```


##### Random discrete grid search {#rf-h2o-binary-class-tune-random}

Rather than perform a full grid search, we could've sped up the search process by using the random discrete grid search.  The following performs a random search across the same __360__ hyperparameter combinations, stopping if none of the last 10 models have managed to have a 0.01% improvement in AUC compared to the best model before that. I cut the grid search off after 1200 seconds (20 minutes) if a final approximately optimal model is not found. Our grid search assessed 171 of the 360 models before stopping and the best model achieved an AUC of 0.812. 


```{r rf-h2o-binary-classification-random-grid-search, eval=FALSE}
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "AUC",
  stopping_tolerance = 0.0001,
  stopping_rounds = 10,
  max_runtime_secs = 60*20
  )

# build grid search 
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response, 
  training_frame = attrit_train_h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria,
  ntrees = 500,
  seed = 123,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE,
  stopping_metric = "AUC",     
  stopping_rounds = 10,        
  stopping_tolerance = 0
  )

# collect the results and sort by our model performance metric of choice
random_grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "auc", 
  decreasing = TRUE
  )
print(random_grid_perf)
## H2O Grid Details
## ================
## 
## Grid ID: rf_random_grid 
## Used hyper parameters: 
##   -  balance_classes 
##   -  max_depth 
##   -  min_rows 
##   -  mtries 
##   -  sample_rate 
## Number of models: 171 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by decreasing auc
##   balance_classes max_depth min_rows mtries sample_rate                model_ids                auc
## 1           false        30      3.0      2        0.95  rf_random_grid_model_39 0.8121580547112462
## 2           false        10      3.0      2        0.95 rf_random_grid_model_163 0.8115421532554791
## 3           false        20      1.0      5        0.95 rf_random_grid_model_101 0.7974137471337919
## 4           false        30      3.0      2       0.632  rf_random_grid_model_73 0.7966458699941342
## 5           false        15      5.0      2         0.8   rf_random_grid_model_9 0.7965312216711993
## 
## ---
##     balance_classes max_depth min_rows mtries sample_rate                model_ids                auc
## 166            true        20      3.0     15        0.95 rf_random_grid_model_113 0.7474084146536555
## 167            true        30      3.0     15         0.8  rf_random_grid_model_83 0.7469284914413694
## 168            true        10      1.0     15        0.95 rf_random_grid_model_107   0.74409961072895
## 169            true        25      1.0     15        0.95  rf_random_grid_model_63 0.7314962939263051
## 170            true        20      1.0     15        0.95 rf_random_grid_model_142 0.7314962939263051
## 171            true        30      1.0     15        0.95  rf_random_grid_model_61 0.7290326881032368
```


Once we’ve identifed the best set of hyperparameters, we can extract the model. For the remaining examples I will use the optimal model from the random grid search.

```{r rf-h2o-binary-classification-best-model, cache=TRUE, eval=FALSE}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
```

```{r rf-h2o-binary-classification-best-model-import, echo=FALSE, cache=FALSE}
# save the model
#h2o.saveModel(object = best_model, path = "outputs/h2o_best_rf_model", force = TRUE)

# import model
best_model <- h2o.loadModel(path = "outputs/h2o_best_rf_model/rf_random_grid_model_39")
```

#### Visualizing results {#h2o-rf-binary-classification-viz}

##### Variable importance {#h2o-rf-binary-classification-vip}

Assessing the variable importance, we see similar results as with the __ranger__ model with the most influential variables including `OverTime`, `MonthlyIncome`, `Age`, and `JobRole`.


```{r rf-h2o-binary-classification-vip-plot}
# plot top 25 influential variables
vip(best_model, num_features = 25, bar = FALSE)
```


##### Partial dependence plots {#h2o-rf-multi-classification-pdp}

As with `ranger`, we can also assess PDPs and ICE curves. The following looks at three of the most influential variables in our model (`OverTime`, `MonthlyIncome` and `Age`).  Our centered ICE curves help to illustrate the marginal increasing or decreasing effect on the predicted probability of attrition.  In all three plots we see groups of observations going in opposite directions (i.e. at age 35 many employees experience an increase in the probability of attrition while many others a decrease).  This indicates interaction effects with other features, which we will explore more in the ___Model Interpretability___ chapter.

```{block, type="rmdnote"}
These plots illustrate similar attributes that we saw in the __ranger__ PDP/ICE curve plots.
```


```{r rf-h2o-binary-classification-ice, fig.width=10, fig.height=3, fig.cap="ICE curves for `OverTime`, `MonthlyIncome`, and `Age`."}
pfun <- function(object, newdata) {
  as.data.frame(predict(object, newdata = as.h2o(newdata)))[[3L]]
}

# JobRole partial dependencies
ot.ice <- partial(
  best_model, 
  pred.var = "OverTime", 
  train = attrit_train,
  pred.fun = pfun
)

# MonthlyIncome partial dependencies
income.ice <- partial(
  best_model, 
  pred.var = "MonthlyIncome", 
  train = attrit_train,
  pred.fun = pfun,
  grid.resolution = 20
)

# Age partial dependencies
age.ice <- partial(
  best_model, 
  pred.var = "Age", 
  train = attrit_train,
  pred.fun = pfun,
  grid.resolution = 20
)

p1 <- autoplot(ot.ice, alpha = 0.1, center = TRUE) 
p2 <- autoplot(income.ice, alpha = 0.1, center = TRUE) 
p3 <- autoplot(age.ice, alpha = 0.1, center = TRUE)
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

##### ROC curve

Visualizing our ROC curve helps to illustrate our cross-validated AUC of 0.812.

```{r rf-h2o-binary-classification-roc, fig.height=4.5, fig.width=5.5, fig.cap="ROC curve for our __ranger__ random forest model based on the training data."}
h2o.performance(best_model, xval = TRUE) %>%
  plot()
```


#### Predicting {#h2o-rf-binary-classification-predict}

Finally, if you are satisfied with your final model we can predict values for an unseen data set a couple different ways.  Both `predict` and `h2o.predict` will provide the predicted class and the probability of each class. We can also quickly assess the model's performance on our test set with `h2o.performance`. 

```{r rf-h2o-binary-classification-final-predictions}
# predict new values with base R predict()
predict(best_model, attrit_test_h2o)

# predict new values with h2o.predict()
h2o.predict(best_model, newdata = attrit_test_h2o)

# assess performance on test data
h2o.performance(best_model, newdata = attrit_test_h2o)
```

```{r rf-h2o-binary-classification-shutdown, cache=FALSE}
h2o.shutdown(prompt = FALSE)
```


## Implementation: Multinomial Classification {#rf-multi}

To illustrate various random forest concepts for a multinomial classification problem we will continue with the mnist data, where the goal is to predict handwritten numbers ranging from 0-9. 


```{r import-rf-multi-classification}
# import mnist training and testing data
train <- data.table::fread("../data/mnist_train.csv", data.table = FALSE)
test <- data.table::fread("../data/mnist_test.csv", data.table = FALSE)
```

### `ranger` {#rf-ranger-multi}

To use __ranger__ on a multiclassification problem we need our response variable to be encoded as a character or factor. Since our response variable for the mnist data (`V785`) contains numeric responses (integers ranging from 0-9), we need to convert these to a factor.

```{r}
# convert response variable to a factor
train <- mutate(train, V785 = factor(V785))
test <- mutate(test, V785 = factor(V785))
```


#### Basic implementation {#ranger-multi-basic}

Once our response variable is properly encoded as a factor or character, `ranger::ranger` will automatically apply a random forest model with multinomial terminal nodes without you having to specify. Consequently, the following applies a default random forest model with 500 trees.  Since all the predictors in the mnist data set are numeric we do not need to worry about setting the `respect.unordered.factors` parameter.

As your data set grows in size, random forests can become slow.  Parallel processing can speed up the process and, by default, __ranger__ will use the number of CPUs available (you can manually set the number of CPUs to use with `num.threads`).  However, even so, on the mnist data the default __ranger__ model takes three minutes to train.  The results show an OOB error of 2.98%, which is already more accurate than the tuned regularized regression results (see Section \@ref(glm-multinomial-classification)). 

```{block, type="rmdnote"}
This basic default model took a little over 3 minutes to train.
```


```{r rf-multi-classification-ranger-basic, eval=FALSE}
# perform basic random forest model
m1_ranger <- ranger(
  formula    = V785 ~ ., 
  data       = train, 
  num.trees  = 500,
  verbose    = FALSE,
  seed       = 123
  )

# look at results
m1_ranger
## Ranger result
## 
## Call:
##  ranger(formula = V785 ~ ., data = train, num.trees = 500, seed = 123) 
## 
## Type:                             Classification 
## Number of trees:                  500 
## Sample size:                      60000 
## Number of independent variables:  784 
## Mtry:                             28 
## Target node size:                 1 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error:             2.98 %

# look at confusion matrix
m1_ranger$confusion.matrix
##     predicted
## true    0    1    2    3    4    5    6    7    8    9
##    0 5855    1    7    2    3    6   18    0   28    3
##    1    1 6646   36   11    9    4    5   13    9    8
##    2   24    9 5787   23   22    2   14   37   36    4
##    3    9    6   73 5840    1   62    8   44   62   26
##    4    8   11    9    0 5683    0   23   10   12   86
##    5   21    5    8   61    9 5223   41    5   27   21
##    6   20   11    5    1    9   30 5824    0   18    0
##    7    3   22   55    6   28    0    0 6069   10   72
##    8    8   27   30   32   20   36   26    4 5610   58
##    9   21    9   13   65   55   19    4   45   42 5676
```


#### Tuning {#ranger-multi-tune}

As in the binary classification section, we will tune the various hyperparameters of the `ranger` function.  The following creates a hyperparameter grid of 160 model combinations with varying number of trees (`num_trees`), number of variables to possibly split at (`mtry`), terminal node size (`node_size`), sample size (`sample_size`), and the split rule (`splitrule`).

```{r rf-multi-classification-ranger-grid, eval=FALSE}
hyper_grid <- expand.grid(
  num_trees   = c(250, 500),
  mtry        = seq(15, 35, by = 5),
  node_size   = seq(1, 10, by = 3),
  sample_size = c(.632, .80),
  splitrule   = c("gini", "extratrees"),
  OOB_error   = 0
)

nrow(hyper_grid)
```

Our OOB classification error ranges between 0.0302-0.0383.  Our top 10 performing models all have classification error rates in the low 0.03 range. The results show that most of the top 10 models use higher `mtry` values, smaller `min.node.size`, and include a stochastic nature with `sample.fraction` $> 1$. We also see that the two top models use the `extratrees` split rule versus the `gini`; however, the improvement is marginal.

```{block, type="rmdtip"}
This grid search took 5 hours and 38 minutes!
```


```{r rf-multi-classification-ranger-grid-search, eval=FALSE}
for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger(
    formula         = V785 ~ ., 
    data            = train,
    seed            = 123,
    verbose         = FALSE,
    num.trees       = hyper_grid$num_trees[i],
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    splitrule       = hyper_grid$splitrule[i]
  )
  
  # add OOB error to grid
  hyper_grid$OOB_error[i] <- model$prediction.error
}

hyper_grid %>% 
  dplyr::arrange(OOB_error) %>%
  head(10)
##    num.trees mtry node_size sample_size  splitrule  OOB_error
## 1        250   35         1         0.8 extratrees 0.03021667
## 2        500   35         1         0.8 extratrees 0.03021667
## 3        250   35         1         0.8       gini 0.03046667
## 4        500   35         1         0.8       gini 0.03046667
## 5        250   35         4         0.8       gini 0.03061667
## 6        500   35         4         0.8       gini 0.03061667
## 7        250   30         1         0.8       gini 0.03065000
## 8        500   30         1         0.8       gini 0.03065000
## 9        250   20         1         0.8       gini 0.03078333
## 10       500   20         1         0.8       gini 0.03078333
```

The above grid search helps to focus where we can further refine our model tuning.  As a next step, we could perform additional grid searches focusing on additional ranges of these parameters.  However, for brevity we leave this as an exercise for the reader.

#### Visual interpretation {#ranger-multi-viz}

##### Variable importance {ranger-multi-vip}

As in the regression and binary classification setting, once we've found our optimal hyperparameter settings we can re-run our model and set the `importance` argument to "impurity" and/or "permutation". The following applies both settings so that we can compare and contrast the influential variables each method identifies.


```{r rf-multi-classification-ranger-optimal-model-with-importance}
# re-run model with impurity-based variable importance
m3_ranger_impurity <- ranger(
  formula         = V785 ~ ., 
  data            = train, 
  num.trees       = 250,
  mtry            = 35,
  min.node.size   = 1,
  sample.fraction = .80,
  splitrule       = "extratrees",
  importance      = 'impurity',
  verbose         = FALSE,
  seed            = 123
  )

# re-run model with permutation-based variable importance
m3_ranger_permutation <- ranger(
  formula         = V785 ~ ., 
  data            = train, 
  num.trees       = 250,
  mtry            = 35,
  min.node.size   = 1,
  sample.fraction = .80,
  splitrule       = "extratrees",
  importance      = 'permutation',
  verbose         = FALSE,
  seed            = 123
  )
```


In the regularized regression chapter, we saw that 67 predictors were not used because they contained zero variance.  For random forests, we can assess how many and which variables were not used for any splits within our random forest model to improve impurtiy.  This signals those variables that do not provide any increase in the accuracy within each terminal node.  For our `m3_ranger_impurity` model there are 102 variables not used for any splits.  We can do the same with the `m3_ranger_permutation` model; however, variables with zero importance represent those variables that when scrambled, still do not hurt the performance of our model.  We see there are 153 variables that have zero importance for the permutation-based approach.

```{r}
# how many variables not used for a split
which(m3_ranger_impurity$variable.importance == 0) %>% length()

# how many variables where randomizing their values does not hurt performance
which(m3_ranger_permutation$variable.importance == 0) %>% length()
```


Alternatively, to find important variables we can extract the top 10 influential variables as we did in the regression and binary classification problems.  Unlike, in the regularized regression chapter, we cannot extract the most influential variables for each class.  This is due to how variable importance is calculated differently between the two.  However, shortly we will see how to understand the relationship an influential variable and the different classes of our response variable.

Figure \@ref(fig:rf-multi-classification-ranger-top10-vip) illustrates the top 25 influential variables in our random forest model. We see many of the same variables towards the top albeit in differing order (i.e. `V379`, `V351`, `V462`, `V407`) signaling that these variables appear influential regardless of the importance measure used.

```{r rf-multi-classification-ranger-top10-vip, fig.width=10, fig.cap="Top 25 most important variables based on impurity (left) and permutation (right)."}
p1 <- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle("Impurity-based variable importance")
p2 <- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle("Permutation-based variable importance")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

##### Partial dependence plots {ranger-multi-pdp}

After the most relevant variables have been identified, we can assess the relationship between these influential predictors and the response variable with PDP plots and ICE curves. As with the binary classification model, to generate PDPs and ICE curves we need to use the probability model (`probability = TRUE`) so that can extract the class probabilities.

```{block, type="rmdnote"}
To produce a PDP with multi-classification problems, we need to create a custom prediction function that will return a data frame of the ___mean predicted probability___ for each response class. We supply this custom prediction function
```

In this example, we assess the PDP of each response category with variable `V379`, which ranked first as the most influential variable.  We can see those response categories where this variable has a large impact (stronger changes in the predicted value $\hat y$ as `V379` changes) versus those that are less influnced by this predictor (mostly flat-lined plots).

```{r rf-ranger-multi-classification-pdp, fig.height=4, fig.width=10, fig.cap="Partial dependence plots of our most influential variable (`V379`) across the 10 response levels. This variable appears to be most influential in predicting the number 0, 1, 3, and 7."}
# probability model
m3_ranger_prob <- ranger(
  formula         = V785 ~ ., 
  data            = train, 
  num.trees       = 250,
  mtry            = 35,
  min.node.size   = 1,
  sample.fraction = .80,
  importance      = 'impurity',
  probability     = TRUE,
  verbose         = FALSE,
  seed            = 123,
  )

# custom prediction function
custom_pred <- function(object, newdata) {
  pred <- predict(object, newdata)$predictions
  avg <- purrr::map_df(as.data.frame(pred), mean)
  return(avg)
}

# partial dependence of V379
pd <- partial(m3_ranger_prob, pred.var = "V379", pred.fun = custom_pred, train = train)
ggplot(pd, aes(V379, yhat, color = factor(yhat.id))) + 
  geom_line(show.legend = FALSE) +
  facet_wrap(~ yhat.id, nrow = 2) +
  expand_limits(y = 0)
```





#### Predicting {#ranger-multi-predict}

Finally, if you are satisfied with your final model we can predict values for an unseen data set with `predict`.  If using a non-probability model then your predicted outputs will be a vector containing the predicted class for each observation.  If using a probability model then your predicted outputs will be the probability of each class. In both cases, the result of `predict` is a list with the actual predictions contained in `object$predictions`.

```{r rf-ranger-multi-classification-final-predictions}
# predict class as the output
pred_class <- predict(m3_ranger_impurity, test)
head(pred_class$predictions)

# predict probability as the output
pred_prob <- predict(m3_ranger_prob, test)
pred_prob$predictions[1:5, ]
```

Lastly, to assess various performance metrics on our test data we can use `caret::confusionMatrix`, which provides the majority of the performance measures we are typically concerned with in classification models.  We can see that the overall accuracy rate of 0.9702 is significantly higher than our accuracy of $\approx 0.93$ for both regularized regression models.  We also see our model is doing a much better job of predicting "2", "3", "5", "8", and "9" which were all poorly predicted by the regularized regression models.

```{r ranger-multi-class-confusion-matrix}
caret::confusionMatrix(factor(pred_class$predictions), factor(test$V785))
```



### `h2o` {#rf-h2o-multi}

#### Basic implementation {#rf-h2o-multi-basic}


#### Tuning {#rf-h2o-multi-tune}


#### Visual interpretation {#rf-h2o-multi-viz}


#### Predicting {#rf-h2o-multi-predict}


## Learning More {#rf-learn}

Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. Because of their more simplistic tuning nature and the fact that they require very little, if any, feature pre-processing they are often one of the first go-to algorithms when facing a predictive modeling problem. To learn more I would start with the following resources listed in order of complexity:

* [Practical Machine Learning with H2O](https://www.amazon.com/Practical-Machine-Learning-H2O-Techniques-ebook/dp/B01MQST5Y5)
* [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
* [Applied Predictive Modeling](http://appliedpredictivemodeling.com/)
* [Computer Age Statistical Inference](https://www.amazon.com/Computer-Age-Statistical-Inference-Mathematical/dp/1107149894)
* [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)



[^ledell]: The features highlighted for each package were originally identified by Erin LeDell in her [useR! 2016 tutorial](https://github.com/ledell/useR-machine-learning-tutorial).
[^task]: See the Random Forest section in the [Machine Learning Task View](https://CRAN.R-project.org/view=MachineLearning) on CRAN and Erin LeDell's [useR! Machine Learning Tutorial](https://koalaverse.github.io/machine-learning-in-R/random-forest.html#random-forest-software-in-r) for a non-comprehensive list.
[^var]: H2O documentation [link](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html)








